{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aditya Sawant's Version of SPN_IP_5Shot.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Used\n",
    "\n",
    "- tensorflow\n",
    "- sklearn\n",
    "- numpy\n",
    "- matplotlib\n",
    "- pandas\n",
    "- scipy\n",
    "- tensorflow_probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install easyfsl tensorflow sklearn numpy matplotlib scipytensorflow_probability pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def timeIt(func):\n",
    "    \"\"\"\n",
    "    timeIt is a decorator function to time the execution of a function.\n",
    "    \n",
    "    :param func: function to be timed\n",
    "    :return: wrapper function\n",
    "    \"\"\"\n",
    "    def wrap_func(*args, **kwargs):\n",
    "        t1 = time()\n",
    "        result = func(*args, **kwargs)\n",
    "        t2 = time()\n",
    "        print(f'Function {func.__name__!r} executed in {(t2-t1):.4f}s')\n",
    "        return result\n",
    "    return wrap_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# trainingData = pd.DataFrame(columns=['Loss', 'Accuracy'])\n",
    "def plotData(data, testing=False):\n",
    "    \n",
    "    if testing:\n",
    "        accuracy1_values = [item[0] for item in data]\n",
    "        accuracy2_values = [item[1] for item in data]\n",
    "        loss1_values = [item[2] for item in data]\n",
    "        loss2_values = [item[3] for item in data]\n",
    "\n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(loss1_values, color='red', label='Loss')\n",
    "        plt.plot(accuracy1_values, color='blue', label='Accuracy')\n",
    "        plt.xlabel('Epoch/Episode')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Loss and Overall Accuracy 1')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(loss2_values, color='red', label='Loss')\n",
    "        plt.plot(accuracy2_values, color='blue', label='Accuracy')\n",
    "        plt.xlabel('Epoch/Episode')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Loss and Overall Accuracy 2')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        loss_values = [item[0] for item in data]\n",
    "        accuracy_values = [item[1] for item in data]\n",
    "\n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(loss_values, color='red', label='Loss')\n",
    "        plt.plot(accuracy_values, color='blue', label='Accuracy')\n",
    "        plt.xlabel('Epoch/Episode')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Loss and Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import statistics\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "  \n",
    "from operator import truediv\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.compat.v1.distributions import Bernoulli\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
    "\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Test each code block individually\n",
    "TEST_BLOCKS: bool = False\n",
    "\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "\n",
    "# Dataset Used : Indian Pines\n",
    "DATASET: str = 'indian_pines'\n",
    "BASE_PATH: str = 'D:\\\\HSI FSL BE-10 Major Project\\\\'\n",
    "PATH_TO_DATASET: str = BASE_PATH + 'Datasets\\\\'\n",
    "NUM_CLASSES: int\n",
    "\n",
    "# PCA\n",
    "PCA_COMPONENTS: int = 30 # Number of components to keep after PCA reduction\n",
    "\n",
    "# Window size for forming image cubes\n",
    "WINDOW_SIZE: int = 11\n",
    "\n",
    "# Image dimensions after forming image cubes\n",
    "IMAGE_WIDTH: int\n",
    "IMAGE_HEIGHT: int\n",
    "IMAGE_DEPTH: int\n",
    "IMAGE_CHANNEL: int \n",
    "IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH, IMAGE_CHANNEL = 11, 11, 30, 1\n",
    "\n",
    "# Model Parameters\n",
    "\n",
    "N_TIMES = 1 # Number of times to run the model. Internally, the model is runs each episode N_TIMES times\n",
    "\n",
    "# Learning Rate\n",
    "LEARNING_RATE: float = 0.00001\n",
    "\n",
    "\n",
    "# Training & Testing Parameters\n",
    "TRAINING_CLASSES: list # Classes to be used for training\n",
    "TRAINING_LABELS: list # Labels to be used for training\n",
    "\n",
    "TESTING_CLASSES: list # Classes to be used for testing\n",
    "TESTING_LABELS: list # Labels to be used for training\n",
    "\n",
    "TUNNING_CLASSES: list # Declared in Main Block. Classes to be used for tunning\n",
    "TUNNING_LABELS: list  # Labels to be used for tunning\n",
    "\n",
    "\n",
    "TRAIN_C: int # Number of samples per class to be used for training\n",
    "TRAIN_K: int # Number of patches per class to be used for support during training\n",
    "TRAIN_N: int # Number of patches per class to be used for query during training\n",
    "\n",
    "TUNE_C: int # Number of samples per class to be used for testing\n",
    "TUNE_K: int # Number of patches per class to be used for support during testing\n",
    "TUNE_N: int # Number of patches per class to be used for query during testing\n",
    "\n",
    "TEST_C: int # Number of samples per class to be used for testing\n",
    "TEST_K: int # Number of patches per class to be used for support during testing\n",
    "TEST_N: int # Number of patches per class to be used for query during testing\n",
    "# ===================================\n",
    "# DO NOT REMOVE THIS.\n",
    "tC = 3   # classes in a test episode \n",
    "# Don't know this yet, probably used in the model to calculate loss\n",
    "MC_LOSS_WEIGHT: int = 5 \n",
    "# DIRECTLY USED IN PROTOTYPICAL NETWORK CLASS IN TESTING CASE\n",
    "# ===================================\n",
    "\n",
    "\n",
    "\n",
    "# Training Epochs\n",
    "TRAINING_EPOCH: int = 10\n",
    "\n",
    "# Training Episode\n",
    "TRAINING_EPISODE: int = 50\n",
    "\n",
    "# Tunning Epochs\n",
    "TUNNING_EPOCH: int = 41\n",
    "\n",
    "# Tunning Episode\n",
    "TUNNING_EPISODE: int = 100\n",
    "\n",
    "# Testing Epochs\n",
    "TESTING_EPOCH: int = 1000\n",
    "\n",
    "# Metrics to be used for evaluation\n",
    "train_loss = tf.metrics.Mean(name='train_loss')\n",
    "train_acc = tf.metrics.Mean(name='train_accuracy')\n",
    "tune_loss = tf.metrics.Mean(name='tune_loss')\n",
    "tune_acc = tf.metrics.Mean(name='tune_accuracy')\n",
    "test_loss = tf.metrics.Mean(name='test_loss')\n",
    "test_acc = tf.metrics.Mean(name='test_accuracy')\n",
    "\n",
    "trainingData = []\n",
    "tunningData = []\n",
    "testingData = []\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_dir = BASE_PATH + 'saves\\\\checkpoints\\\\' + DATASET + '\\\\' + TRAIN_C + '\\\\' + 'Train'\n",
    "checkpoint_prefix_train = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "checkpoint_dir1 = BASE_PATH + 'saves\\\\checkpoints\\\\' + DATASET + '\\\\' + TRAIN_C + '\\\\' + 'Train\\\\Tune'\n",
    "checkpoint_prefix_tune = os.path.join(checkpoint_dir1, \"ckpt\")\n",
    "\n",
    "checkpoint = None  # To be used for loading checkpoints. Declared in the Main Block\n",
    "ProtoModel = None  # Prototypical Network Object. Declared in the Main Block\n",
    "model = None  # Model Object. Declared in the Main Block\n",
    "optimizer = None  # Optimizer Object. Declared in the Main Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def loadData(name: str) -> (np.ndarray, np.ndarray):\n",
    "    '''\n",
    "     loadData loads the data from the .mat files\n",
    "     \n",
    "     :param name: name of the dataset \n",
    "     :return: (data, labels)\n",
    "    '''\n",
    "    if(name == 'IP'):\n",
    "      NUM_CLASSES: int = 16      \n",
    "      # Training & Testing Parameters\n",
    "      TRAINING_CLASSES: list = [1,2,4,5,7,9,10,11,13,14] # Classes to be used for training\n",
    "      TRAINING_LABELS: list = list(map(lambda x: x+1, TRAINING_CLASSES)) # Labels to be used for training\n",
    "\n",
    "      TESTING_CLASSES: list = [0,3,6,8,12,15] # Classes to be used for testing\n",
    "      TESTING_LABELS: list = list(map(lambda x: x+1, TESTING_CLASSES)) # Labels to be used for training\n",
    "\n",
    "      TUNNING_CLASSES = None # Declared in Main Block. Classes to be used for tunning\n",
    "      TUNNING_LABELS = TESTING_LABELS # Labels to be used for tunning\n",
    "\n",
    "\n",
    "      TRAIN_C: int = 5 # Number of samples per class to be used for training\n",
    "      TRAIN_K: int = 5 # Number of patches per class to be used for support during training\n",
    "      TRAIN_N: int = 15 # Number of patches per class to be used for query during training\n",
    "\n",
    "      TUNE_C: int = 3 # Number of samples per class to be used for testing\n",
    "      TUNE_K: int = 1 # Number of patches per class to be used for support during testing\n",
    "      TUNE_N: int = 4 # Number of patches per class to be used for query during testing\n",
    "\n",
    "      TEST_C: int = 3 # Number of samples per class to be used for testing\n",
    "      TEST_K: int = 5 # Number of patches per class to be used for support during testing\n",
    "      TEST_N: int = 5 # Number of patches per class to be used for query during testing\n",
    "      data = sio.loadmat(f'{PATH_TO_DATASET}indian_pines_corrected.mat')[ f'indian_pines_corrected']\n",
    "      labels = sio.loadmat(f'{PATH_TO_DATASET}indian_pines_gt.mat')[ f'indian_pines_gt'] \n",
    "\n",
    "\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "if(TEST_BLOCKS): \n",
    "  print(loadData('indian_pines'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def applyPCA(X: np.ndarray, n_components: int = 30) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    applyPCA reduces the dimensionality of the data using PCA.\n",
    "    \n",
    "    :param X: The data to be reduced.\n",
    "    :param n_components: The number of components to keep.\n",
    "    :return: The data with reduced dimensionality.\n",
    "    \"\"\"\n",
    "    \n",
    "    pca = PCA(n_components=n_components, whiten=True) # create a PCA object\n",
    "    new_X = np.reshape(X, (-1, X.shape[2])) # reshape the data into a 2D matrix\n",
    "    new_X = pca.fit_transform(new_X) # fit the PCA object\n",
    "    new_X = np.reshape(new_X, (X.shape[0], X.shape[1], n_components)) # reshape the data into a 3D matrix\n",
    "    del pca # delete the PCA object\n",
    "    return new_X\n",
    "    \n",
    "if(TEST_BLOCKS):\n",
    "    # TODO: Implement a test for applyPCA\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def padWithZeros(X: np.ndarray, margin:int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    padWithZeros pads the input array X with zero margins in height and width dimensions.\n",
    "    \n",
    "    :param X: input array of shape (W, H, C).\n",
    "    :param margin: number of zeros to pad on each side of the height and width dimensions.\n",
    "    :return: X padded with zeros of shape (W + 2*margin, H + 2*margin, C).\n",
    "    \"\"\"\n",
    "    return np.pad(X, ((margin,margin), (margin,margin), (0,0)), 'constant', constant_values=0)\n",
    "    # Previous implementation\n",
    "    # newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    # x_offset = margin\n",
    "    # y_offset = margin\n",
    "    # newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "    # return newX\n",
    "\n",
    "if(TEST_BLOCKS):\n",
    "    test_X = np.random.randn( 3, 3, 2)\n",
    "    test_margin = 2\n",
    "    print(padWithZeros(test_X, test_margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def createImageCubes(X: np.ndarray, Y: np.ndarray, windowSize: int) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    createImageCubes creates image cubes from the given image and label data. Only height and width are considered for the image cube creation. The depth of the image cube is the number of bands in the image. The depth is preserved from the input image.\n",
    "    \n",
    "    :param X: input image\n",
    "    :param Y: input label\n",
    "    :param windowSize: size of the image cube to be created. Height and width of the image cube is (windowSize, windowSize)\n",
    "    :return (dataPatches, dataLabels): dataPatches is a list of image cubes. dataLabels is a list of labels corresponding to the image cubes in dataPatches\n",
    "    \n",
    "    Algorithm:\n",
    "    - Calculate the margin to be padded to the image.\n",
    "    - Pad the image with zeros.\n",
    "    - Create image cubes from the padded image.\n",
    "    - Expand the dimensions of the image cubes to include the channel dimension.\n",
    "    - Create labels for the image cubes.\n",
    "    - Return the image cubes and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    margin = int(windowSize // 2)\n",
    "    zeroPaddedX = padWithZeros(X, margin)\n",
    "    dataPatches = [zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1] for r in range(margin, zeroPaddedX.shape[0] - margin) for c in range(margin, zeroPaddedX.shape[1] - margin)]\n",
    "    dataPatches = np.expand_dims(dataPatches, axis=-1)\n",
    "    dataLabels = [Y[r-margin, c-margin] for r in range(margin, zeroPaddedX.shape[0] - margin) for c in range(margin, zeroPaddedX.shape[1] - margin)]\n",
    "    return dataPatches, np.array(dataLabels)\n",
    "\n",
    "if(TEST_BLOCKS):\n",
    "    ip_x1, ip_y = loadData(DATASET)                            \n",
    "    ip_x2 = applyPCA(ip_x1,n_components=30)                   \n",
    "    ip_X,ip_Y = createImageCubes(ip_x2, ip_y, windowSize=IMAGE_WIDTH)\n",
    "    print(ip_X.shape, ip_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def classWisePatches(X:np.ndarray, Y:np.ndarray) -> list:\n",
    "    \"\"\"\n",
    "    classWisePatches function takes in the input image and its corresponding label and returns the patches of the image classwise.\n",
    "    \n",
    "    It will return a list of patches of the image classwise. For example, if the image has 3 classes, it will return a list of 3 elements, where each element is a 5D array of shape (num_patches, patch_size, patch_size, num_channels, 1)\n",
    "    \n",
    "    :param X: Input image\n",
    "    :param Y: Corresponding label\n",
    "    :return: Classwise patches of the image\n",
    "    \"\"\"\n",
    "    patches =  [ X[Y==i,:,:,:,:] for i in range(1,NUM_CLASSES+1) ]\n",
    "    return patches\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Construction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    \"\"\"\n",
    "    createModel() function creates the model architecture for the 3D CNN model.\n",
    "    :return: model \n",
    "    \n",
    "    The model architecture is as follows:\n",
    "    1. Input layer\n",
    "    2. 3D Convolution layer with 8 filters, kernel size (3,3,7), activation function 'relu' and padding 'same'\n",
    "    3. Spatial Dropout layer with dropout rate 0.3\n",
    "    4. 3D Convolution layer with 16 filters, kernel size (3,3,5), activation function 'relu' and padding 'same'\n",
    "    5. Spatial Dropout layer with dropout rate 0.3\n",
    "    6. 3D Convolution layer with 32 filters, kernel size (3,3,3), activation function 'relu'\n",
    "    7. Reshape layer to reshape the output of 3D Convolution layer to 2D\n",
    "    8. 2D Convolution layer with 64 filters, kernel size (3,3), activation function 'relu'\n",
    "    9. Flatten layer to flatten the output of 2D Convolution layer\n",
    "    10. Dropout layer with dropout rate 0.4\n",
    "    11. Dense layer with 256 neurons and activation function 'relu'\n",
    "    12. Dropout layer with dropout rate 0.4\n",
    "    13. Dense layer with 128 neurons and activation function 'relu'\n",
    "    14. Output layer with 128 neurons and activation function 'relu'\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    input_layer = layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH, IMAGE_CHANNEL))\n",
    "    \n",
    "    output_layer_1_conv = layers.Conv3D(filters=8, kernel_size=(3,3,7), activation='relu',input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH, IMAGE_CHANNEL),padding='same')(input_layer)\n",
    "    \n",
    "    output_layer_1_drop3d = layers.SpatialDropout3D(rate=0.3, data_format='channels_last')(output_layer_1_conv,training=True)\n",
    "    \n",
    "    output_layer_2_conv = layers.Conv3D(filters=16, kernel_size=(3,3,5), activation='relu',padding='same')(output_layer_1_drop3d)\n",
    "    \n",
    "    output_layer_2_drop3d = layers.SpatialDropout3D(rate=0.3, data_format='channels_last')(output_layer_2_conv,training=True)\n",
    "    \n",
    "    output_layer_3_conv = layers.Conv3D(filters=32, kernel_size=(3,3,3), activation= 'relu')(output_layer_2_drop3d)\n",
    "    \n",
    "    output_layer_3_reshaped = layers.Reshape((output_layer_3_conv.shape[1], output_layer_3_conv.shape[2], output_layer_3_conv.shape[3]*output_layer_3_conv.shape[4]))(output_layer_3_conv)\n",
    "    \n",
    "    output_layer_4_conv = layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu')(output_layer_3_reshaped)\n",
    "    \n",
    "    output_layer_4_flatten = layers.Flatten()(output_layer_4_conv)\n",
    "    \n",
    "    output_layer_4_drop = layers.Dropout(rate=0.4)(output_layer_4_flatten,training=True)\n",
    "    \n",
    "    output_layer_4_dense = layers.Dense(256, activation='relu')(output_layer_4_drop)\n",
    "    \n",
    "    output_layer_5_conv = layers.Dropout(0.4)(output_layer_4_dense,training=True)\n",
    "    \n",
    "    output_layer_5_dense = layers.Dense(128, activation='relu')(output_layer_5_conv)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer_5_dense)\n",
    "    \n",
    "    # print(model.summary())\n",
    "    return model\n",
    "    \n",
    "    \n",
    "if(TEST_BLOCKS):\n",
    "    model = createModel()\n",
    "    print(model.summary())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototypical Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def calc_euclidian_dists(x, y):\n",
    "    \"\"\"\n",
    "    calc_euclidian_dists: Calculates the euclidian distance between two tensors\n",
    "    :param x: Tensor of shape (n, d)\n",
    "    :param y: Tensor of shape (m, d)\n",
    "    :return: Tensor of shape (n, m) with euclidian distances\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    m = y.shape[0]\n",
    "    x = tf.tile(tf.expand_dims(x, 1), [1, m, 1])\n",
    "    y = tf.tile(tf.expand_dims(y, 0), [n, 1, 1])\n",
    "    return tf.reduce_mean(tf.math.pow(x - y, 2), 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Prototypical(Model):\n",
    "    def __init__(self, model, w, h, d, c):\n",
    "        super(Prototypical, self).__init__()\n",
    "        self.w, self.h, self.d, self.c = w, h, d, c\n",
    "        self.encoder = model\n",
    "\n",
    "    def call(self, support, query, support_labels, query_labels, K, C, N,n_times,training=True):\n",
    "      # support : support images (25, 11, 11, 30, 1)\n",
    "      # query : query images (75, 11, 11, 30, 1)\n",
    "    \n",
    "      n_class = C                                                               #5\n",
    "      n_support = K                                                             #5\n",
    "      n_query = N                                                               #15 \n",
    "\n",
    "      if training == True : \n",
    "        loss = 0\n",
    "        mc_predictions = []                                                     # list of predictions for multiple passes\n",
    "        for i in range(n_times) :     \n",
    "          y = np.zeros((int(C*N),C))                                              #(75, 5)\n",
    "          \n",
    "          \n",
    "          for i in range(int(C*N)) :                                             # 75\n",
    "            x = support_labels.index(query_labels[i])                           # creation of 1-hot for true labels\n",
    "            y[i][x] = 1.                                               # n_times passing every query image for calculating variance \n",
    "            #  basically we are creating OMR sheet for each query image where each column represents the class and each row represents the query image, where the true class is 1 and rest are 0\n",
    "            \n",
    "          cat = tf.concat([support,query], axis=0)       \n",
    "        #   print('cat', cat.shape, cat)                                       # [100, 11, 11, 30, 1]\n",
    "          z = self.encoder(cat)    \n",
    "        #   print('z', z.shape, z)                                             # [100, 128]   # build a new computational graph from the provided inputs\n",
    "          # Divide embedding into support and query\n",
    "          z_prototypes = tf.reshape(z[:n_class * n_support],[n_class, n_support, z.shape[-1]])   #[5, 5, 128])\n",
    "        #   print('z_p', z_prototypes.shape, z_prototypes)   \n",
    "          # Prototypes are means of n_support examples\n",
    "          z_prototypes = tf.math.reduce_mean(z_prototypes, axis=1)              #[5, 128]\n",
    "        #   print('z_p', z_prototypes.shape, z_prototypes)   \n",
    "          z_query = z[n_class * n_support:]                                     #[75, 128]                         \n",
    "        #   print('z_q', z_query.shape, z_query)   \n",
    "          # Calculate distances between query and prototypes\n",
    "          dists = calc_euclidian_dists(z_query, z_prototypes)                   #[75, 5]\n",
    "        #   print('dist', dists)   \n",
    "          # log softmax of calculated distances\n",
    "          log_p_y = tf.nn.log_softmax(-dists, axis=-1)                          #[75, 5]     this activation function heavily penalizes wrong class prediction as compared to its Softmax counterpart    \n",
    "        #   print('log', log_p_y)   \n",
    "          loss1 = -tf.reduce_mean((tf.reduce_sum(tf.multiply(y, log_p_y), axis=-1)))   #loss for the current pass                     \n",
    "        #   print('loss1', loss1)                                                 # []\n",
    "          loss += loss1                                                         # adding loss for each pass                   \n",
    "          predictions = tf.nn.softmax(-dists, axis=-1)                         # [75, 5] prediction probability for the search-space classes per query image(for current pass)\n",
    "        #   print('pred', predictions)   \n",
    "          mc_predictions.append(predictions)                                          \n",
    "\n",
    "        y = np.zeros((int(C*N),C))\n",
    "        for i in range(int(C*N)) :\n",
    "            x = support_labels.index(query_labels[i])                           # creation of 1-hot for true labels\n",
    "            y[i][x] = 1. \n",
    "        mc_predictions = tf.convert_to_tensor(np.reshape(np.asarray(mc_predictions),(n_times,int(C*N),C)))  #(n_times,75,5)\n",
    "        std_predictions = tf.math.reduce_std(mc_predictions,axis=0)                                         # (75,5)\n",
    "        std = tf.reduce_sum(tf.reduce_sum(tf.multiply(std_predictions,y),axis=1))\n",
    "        # print('std', std)        \n",
    "        loss += MC_LOSS_WEIGHT*std\n",
    "        \n",
    "        # calculating mean accuracy\n",
    "        mean_predictions = tf.reduce_mean(mc_predictions,axis=0)                # mean prediction probability for each class (75,5)\n",
    "        mean_eq = tf.cast(tf.equal(                                             # accuracy for the current pass  c\n",
    "            tf.cast(tf.argmax(mean_predictions, axis=-1), tf.int32),            # check if the index of max probability is equal to the true class index\n",
    "            tf.cast(tf.argmax(y,axis=-1), tf.int32)), tf.float32)               # argmax returns the index of max probability\n",
    "        mean_accuracy = tf.reduce_mean(mean_eq)\n",
    "        mean_predictions = tf.reduce_mean(mc_predictions,axis=0)                # mean prediction probability for each class (5)\n",
    "        return loss, mean_accuracy, mean_predictions   \n",
    "      \n",
    "      if training == False :\n",
    "        loss = 0\n",
    "        mc_predictions = []                                                     # list of predictions for multiple passes  \n",
    "        for i in range(n_times) :                                               # n_times passing the query images for variance calculation\n",
    "          y = np.zeros((int(C*N),C))                                            # (150,10)\n",
    "          for i in range(int(C*N)) :\n",
    "            x = support_labels.index(query_labels[i])                           # creation of 1-hot for the true labels\n",
    "            y[i][x] = 1.  \n",
    "          # merge support and query to forward through encoder\n",
    "          cat = tf.concat([support,query], axis=0)                              # [200,9,9,20,1]   \n",
    "          z = self.encoder(cat)                                                 # [200, 320]\n",
    "          # Divide embedding into support and query\n",
    "          z_prototypes = tf.reshape(z[:n_class * n_support],[n_class, n_support, z.shape[-1]])   #[10, 5, 320])\n",
    "          # Prototypes are means of n_support examples\n",
    "          z_prototypes = tf.math.reduce_mean(z_prototypes, axis=1)              #[10, 320]\n",
    "          z_query = z[n_class * n_support:]                                     #[150, 320]                         \n",
    "          # Calculate distances between query and prototypes\n",
    "          dists = calc_euclidian_dists(z_query, z_prototypes)                   #[150, 10]\n",
    "          # log softmax of calculated distances\n",
    "          log_p_y = tf.nn.log_softmax(-dists, axis=-1)                          #[150, 10]        \n",
    "          loss1 = -tf.reduce_mean((tf.reduce_sum(tf.multiply(y, log_p_y), axis=-1)))        \n",
    "          loss += loss1\n",
    "          predictions = tf.nn.softmax(-dists, axis=-1)                                 # prediction probabilities for the classes for current pass\n",
    "          mc_predictions.append(predictions)               \n",
    "          \n",
    "                                        \n",
    "        y = np.zeros((int(C*N),C))                                            # (150,10)\n",
    "        for i in range(int(C*N)) :\n",
    "            x = support_labels.index(query_labels[i])                           # creation of 1-hot for the true labels\n",
    "            y[i][x] = 1.  \n",
    "        mean_predictions = tf.reduce_mean(mc_predictions,axis=0)                # mean prediction probability for each class (150,10)\n",
    "        mean_eq = tf.cast(tf.equal(                                             # accuracy for the current pass\n",
    "            tf.cast(tf.argmax(mean_predictions, axis=-1), tf.int32), \n",
    "            tf.cast(tf.argmax(y,axis=-1), tf.int32)), tf.float32)\n",
    "        mean_accuracy = tf.reduce_mean(mean_eq)\n",
    "        mean_pred_index = tf.argmax(mean_predictions,axis=1)\n",
    "        # mean class-wise accuracies\n",
    "        mean_correct_class = [[] for i in range(tC)]\n",
    "        mean_correct_pred = [[] for i in range(tC)]\n",
    "        classwise_mean_acc = [[] for i in range(tC)]\n",
    "        for i in range(int(C*N)):\n",
    "          x = support_labels.index(query_labels[i])\n",
    "          mean_correct_class[x].append('4')\n",
    "          if(mean_pred_index[i] == x) :\n",
    "            mean_correct_pred[x].append('4')\n",
    "        for i in range(tC) :\n",
    "           z = len(mean_correct_pred[i])/len(mean_correct_class[i])\n",
    "           classwise_mean_acc[i].append(z)  \n",
    "        #std calculation\n",
    "        std = 0\n",
    "        for i in range(int(C*N)) :\n",
    "           x = support_labels.index(query_labels[i])\n",
    "           p_i = np.array([p[i,:] for p in mc_predictions])\n",
    "           std_i = tf.math.reduce_std(p_i,axis=0) \n",
    "           std_i_true = std_i[x]\n",
    "           std += std_i_true                                                    # adding std of each class\n",
    "        # print('std',std)\n",
    "        loss += MC_LOSS_WEIGHT*std \n",
    "        y = np.zeros((int(C*N),C))                                            # (150,10)\n",
    "        for i in range(int(C*N)) :\n",
    "            x = support_labels.index(query_labels[i])                           # creation of 1-hot for the true labels\n",
    "            y[i][x] = 1.                                                                \n",
    "        return loss, mc_predictions, mean_accuracy, classwise_mean_acc, y\n",
    "\n",
    "\n",
    "      def save(self, model_path):\n",
    "        self.encoder.save(model_path)\n",
    "\n",
    "      def load(self, model_path):\n",
    "        self.encoder(tf.zeros([1, self.w, self.h, self.c]))\n",
    "        self.encoder.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def createTrainingEpisode(patches:list, labels:list, K:int, C:int, N:int ):\n",
    "    \"\"\"\n",
    "    createTrainingEpisode creates a training episode for the N-way K-shot learning task.\n",
    "    \n",
    "    :param patches: list of all patches classified into different classes.\n",
    "    :param labels: list of classes from which the traning episode is to be created.\n",
    "    :param K: number of patches per class in the support set.\n",
    "    :param C: number of classes in the training episode.\n",
    "    :param N: number of patches per class in the query set.\n",
    "    :return queryPatches, queryLabels, supportPatches, supportLabels: training episode\n",
    "    \n",
    "    Algorithm:\n",
    "    - Select N classes from the list of labels. They should be unique.\n",
    "    - For each class, select K+Q patches. They should be unique.\n",
    "        - First K patches are support patches.\n",
    "        - Last Q patches are query patches.\n",
    "        - Append the support patches to supportPatches.\n",
    "        - Append the query patches to queryPatches.\n",
    "        - Append the class label to queryLabels Q times.\n",
    "    - Shuffle the queryPatches and queryLabels in the same order.\n",
    "    - Convert the queryPatches and supportPatches to tensors.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    selectedLabels = random.sample(labels, C)\n",
    "    supportPatches = []\n",
    "    supportLabels = list(selectedLabels)\n",
    "    queryPatches = []\n",
    "    queryLabels = []\n",
    "    \n",
    "    for n in selectedLabels:\n",
    "        sran_indices = np.random.choice(len(patches[n-1]),K,replace=False)  # for class no X-1: select K samples \n",
    "        supportPatches.extend( patches[n-1][sran_indices,:,:,:,:])\n",
    "        qran_indices = np.random.choice(len(patches[n-1]),N,replace=False)  # N Samples for Query\n",
    "        queryPatches.extend(patches[n-1][qran_indices,:,:,:,:])\n",
    "        queryLabels.extend([n]*N)\n",
    "    \n",
    "    shuffled = list(zip(queryPatches, queryLabels))\n",
    "    random.shuffle(shuffled)\n",
    "    queryPatches, queryLabels = zip(*shuffled)\n",
    "    \n",
    "    queryPatches = tf.convert_to_tensor(np.reshape(np.asarray(queryPatches),(C*N,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    supportPatches = tf.convert_to_tensor(np.reshape(np.asarray(supportPatches),(C*K,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    \n",
    "    return queryPatches, queryLabels, supportPatches, supportLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def train_step(support, query, support_labels, query_labels, K, C, N):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, mean_accuracy, mean_predictions = ProtoModel(support, query, support_labels, query_labels, K, C, N,N_TIMES,training=True)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # A gradient simply measures the change in all weights with regard to the change in error. You can also think of a gradient as the slope of a function. The higher the gradient, the steeper the slope and the faster a model can learn. But if the slope is zero, the model stops learning\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Log loss and accuracy for step\n",
    "    train_loss(loss)\n",
    "    train_acc(mean_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def trainingEpochs(patches, labels, n_epochs, n_episodes):\n",
    "    \"\"\"\n",
    "    trainingEpochs function trains the model for n_epochs and n_episodes.\n",
    "    \n",
    "    :param patches: image patches to be trained\n",
    "    :param labels: corresponding labels to be used\n",
    "    :param n_epochs: number of epochs\n",
    "    :param n_episodes: number of episodes\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    template = 'Epoch {}/{}, Episode {}/{}, Train Loss: {:.2f}, Train Accuracy: {:.2f}'\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss.reset_states()\n",
    "        train_acc.reset_states()\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            queryPatches, queryLabels, supportPatches, supportLabels = createTrainingEpisode(patches, labels, TRAIN_K, TRAIN_C, TRAIN_N)\n",
    "            train_step(supportPatches, queryPatches,supportLabels,  queryLabels, TRAIN_K, TRAIN_C, TRAIN_N)\n",
    "            trainingData.append([train_loss.result(),  train_acc.result()*100])\n",
    "            print(template.format(epoch+1, n_epochs, episode+1, n_episodes, train_loss.result(), train_acc.result()*100))\n",
    "            plotData(trainingData)\n",
    "        if(epoch and epoch % 5 == 0):\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix_train)    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def createTunningEpisodes(patches:list, labels:list, K:int, C:int, N:int):\n",
    "    \"\"\"\n",
    "    createTuningEpisodes creates a tuning episode for the N-way K-shot learning task.\n",
    "    \n",
    "    :param patches: list of all patches classified into different classes.\n",
    "    :param labels: list of classes from which the tuning episode is to be created.\n",
    "    :param K: number of patches per class in the support set.\n",
    "    :param C: number of classes in the tuning episode.\n",
    "    :param N: number of patches per class in the query set.\n",
    "    :return queryPatches, queryLabels, supportPatches, supportLabels: tuning episode\n",
    "    \n",
    "    Algorithm:\n",
    "    - Select C classes from the list of labels. They should be unique.\n",
    "    - For each selected class.\n",
    "        - Shuffle the patches of that class.\n",
    "        - First K patches are support patches.\n",
    "        - Next N patches are query patches. \n",
    "        - Append the support patches to supportPatches.\n",
    "        - Append the query patches to queryPatches.\n",
    "        - Append the class label to queryLabels N times.\n",
    "    - Shuffle the queryPatches and queryLabels in the same order.\n",
    "    - Convert the queryPatches and supportPatches to tensors.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    selected_classes = np.random.choice(labels,C,replace=False)\n",
    "    supportLabels  = list(selected_classes)\n",
    "    queryLabels = []\n",
    "    supportPatches = []\n",
    "    queryPatches = []\n",
    "    \n",
    "    for x in selected_classes :\n",
    "        y = labels.index(x)\n",
    "        np.random.shuffle(patches[y])    \n",
    "        supportPatches.extend(patches[y][:K,:,:,:,:])  # 1st K patches for support set\n",
    "        queryPatches.extend(patches[y][K:K+N,:,:,:,:])   # next N patches for query set\n",
    "        queryLabels.extend([x]*N)            \n",
    "          # next 5 labels for query set\n",
    "    \n",
    "    shuffled = list(zip(queryPatches, queryLabels))\n",
    "    random.shuffle(shuffled)\n",
    "    queryPatches, queryLabels = zip(*shuffled)\n",
    "    \n",
    "    queryPatches = tf.convert_to_tensor(np.reshape(np.asarray(queryPatches),(C*N,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    supportPatches = tf.convert_to_tensor(np.reshape(np.asarray(supportPatches),(C*K,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    \n",
    "    return queryPatches, queryLabels, supportPatches, supportLabels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def tune_step(support, query, support_labels, query_labels, K, C, N):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, mean_accuracy, mean_predictions = ProtoModel(support, query, support_labels, query_labels, K, C, N,N_TIMES,training=True)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Log loss and accuracy for step\n",
    "    tune_loss(loss)\n",
    "    tune_acc(mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def tunningEpochs(patches, labels, n_epochs, n_episodes):\n",
    "    \"\"\"\n",
    "    trainingEpochs function trains the model for n_epochs and n_episodes.\n",
    "    \n",
    "    :param patches: image patches to be trained\n",
    "    :param labels: corresponding labels to be used\n",
    "    :param n_epochs: number of epochs\n",
    "    :param n_episodes: number of episodes\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    template = 'Epoch {}/{}, Tune Loss: {:.2f}, Tune Accuracy: {:.2f}'\n",
    "\n",
    "    for epoch in range(n_epochs): \n",
    "        tune_loss.reset_states()  \n",
    "        tune_acc.reset_states()    \n",
    "        for epi in range(n_episodes+1):  \n",
    "            queryPatches, queryLabels, supportPatches, supportLabels = createTunningEpisodes(patches, labels, TUNE_K, TUNE_C, TUNE_N)    \n",
    "            tune_step(supportPatches, queryPatches,supportLabels, queryLabels, TUNE_K, TUNE_C, TUNE_N)      \n",
    "            \n",
    "        print(template.format(epoch+1, n_epochs,tune_loss.result(),tune_acc.result()*100))\n",
    "        tunningData.append([tune_loss.result(),  tune_acc.result()*100])\n",
    "        plotData(tunningData)\n",
    "        if (epoch+1)%5 == 0 :\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix_tune) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def createTestingEpisode(patches, labels, K, C, i, f):\n",
    "    selected_classes = labels[i:f]   # [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    support_labels = list(selected_classes)\n",
    "    query_labels = []\n",
    "    support_patches = []\n",
    "    query_patches = []\n",
    "    for x in selected_classes :\n",
    "        y = labels.index(x)\n",
    "        support_imgs = patches[y][:K,:,:,:,:]\n",
    "        query_imgs = patches[y][K:,:,:,:,:]\n",
    "        support_patches.extend(support_imgs)\n",
    "        query_patches.extend(query_imgs)\n",
    "        for i in range(query_imgs.shape[0]) :\n",
    "            query_labels.append(x)\n",
    "    temp1 = list(zip(query_patches, query_labels)) \n",
    "    random.shuffle(temp1) \n",
    "    query_patches, query_labels = zip(*temp1)\n",
    "    x = len(query_labels)\n",
    "    query_patches = tf.convert_to_tensor(np.reshape(np.asarray(query_patches),(x,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    support_patches = tf.convert_to_tensor(np.reshape(np.asarray(support_patches),(C*K,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    return query_patches, support_patches, query_labels, support_labels,x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def test_step(support, query, support_labels, query_labels, K, C, y):\n",
    "    loss, mc_predictions, mean_accuracy, classwise_mean_acc, y = ProtoModel(support, query, support_labels, query_labels, K, C, y,N_TIMES,training=False)\n",
    "    return loss, mc_predictions, mean_accuracy, classwise_mean_acc, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def testingEpochs(patches, labels, n_epochs):\n",
    "    \"\"\"\n",
    "    testingEpochs function tests the model for n_epochs.\n",
    "    \n",
    "    :param patches: image patches to be trained\n",
    "    :param labels: corresponding labels to be used\n",
    "    :param n_epochs: number of epochs\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for epoch in range(n_epochs):\n",
    "        test_loss.reset_states()  \n",
    "        test_acc.reset_states()     \n",
    "        \n",
    "        tquery_patches1, tsupport_patches1, query_labels1, support_labels1, x1 = createTestingEpisode(patches,labels,TEST_K,TEST_C,0,3)    \n",
    "        loss1, mc_predictions1, mean_accuracy1, classwise_mean_acc1, y1 = test_step(tsupport_patches1, tquery_patches1,support_labels1, query_labels1, TEST_K, TEST_C, y=x1/3) \n",
    "        tquery_patches2, tsupport_patches2, query_labels2, support_labels2, x2 = createTestingEpisode(patches,labels,TEST_K,TEST_C,3,6)    \n",
    "        loss2, mc_predictions2, mean_accuracy2, classwise_mean_acc2, y2 = test_step(tsupport_patches2, tquery_patches2,support_labels2, query_labels2, 5, 3, x2/3)\n",
    "        print(\"=========================================\")\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "        print(\"-----------------------------------------\")\n",
    "        print(f\"Overall Accuracy 1 (OA1): {mean_accuracy1}\")\n",
    "        # Class Wise Accuracy\n",
    "        for i in range(TEST_C):\n",
    "            print(f\"Class {i+1} Accuracy: {classwise_mean_acc1[i]}\")\n",
    "        print(f\"Loss: {loss1.numpy():.3f}\")\n",
    "        print(\"-----------------------------------------\")\n",
    "        print(f\"Overall Accuracy 2 (OA2): {mean_accuracy2}\")\n",
    "        # Class Wise Accuracy\n",
    "        for i in range(TEST_C):\n",
    "            print(f\"Class {i+1+TEST_C} Accuracy: {classwise_mean_acc2[i]}\")\n",
    "        print(f\"Loss: {loss2.numpy():.3f}\")\n",
    "        print(\"=========================================\")\n",
    "        \n",
    "        testingData.append([mean_accuracy1*100, mean_accuracy2*100, loss1.numpy(), loss2.numpy()])\n",
    "        plotData(testingData, testing=True)\n",
    "        \n",
    "    return mc_predictions1, mc_predictions2, y1, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def calculateAccuracy(mc_predictions1, mc_predictions2, y1, y2):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of the model, given the model's predictions and the ground truth labels.\n",
    "    \"\"\"\n",
    "    mean_predictions1 =  tf.reduce_mean(mc_predictions1,axis=0)\n",
    "    mean_predictions2 =  tf.reduce_mean(mc_predictions2,axis=0)\n",
    "    overall_predictions = tf.concat([mean_predictions1,mean_predictions2],axis=0)\n",
    "    overall_true_labels = tf.concat([y1,y2],axis=0)\n",
    "    correct_pred = tf.cast(tf.equal(                                             # accuracy for the current pass\n",
    "                tf.cast(tf.argmax(overall_predictions, axis=-1), tf.int32), \n",
    "                tf.cast(tf.argmax(overall_true_labels,axis=-1), tf.int32)), tf.float32)\n",
    "    o_acc = tf.reduce_mean(correct_pred) \n",
    "    print(f\"Overall accuracy:{o_acc.numpy():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def calculateConfusionMatrix(mc_predictions1, mc_predictions2, y1, y2):\n",
    "    mean_predictions1 =  tf.reduce_mean(mc_predictions1,axis=0)\n",
    "    cm_pred1 = tf.argmax(mean_predictions1, axis=-1)\n",
    "    mean_predictions2 =  tf.reduce_mean(mc_predictions2,axis=0)\n",
    "    cm_pred2 = tf.argmax(mean_predictions2, axis=-1) + 3\n",
    "    overall_predictions = tf.concat([cm_pred1,cm_pred2],axis=0)\n",
    "    cm_true1 = tf.argmax(y1,axis=-1)\n",
    "    cm_true2 = tf.argmax(y2,axis=-1) + 3\n",
    "    overall_true_labels = tf.concat([cm_true1,cm_true2],axis=0)\n",
    "    results = confusion_matrix(overall_true_labels,overall_predictions) \n",
    "    print ('Confusion Matrix :')\n",
    "    print(results) \n",
    "    print ('Report : ')\n",
    "    print (classification_report(overall_true_labels, overall_predictions))    \n",
    "    return overall_true_labels, overall_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen Kappa Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def calculateKappaScore(overall_true_labels, overall_predictions):\n",
    "    print(\"Cohen's Kappa Score: \", cohen_kappa_score(overall_true_labels, overall_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "X, Y = loadData(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Apply PCA to reduce dimensionality of the data\n",
    "X_pca = applyPCA(X, n_components=PCA_COMPONENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m checkpoint \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mCheckpoint(optimizer\u001b[39m=\u001b[39moptimizer, ProtoModel \u001b[39m=\u001b[39m ProtoModel)\n\u001b[0;32m     30\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m trainingEpochs(patches, TRAINING_LABELS, TRAINING_EPOCH, TRAINING_EPISODE)\n\u001b[0;32m     33\u001b[0m \u001b[39m# TUNNING_PATCHES = [TESTING_PATCHES[i][:5,:,:,:,:] for i in range(6)]\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[39m# tunningEpochs(TUNNING_PATCHES, TESTING_LABELS, TUNNING_EPOCH, TUNNING_EPISODE)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m mc_predictions1, mc_predictions2, y1, y2 \u001b[39m=\u001b[39m  testingEpochs(TESTING_PATCHES, TESTING_LABELS, TESTING_EPOCH)\n",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m, in \u001b[0;36mtimeIt.<locals>.wrap_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_func\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     11\u001b[0m     t1 \u001b[39m=\u001b[39m time()\n\u001b[1;32m---> 12\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     13\u001b[0m     t2 \u001b[39m=\u001b[39m time()\n\u001b[0;32m     14\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFunction \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m executed in \u001b[39m\u001b[39m{\u001b[39;00m(t2\u001b[39m-\u001b[39mt1)\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[37], line 22\u001b[0m, in \u001b[0;36mtrainingEpochs\u001b[1;34m(patches, labels, n_epochs, n_episodes)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_episodes):\n\u001b[0;32m     21\u001b[0m     queryPatches, queryLabels, supportPatches, supportLabels \u001b[39m=\u001b[39m createTrainingEpisode(patches, labels, TRAIN_K, TRAIN_C, TRAIN_N)\n\u001b[1;32m---> 22\u001b[0m     train_step(supportPatches, queryPatches,supportLabels,  queryLabels, TRAIN_K, TRAIN_C, TRAIN_N)\n\u001b[0;32m     23\u001b[0m     trainingData\u001b[39m.\u001b[39mappend([train_loss\u001b[39m.\u001b[39mresult(),  train_acc\u001b[39m.\u001b[39mresult()\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m])\n\u001b[0;32m     24\u001b[0m     \u001b[39m# clear_output(wait=True)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(support, query, support_labels, query_labels, K, C, N)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m      4\u001b[0m     loss, mean_accuracy, mean_predictions \u001b[39m=\u001b[39m ProtoModel(support, query, support_labels, query_labels, K, C, N,N_TIMES,training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss, model\u001b[39m.\u001b[39mtrainable_variables)\n\u001b[0;32m      7\u001b[0m \u001b[39m# A gradient simply measures the change in all weights with regard to the change in error. You can also think of a gradient as the slope of a function. The higher the gradient, the steeper the slope and the faster a model can learn. But if the slope is zero, the model stops learning\u001b[39;00m\n\u001b[0;32m      9\u001b[0m optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(gradients, model\u001b[39m.\u001b[39mtrainable_variables))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1063\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1057\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[0;32m   1058\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1059\u001b[0m           output_gradients))\n\u001b[0;32m   1060\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1061\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[1;32m-> 1063\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39mimperative_grad(\n\u001b[0;32m   1064\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape,\n\u001b[0;32m   1065\u001b[0m     flat_targets,\n\u001b[0;32m   1066\u001b[0m     flat_sources,\n\u001b[0;32m   1067\u001b[0m     output_gradients\u001b[39m=\u001b[39moutput_gradients,\n\u001b[0;32m   1068\u001b[0m     sources_raw\u001b[39m=\u001b[39mflat_sources_raw,\n\u001b[0;32m   1069\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39munconnected_gradients)\n\u001b[0;32m   1071\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[0;32m   1072\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_TapeGradient(\n\u001b[0;32m     68\u001b[0m     tape\u001b[39m.\u001b[39m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     target,\n\u001b[0;32m     70\u001b[0m     sources,\n\u001b[0;32m     71\u001b[0m     output_gradients,\n\u001b[0;32m     72\u001b[0m     sources_raw,\n\u001b[0;32m     73\u001b[0m     compat\u001b[39m.\u001b[39mas_str(unconnected_gradients\u001b[39m.\u001b[39mvalue))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:146\u001b[0m, in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    144\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[1;32m--> 146\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n\u001b[0;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py:164\u001b[0m, in \u001b[0;36m_Conv3DGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    153\u001b[0m data_format \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39mdata_format\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mdecode()\n\u001b[0;32m    154\u001b[0m shape_0, shape_1 \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39mshape_n([op\u001b[39m.\u001b[39minputs[\u001b[39m0\u001b[39m], op\u001b[39m.\u001b[39minputs[\u001b[39m1\u001b[39m]])\n\u001b[0;32m    155\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    156\u001b[0m     nn_ops\u001b[39m.\u001b[39mconv3d_backprop_input_v2(\n\u001b[0;32m    157\u001b[0m         shape_0,\n\u001b[0;32m    158\u001b[0m         op\u001b[39m.\u001b[39minputs[\u001b[39m1\u001b[39m],\n\u001b[0;32m    159\u001b[0m         grad,\n\u001b[0;32m    160\u001b[0m         dilations\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39mdilations\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    161\u001b[0m         strides\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39mstrides\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    162\u001b[0m         padding\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39mpadding\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    163\u001b[0m         data_format\u001b[39m=\u001b[39mdata_format),\n\u001b[1;32m--> 164\u001b[0m     nn_ops\u001b[39m.\u001b[39mconv3d_backprop_filter_v2(\n\u001b[0;32m    165\u001b[0m         op\u001b[39m.\u001b[39minputs[\u001b[39m0\u001b[39m],\n\u001b[0;32m    166\u001b[0m         shape_1,\n\u001b[0;32m    167\u001b[0m         grad,\n\u001b[0;32m    168\u001b[0m         dilations\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39mdilations\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    169\u001b[0m         strides\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39mstrides\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    170\u001b[0m         padding\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39mpadding\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    171\u001b[0m         data_format\u001b[39m=\u001b[39mdata_format)\n\u001b[0;32m    172\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:2394\u001b[0m, in \u001b[0;36mconv3d_backprop_filter_v2\u001b[1;34m(input, filter_sizes, out_backprop, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   2392\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   2393\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2394\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_FastPathExecute(\n\u001b[0;32m   2395\u001b[0m       _ctx, \u001b[39m\"\u001b[39m\u001b[39mConv3DBackpropFilterV2\u001b[39m\u001b[39m\"\u001b[39m, name, \u001b[39minput\u001b[39m, filter_sizes,\n\u001b[0;32m   2396\u001b[0m       out_backprop, \u001b[39m\"\u001b[39m\u001b[39mstrides\u001b[39m\u001b[39m\"\u001b[39m, strides, \u001b[39m\"\u001b[39m\u001b[39mpadding\u001b[39m\u001b[39m\"\u001b[39m, padding, \u001b[39m\"\u001b[39m\u001b[39mdata_format\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2397\u001b[0m       data_format, \u001b[39m\"\u001b[39m\u001b[39mdilations\u001b[39m\u001b[39m\"\u001b[39m, dilations)\n\u001b[0;32m   2398\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   2399\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# clear_output(wait=True)\n",
    "\n",
    "\n",
    "# Apply PCA to reduce dimensionality of the data\n",
    "X_pca = applyPCA(X, n_components=PCA_COMPONENTS)\n",
    "\n",
    "# Create Image Cubes from data, and labels\n",
    "X_cubes, Y_cubes = createImageCubes(X_pca, Y, windowSize=WINDOW_SIZE)\n",
    "\n",
    "# Creates class wise patches from the image cubes\n",
    "patches = classWisePatches(X_cubes, Y_cubes)\n",
    "\n",
    "# Seperate the patches into train and test sets\n",
    "TRAINING_PATCHES: list = [patches[i] for i in TRAINING_CLASSES]\n",
    "TESTING_PATCHES: list = [patches[i] for i in TESTING_CLASSES]\n",
    "\n",
    "# Create instance of the model\n",
    "model = createModel()\n",
    "\n",
    "# Create instance of the Prototypical Network\n",
    "ProtoModel = Prototypical(model, IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH, IMAGE_CHANNEL)\n",
    "\n",
    "# Create instance of the Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Create instance of the Checkpoint\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, ProtoModel = ProtoModel)\n",
    "\n",
    "# Train the model\n",
    "trainingEpochs(patches, TRAINING_LABELS, TRAINING_EPOCH, TRAINING_EPISODE)\n",
    "\n",
    "TUNNING_PATCHES = [TESTING_PATCHES[i][:5,:,:,:,:] for i in range(6)]\n",
    "\n",
    "tunningEpochs(TUNNING_PATCHES, TESTING_LABELS, TUNNING_EPOCH, TUNNING_EPISODE)\n",
    "\n",
    "mc_predictions1, mc_predictions2, y1, y2 =  testingEpochs(TESTING_PATCHES, TESTING_LABELS, TESTING_EPOCH)\n",
    "\n",
    "calculateAccuracy(mc_predictions1, mc_predictions2, y1, y2)\n",
    "\n",
    "overall_true_labels, overall_predictions = calculateConfusionMatrix(mc_predictions1, mc_predictions2, y1, y2)\n",
    "\n",
    "calculateKappaScore(overall_true_labels, overall_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
