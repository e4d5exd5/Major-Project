{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6491,
     "status": "ok",
     "timestamp": 1675333348365,
     "user": {
      "displayName": "Mayank Kantharia",
      "userId": "02386211270688916554"
     },
     "user_tz": -330
    },
    "id": "IawStAMhtY0q",
    "outputId": "1a5d6537-c37e-44de-8d0e-c74c98f9529a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5541,
     "status": "ok",
     "timestamp": 1675333353902,
     "user": {
      "displayName": "Mayank Kantharia",
      "userId": "02386211270688916554"
     },
     "user_tz": -330
    },
    "id": "SctI2LsOt3dh",
    "outputId": "25af520c-97d8-4b01-a79a-b8ef3f36c49b"
   },
   "outputs": [],
   "source": [
    "!pip install easyfsl tensorflow sklearn numpy matplotlib scipy tensorflow_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1675333353903,
     "user": {
      "displayName": "Mayank Kantharia",
      "userId": "02386211270688916554"
     },
     "user_tz": -330
    },
    "id": "xi7TOrG8tO6X",
    "outputId": "0bd4ef5f-bdd0-40e6-b173-fbe1b7509466"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras import Sequential,layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import statistics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
    "import tensorflow_probability as tfp\n",
    "from operator import truediv\n",
    "from tensorflow.compat.v1.distributions import Bernoulli\n",
    "from plotly.offline import init_notebook_mode\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import os\n",
    "from tensorflow.python.keras import backend as K\n",
    "# import spectral\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dVy7URxfN6tc"
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "im_width, im_height, im_depth, im_channel = 11,11,30, 1 # im -> image\n",
    "mc_loss_weight = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaFcFKyBN6tf"
   },
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lmFAiil_N6tf"
   },
   "outputs": [],
   "source": [
    "def calc_euclidian_dists(x, y):\n",
    "    n = x.shape[0]\n",
    "    m = y.shape[0]\n",
    "    x = tf.tile(tf.expand_dims(x, 1), [1, m, 1])\n",
    "    y = tf.tile(tf.expand_dims(y, 0), [n, 1, 1])\n",
    "    return tf.reduce_mean(tf.math.pow(x - y, 2), 2)    \n",
    "                                  #   ^^^^^^^^  ^^ -> axis \n",
    "                                  #      x-y -> subtracts each element of x from y\n",
    "                                  #     2 -> square of the difference ^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n",
      "(2, 3)\n",
      "(3, 3)\n",
      "tf.Tensor(\n",
      "[[[1 2 3]\n",
      "  [1 2 3]\n",
      "  [1 2 3]]\n",
      "\n",
      " [[4 5 6]\n",
      "  [4 5 6]\n",
      "  [4 5 6]]], shape=(2, 3, 3), dtype=int32)\n",
      "(2, 3, 3)\n",
      "tf.Tensor(\n",
      "[[[1 2 3]\n",
      "  [4 5 6]\n",
      "  [7 8 9]]\n",
      "\n",
      " [[1 2 3]\n",
      "  [4 5 6]\n",
      "  [7 8 9]]], shape=(2, 3, 3), dtype=int32)\n",
      "(2, 3, 3)\n",
      "tf.Tensor(\n",
      "[[[ 0  0  0]\n",
      "  [-3 -3 -3]\n",
      "  [-6 -6 -6]]\n",
      "\n",
      " [[ 3  3  3]\n",
      "  [ 0  0  0]\n",
      "  [-3 -3 -3]]], shape=(2, 3, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[ 0  0  0]\n",
      "  [ 9  9  9]\n",
      "  [36 36 36]]\n",
      "\n",
      " [[ 9  9  9]\n",
      "  [ 0  0  0]\n",
      "  [ 9  9  9]]], shape=(2, 3, 3), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[ 0,  9, 36],\n",
       "       [ 9,  0,  9]])>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[1,2,3],[4,5,6]]) 2 \n",
    "b = tf.constant([[1,2,3],[4,5,6],[7,8,9]]) 3 \n",
    "calc_euclidian_dists(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGfdaFk4KNE6"
   },
   "outputs": [],
   "source": [
    "def calc_euclidian_dists(x, y):\n",
    "    n = x.shape[0]\n",
    "    m = y.shape[0]\n",
    "    x = tf.tile(tf.expand_dims(x, 1), [1, m, 1])\n",
    "    y = tf.tile(tf.expand_dims(y, 0), [n, 1, 1])\n",
    "    return tf.reduce_mean(tf.math.pow(x - y, 2), 2)    \n",
    "\n",
    "def _bernoulli(shape, mean):\n",
    "    return tf.nn.relu(tf.sign(mean - tf.random.uniform(shape, minval=0, maxval=1, dtype=tf.float32)))\n",
    "\n",
    "class DropBlock3D(tf.keras.layers.Layer):\n",
    "    def __init__(self, keep_prob, block_size, scale=True, **kwargs):\n",
    "        super(DropBlock3D, self).__init__(**kwargs)\n",
    "        self.keep_prob = float(keep_prob) if isinstance(keep_prob, int) else keep_prob\n",
    "        self.block_size = int(block_size)\n",
    "        self.scale = tf.constant(scale, dtype=tf.bool) if isinstance(scale, bool) else scale\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 5\n",
    "        _, self.d, self.h, self.w, self.channel = input_shape.as_list()\n",
    "        # pad the mask\n",
    "        p0 = (self.block_size - 1) // 2\n",
    "        p1 = (self.block_size - 1) - p0\n",
    "        self.padding = [[0, 0], [p0, p1], [p0, p1], [p0, p1], [0, 0]]\n",
    "        self.set_keep_prob()\n",
    "        super(DropBlock3D, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=True, **kwargs):\n",
    "        def drop():\n",
    "            mask = self._create_mask(tf.shape(inputs))\n",
    "            output = inputs * mask\n",
    "            output = tf.cond(self.scale,\n",
    "                             true_fn=lambda: output * tf.compat.v1.to_float(tf.size(mask)) / tf.reduce_sum(mask),\n",
    "                             false_fn=lambda: output)\n",
    "            return output\n",
    "\n",
    "        if training is None:\n",
    "            training = K.learning_phase()\n",
    "        output = tf.cond(tf.logical_or(tf.logical_not(training), tf.equal(self.keep_prob, 1.0)),\n",
    "                         true_fn=lambda: inputs,\n",
    "                         false_fn=drop)\n",
    "        return output\n",
    "\n",
    "    def set_keep_prob(self, keep_prob=None):\n",
    "        \"\"\"This method only supports Eager Execution\"\"\"\n",
    "        if keep_prob is not None:\n",
    "            self.keep_prob = keep_prob\n",
    "        d, w, h = tf.compat.v1.to_float(self.d), tf.compat.v1.to_float(self.w), tf.compat.v1.to_float(self.h)\n",
    "        self.gamma = ((1. - self.keep_prob) * (d * w * h) / (self.block_size ** 3) /\n",
    "                      ((d - self.block_size + 1) * (w - self.block_size + 1) * (h - self.block_size + 1)))\n",
    "\n",
    "    def _create_mask(self, input_shape):\n",
    "        sampling_mask_shape = tf.stack([input_shape[0],\n",
    "                                        self.d - self.block_size + 1,\n",
    "                                        self.h - self.block_size + 1,\n",
    "                                        self.w - self.block_size + 1,\n",
    "                                        self.channel])\n",
    "        mask = _bernoulli(sampling_mask_shape, self.gamma)\n",
    "        mask = tf.pad(mask, self.padding)\n",
    "        mask = tf.nn.max_pool3d(mask, [1, self.block_size, self.block_size, self.block_size, 1], [1, 1, 1, 1, 1], 'SAME')\n",
    "        mask = 1 - mask\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1451,
     "status": "ok",
     "timestamp": 1675333355346,
     "user": {
      "displayName": "Mayank Kantharia",
      "userId": "02386211270688916554"
     },
     "user_tz": -330
    },
    "id": "215a8XhEN6ti",
    "outputId": "472f7931-f8c2-45a2-e708-a6f4d82c07bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 11, 11, 30, 1)]   0         \n",
      "                                                                 \n",
      " conv3d (Conv3D)             (None, 11, 11, 30, 8)     512       \n",
      "                                                                 \n",
      " drop_block3d (DropBlock3D)  (None, 11, 11, 30, 8)     0         \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 11, 11, 30, 16)    5776      \n",
      "                                                                 \n",
      " drop_block3d_1 (DropBlock3  (None, 11, 11, 30, 16)    0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 9, 9, 28, 32)      13856     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 9, 9, 896)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 7, 7, 64)          516160    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               803072    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1372272 (5.23 MB)\n",
      "Trainable params: 1372272 (5.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = layers.Input(shape = (im_height, im_width, im_depth, im_channel))\n",
    "out1 = layers.Conv3D(filters=8, kernel_size=(3,3,7), activation='relu',input_shape=(im_height, im_width, im_depth, im_channel),padding='same')(input_layer)\n",
    "out1 = DropBlock3D(0.7,3)(out1) \n",
    "out2 = layers.Conv3D(filters=16, kernel_size=(3,3,5), activation='relu',padding='same')(out1) # no of prams=16*(3*3*5*8 + 1)=2880\n",
    "out2 = DropBlock3D(0.7,3)(out2)\n",
    "out3 = layers.Conv3D(filters=32, kernel_size=(3,3,3), activation= 'relu')(out2)\n",
    "out3 = layers.Reshape((out3.shape[1], out3.shape[2], out3.shape[3]*out3.shape[4]))(out3)\n",
    "out3 = layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu')(out3)\n",
    "out4 = layers.Flatten()(out3)\n",
    "out4 = layers.Dropout(0.4)(out4, training=True)\n",
    "out4 = layers.Dense(256, activation='relu')(out4)  # output = activation(dot(input, kernel) + bias)\n",
    "out5 = layers.Dropout(0.4)(out4,training=True) \n",
    "out5 = layers.Dense(128, activation='relu')(out5)\n",
    "#out5 = layers.Dropout(0.4)(out5, training=True)\n",
    "#out6 = layers.Dense(16, activation='relu')(out5)\n",
    "model = Model(inputs=input_layer,outputs=out5) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wAOZ0z1rN6to"
   },
   "outputs": [],
   "source": [
    "class Prototypical(Model):\n",
    "    def __init__(self, model, w, h, d, c):\n",
    "        super(Prototypical, self).__init__()\n",
    "        self.w, self.h, self.d, self.c = w, h, d, c\n",
    "        self.encoder = model\n",
    "\n",
    "    def call(self, support, query, support_labels, query_labels, K, C, N,n_times,training=True):\n",
    "      # support : support images (25, 11, 11, 30, 1)\n",
    "      # query : query images (75, 11, 11, 30, 1)\n",
    "    \n",
    "      n_class = C                                                               #5\n",
    "      n_support = K                                                             #5\n",
    "      n_query = N                                                               #15 \n",
    "\n",
    "      if training == True : \n",
    "        loss = 0\n",
    "        mc_predictions = []                                                     # list of predictions for multiple passes\n",
    "        for i in range(n_times) :     \n",
    "          y = np.zeros((int(C*N),C))                                              #(75, 5)\n",
    "          \n",
    "          \n",
    "          for i in range(int(C*N)) :                                             # 75\n",
    "            x = support_labels.index(query_labels[i])                           # creation of 1-hot for true labels\n",
    "            y[i][x] = 1.                                               # n_times passing every query image for calculating variance \n",
    "            #  basically we are creating OMR sheet for each query image where each column represents the class and each row represents the query image, where the true class is 1 and rest are 0\n",
    "            \n",
    "          cat = tf.concat([support,query], axis=0)       \n",
    "          print('cat', cat.shape, cat)                                       # [100, 11, 11, 30, 1]\n",
    "          z = self.encoder(cat)    \n",
    "          print('z', z.shape, z)                                             # [100, 128]   # build a new computational graph from the provided inputs\n",
    "          # Divide embedding into support and query\n",
    "          z_prototypes = tf.reshape(z[:n_class * n_support],[n_class, n_support, z.shape[-1]])   #[5, 5, 128])\n",
    "          print('z_p', z_prototypes.shape, z_prototypes)   \n",
    "          # Prototypes are means of n_support examples\n",
    "          z_prototypes = tf.math.reduce_mean(z_prototypes, axis=1)              #[5, 128]\n",
    "          print('z_p', z_prototypes.shape, z_prototypes)   \n",
    "          z_query = z[n_class * n_support:]                                     #[75, 182]                         \n",
    "          print('z_q', z_query.shape, z_query)   \n",
    "          # Calculate distances between query and prototypes\n",
    "          dists = calc_euclidian_dists(z_query, z_prototypes)                   #[75, 5]\n",
    "          print('dist', dists)   \n",
    "          # log softmax of calculated distances\n",
    "          log_p_y = tf.nn.log_softmax(-dists, axis=-1)                          #[75, 5]     this activation function heavily penalizes wrong class prediction as compared to its Softmax counterpart    \n",
    "          print('log', log_p_y)   \n",
    "          loss1 = -tf.reduce_mean((tf.reduce_sum(tf.multiply(y, log_p_y), axis=-1)))   #loss for the current pass                     \n",
    "          print('loss1', loss1)                                                 # []\n",
    "          loss += loss1                                                         # adding loss for each pass                   \n",
    "          predictions = tf.nn.softmax(-dists, axis=-1)                         # [75, 5] prediction probability for the search-space classes per query image(for current pass)\n",
    "          print('pred', predictions)   \n",
    "          mc_predictions.append(predictions)                                          \n",
    "\n",
    "        y = np.zeros((int(C*N),C))\n",
    "        for i in range(int(C*N)) :\n",
    "            x = support_labels.index(query_labels[i])                           # creation of 1-hot for true labels\n",
    "            y[i][x] = 1. \n",
    "        mc_predictions = tf.convert_to_tensor(np.reshape(np.asarray(mc_predictions),(n_times,int(C*N),C)))  #(n_times,75,5)\n",
    "        std_predictions = tf.math.reduce_std(mc_predictions,axis=0)                                         # (75,5)\n",
    "        std = tf.reduce_sum(tf.reduce_sum(tf.multiply(std_predictions,y),axis=1))\n",
    "        print('std', std)        \n",
    "        loss += mc_loss_weight*std\n",
    "        \n",
    "        # calculating mean accuracy\n",
    "        mean_predictions = tf.reduce_mean(mc_predictions,axis=0)                # mean prediction probability for each class (75,5)\n",
    "        mean_eq = tf.cast(tf.equal(                                             # accuracy for the current pass  c\n",
    "            tf.cast(tf.argmax(mean_predictions, axis=-1), tf.int32),            # check if the index of max probability is equal to the true class index\n",
    "            tf.cast(tf.argmax(y,axis=-1), tf.int32)), tf.float32)               # argmax returns the index of max probability\n",
    "        mean_accuracy = tf.reduce_mean(mean_eq)\n",
    "        mean_predictions = tf.reduce_mean(mc_predictions,axis=0)                # mean prediction probability for each class (5)\n",
    "        return loss, mean_accuracy, mean_predictions   \n",
    "      \n",
    "      if training == False :\n",
    "        loss = 0\n",
    "        mc_predictions = []                                                     # list of predictions for multiple passes  \n",
    "        for i in range(n_times) :                                               # n_times passing the query images for variance calculation\n",
    "          y = np.zeros((int(C*N),C))                                            # (150,10)\n",
    "          for i in range(int(C*N)) :\n",
    "            x = support_labels.index(query_labels[i])                           # creation of 1-hot for the true labels\n",
    "            y[i][x] = 1.  \n",
    "          # merge support and query to forward through encoder\n",
    "          cat = tf.concat([support,query], axis=0)                              # [200,9,9,20,1]   \n",
    "          z = self.encoder(cat)                                                 # [200, 320]\n",
    "          # Divide embedding into support and query\n",
    "          z_prototypes = tf.reshape(z[:n_class * n_support],[n_class, n_support, z.shape[-1]])   #[10, 5, 320])\n",
    "          # Prototypes are means of n_support examples\n",
    "          z_prototypes = tf.math.reduce_mean(z_prototypes, axis=1)              #[10, 320]\n",
    "          z_query = z[n_class * n_support:]                                     #[150, 320]                         \n",
    "          # Calculate distances between query and prototypes\n",
    "          dists = calc_euclidian_dists(z_query, z_prototypes)                   #[150, 10]\n",
    "          # log softmax of calculated distances\n",
    "          log_p_y = tf.nn.log_softmax(-dists, axis=-1)                          #[150, 10]        \n",
    "          loss1 = -tf.reduce_mean((tf.reduce_sum(tf.multiply(y, log_p_y), axis=-1)))        \n",
    "          loss += loss1\n",
    "          predictions = tf.nn.softmax(-dists, axis=-1)                                 # prediction probabilities for the classes for current pass\n",
    "          mc_predictions.append(predictions)                                             \n",
    "        y = np.zeros((int(C*N),C))                                            # (150,10)\n",
    "        for i in range(int(C*N)) :\n",
    "            x = support_labels.index(query_labels[i])                           # creation of 1-hot for the true labels\n",
    "            y[i][x] = 1.  \n",
    "        mean_predictions = tf.reduce_mean(mc_predictions,axis=0)                # mean prediction probability for each class (150,10)\n",
    "        mean_eq = tf.cast(tf.equal(                                             # accuracy for the current pass\n",
    "            tf.cast(tf.argmax(mean_predictions, axis=-1), tf.int32), \n",
    "            tf.cast(tf.argmax(y,axis=-1), tf.int32)), tf.float32)\n",
    "        mean_accuracy = tf.reduce_mean(mean_eq)\n",
    "        mean_pred_index = tf.argmax(mean_predictions,axis=1)\n",
    "        # mean class-wise accuracies\n",
    "        mean_correct_class = [[] for i in range(tC)]\n",
    "        mean_correct_pred = [[] for i in range(tC)]\n",
    "        classwise_mean_acc = [[] for i in range(tC)]\n",
    "        for i in range(int(C*N)):\n",
    "          x = support_labels.index(query_labels[i])\n",
    "          mean_correct_class[x].append('4')\n",
    "          if(mean_pred_index[i] == x) :\n",
    "            mean_correct_pred[x].append('4')\n",
    "        for i in range(tC) :\n",
    "           z = len(mean_correct_pred[i])/len(mean_correct_class[i])\n",
    "           classwise_mean_acc[i].append(z)  \n",
    "        #std calculation\n",
    "        std = 0\n",
    "        for i in range(int(C*N)) :\n",
    "           x = support_labels.index(query_labels[i])\n",
    "           p_i = np.array([p[i,:] for p in mc_predictions])\n",
    "           std_i = tf.math.reduce_std(p_i,axis=0) \n",
    "           std_i_true = std_i[x]\n",
    "           std += std_i_true                                                    # adding std of each class\n",
    "        print('std',std)\n",
    "        loss += mc_loss_weight*std \n",
    "        y = np.zeros((int(C*N),C))                                            # (150,10)\n",
    "        for i in range(int(C*N)) :\n",
    "            x = support_labels.index(query_labels[i])                           # creation of 1-hot for the true labels\n",
    "            y[i][x] = 1.                                                                \n",
    "        return loss, mc_predictions, mean_accuracy, classwise_mean_acc, y\n",
    "\n",
    "\n",
    "      def save(self, model_path):\n",
    "        self.encoder.save(model_path)\n",
    "\n",
    "      def load(self, model_path):\n",
    "        self.encoder(tf.zeros([1, self.w, self.h, self.c]))\n",
    "        self.encoder.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX2P_sAOFevh"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "m3ipb9FLtO6c"
   },
   "outputs": [],
   "source": [
    "def loadData(name):\n",
    "    data = sio.loadmat('E:\\Engginearing\\SAKEC\\SEM 6\\Major Project\\Data\\Indian_pines_corrected.mat')['indian_pines_corrected']\n",
    "    labels = sio.loadmat('E:\\Engginearing\\SAKEC\\SEM 6\\Major Project\\Data\\Indian_pines_gt.mat')['indian_pines_gt']\n",
    "    return data, labels\n",
    "# without reduction of 200 channels to 30 channels, memory error while creating cube \n",
    "def applyPCA(X, numComponents):\n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=numComponents, whiten=True)\n",
    "    newX = pca.fit_transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
    "    return newX, pca\n",
    "\n",
    "def padWithZeros(X, margin):\n",
    "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "    return newX\n",
    "\n",
    "def createImageCubes(X, y, windowSize, removeZeroLabels = True):\n",
    "    margin = int((windowSize - 1) / 2)\n",
    "    zeroPaddedX = padWithZeros(X, margin=margin)  # X :(145, 145, 30) --> (155, 155, 30) with window =11\n",
    "    # split patches\n",
    "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))  # (21025, 25, 25, 30)   \n",
    "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))  # (21025,)\n",
    "    patchIndex = 0\n",
    "    \n",
    "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]  \n",
    "            patchesData[patchIndex, :, :, :] = patch\n",
    "            patchesLabels[patchIndex] = y[r-margin, c-margin]            \n",
    "            patchIndex = patchIndex + 1\n",
    "  \n",
    "    patchesData = np.expand_dims(patchesData, axis=-1)\n",
    "    return patchesData,patchesLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 634,
     "status": "ok",
     "timestamp": 1675333355976,
     "user": {
      "displayName": "Mayank Kantharia",
      "userId": "02386211270688916554"
     },
     "user_tz": -330
    },
    "id": "_iyX4iTQUOpT",
    "outputId": "71b63c48-8d0e-440c-c0e1-fd9acbdeb05f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21025, 11, 11, 30, 1) (21025,)\n"
     ]
    }
   ],
   "source": [
    "dataset1 = 'IP'                                         # 16 classes   \n",
    "ip_x1, ip_y = loadData(dataset1)                              #((512, 217, 204), (512, 217))\n",
    "ip_x2,pca = applyPCA(ip_x1,numComponents=30)                   # ((512, 217, 20), (512, 217))\n",
    "ip_X,ip_Y = createImageCubes(ip_x2, ip_y, windowSize=11)   #(111104, 9, 9, 20, 1) (111104,)\n",
    "print(ip_X.shape,ip_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BMY1DbyVZtV1"
   },
   "outputs": [],
   "source": [
    "def patches_class(X,Y,n) :\n",
    "  n_classes = n\n",
    "  patches_list = []\n",
    "  for i in range(1,n_classes+1):   # not considering class 0\n",
    "    patchesData_Ith_Label = X[Y==i,:,:,:,:]\n",
    "    patches_list.append(patchesData_Ith_Label)\n",
    "  return patches_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NBjLuxWvxMBG"
   },
   "outputs": [],
   "source": [
    "patches_class_ip = patches_class(ip_X,ip_Y,16) # class_wise list of patches #(16,) for class 0: (2009, 9, 9, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nzvKxHabyVJQ"
   },
   "outputs": [],
   "source": [
    "train_class_indices = [1,2,4,5,7,9,10,11,13,14]\n",
    "test_class_indices = [0,3,6,8,12,15]\n",
    "train_patches_class = [patches_class_ip[i] for i in train_class_indices]        #(10)\n",
    "test_patches_class = [patches_class_ip[i] for i in test_class_indices]        #(6) \n",
    "train_class_labels = [2,3,5,6,8,10,11,12,14,15]   \n",
    "test_class_labels = [1,4,7,9,13,16]     #[11...16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sL367A6HF3qf"
   },
   "source": [
    "**Prepare Training and validation Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MEfZ0-xNEcM"
   },
   "source": [
    "**Create DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kNBNYdJX3STT"
   },
   "outputs": [],
   "source": [
    "C = 5    # n_class\n",
    "K1 = 5   # n_support\n",
    "N = 15   # n_query\n",
    "tC = 3   # classes in a test episode\n",
    "im_height,im_width,im_depth = 11,11,30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vmBBYOcd7wdm"
   },
   "outputs": [],
   "source": [
    "def new_episode(patches_list,K,C,N,class_labels) :\n",
    "  selected_classes = np.random.choice(class_labels,C,replace=False)  # Randomly choice 5 Classes out of classes available # replace: False means no repetition\n",
    "  tsupport_patches = []\n",
    "  tquery_patches = []\n",
    "  query_labels = []\n",
    "  support_labels = list(selected_classes) \n",
    "  for x in selected_classes :\n",
    "    sran_indices = np.random.choice(len(patches_list[x-1]),K,replace=False)  # for class no X-1: select K samples \n",
    "    support_patches = patches_list[x-1][sran_indices,:,:,:,:]\n",
    "    qran_indices = np.random.choice(len(patches_list[x-1]),N,replace=False)  # N Samples for Query\n",
    "    query_patches = patches_list[x-1][qran_indices,:,:,:,:]\n",
    "  # Support and Query patches belong to same Class \n",
    "    for i in range(N) :\n",
    "      query_labels.append(x) #  [1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7] [p1, p1, p1, p1, p1, p3, p3 ...]\n",
    "    tquery_patches.extend(query_patches)   # class 5, query 15 75  # extend: add all elements of a list to another list individually and not append the whole list as one element\n",
    "    tsupport_patches.extend(support_patches) # class 5, support 5 25\n",
    "  temp1 = list(zip(tquery_patches, query_labels))  \n",
    "  random.shuffle(temp1)        # By Doing Shuffling, Support, Query Same class combination got mismatched - mitigated by support index\n",
    "  tquery_patches, query_labels = zip(*temp1) # * is used to unzip\n",
    "  tquery_patches = tf.convert_to_tensor(np.reshape(np.asarray(tquery_patches),(C*N,im_height,im_width,im_depth,1)),dtype=tf.float32)\n",
    "  tsupport_patches = tf.convert_to_tensor(np.reshape(np.asarray(tsupport_patches),(C*K,im_height,im_width,im_depth,1)),dtype=tf.float32)\n",
    "  return tquery_patches, tsupport_patches, query_labels, support_labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mZSEL1Zo3KUh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 14, 6, 3, 10]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tquery_patches, tsupport_patches, query_labels, support_labels = new_episode(patches_class_ip,K1,C,N,train_class_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SyQYRKpN6t4"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "rtxObYGQN6t4"
   },
   "outputs": [],
   "source": [
    "ProtoModel = Prototypical(model,im_width, im_height, im_depth, im_channel)\n",
    "optimizer = tf.keras.optimizers.Adam(0.00001)          #Adam(0.001)\n",
    "n_times = 1  # 25\n",
    "#std_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8HvgsXRvN6t6"
   },
   "outputs": [],
   "source": [
    "# Metrics to gather\n",
    "train_loss = tf.metrics.Mean(name='train_loss')\n",
    "train_acc = tf.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "def train_step(support, query, support_labels, query_labels, K, C, N):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, mean_accuracy, mean_predictions = ProtoModel(support, query, support_labels, query_labels, K, C, N,n_times,training=True)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # A gradient simply measures the change in all weights with regard to the change in error. You can also think of a gradient as the slope of a function. The higher the gradient, the steeper the slope and the faster a model can learn. But if the slope is zero, the model stops learning\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Log loss and accuracy for step\n",
    "    train_loss(loss)\n",
    "    train_acc(mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tquery_patches, tsupport_patches, query_labels, support_labels = new_episode(patches_class_ip,K1,C,N,train_class_labels)     \n",
    "train_step(tsupport_patches, tquery_patches,support_labels, query_labels, K1, C, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6kykozUCMjn"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = 'E:\\Engginearing\\SAKEC\\SEM 6\\Major Project\\Research\\SAMPLE_CODE_TF2_Keras\\Inprogress\\Hyperspectral_Classification\\Training checkpoints\\IP\\5_shot_ckpts\\\\New_train_5_way_ip_25_128'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, ProtoModel = ProtoModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 166476,
     "status": "error",
     "timestamp": 1675333632398,
     "user": {
      "displayName": "Mayank Kantharia",
      "userId": "02386211270688916554"
     },
     "user_tz": -330
    },
    "id": "N4DqXA2lN6t8",
    "outputId": "64dccfb2-f4e5-4242-ad83-d30a9643f285"
   },
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "temp_list = []\n",
    "for epoch in range(50): # n_epochs-140\n",
    "    train_loss.reset_states()  \n",
    "    train_acc.reset_states()\n",
    "    \n",
    "    for epi in range(n_episodes): \n",
    "        tquery_patches, tsupport_patches, query_labels, support_labels = new_episode(patches_class_ip,K1,C,N,train_class_labels)     \n",
    "        train_step(tsupport_patches, tquery_patches,support_labels, query_labels, K1, C, N)    \n",
    "        template = 'Epoch {}, Episode {}, Train Loss: {:.2f}, Train Accuracy: {:.2f}'\n",
    "        print(template.format(epoch+1, epi+1,train_loss.result(),train_acc.result()*100))\n",
    "    if epoch % 5 == 0 and epoch != 0 :\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9p9sGw0yAon"
   },
   "source": [
    "**Tuning_5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f64ExXqB_J_F"
   },
   "outputs": [],
   "source": [
    "tune_set_5 = [[] for i in range(6)]\n",
    "for j in range(6) :\n",
    "  tune_set_5[j] = test_patches_class[j][:5,:,:,:,:]   # for each class first 5 samples taken\n",
    "std_tune_5 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQHAAuML_BF8"
   },
   "outputs": [],
   "source": [
    "def tune_episode(tune_set,tC,tK,tN,test_class_labels) :\n",
    "  selected_classes = np.random.choice(test_class_labels,tC,replace=False)\n",
    "  support_labels  = list(selected_classes)\n",
    "  query_labels = []\n",
    "  support_patches = []\n",
    "  query_patches = []\n",
    "  for x in selected_classes :\n",
    "    y = test_class_labels.index(x)\n",
    "    np.random.shuffle(tune_set[y])    \n",
    "    support_imgs = tune_set[y][:tK,:,:,:,:]    #Support 1, Query 4\n",
    "    query_imgs = tune_set[y][tK:5,:,:,:,:]\n",
    "    support_patches.extend(support_imgs)\n",
    "    query_patches.extend(query_imgs)\n",
    "    for i in range(tN) :\n",
    "      query_labels.append(x)\n",
    "  temp1 = list(zip(query_patches, query_labels)) \n",
    "  random.shuffle(temp1) \n",
    "  query_patches, query_labels = zip(*temp1)\n",
    "  query_patches = tf.convert_to_tensor(np.reshape(np.asarray(query_patches),(tC*tN,im_height,im_width,im_depth,1)),dtype=tf.float32)\n",
    "  support_patches = tf.convert_to_tensor(np.reshape(np.asarray(support_patches),(tC*tK,im_height,im_width,im_depth,1)),dtype=tf.float32)\n",
    "  return query_patches, support_patches, query_labels, support_labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78mUqPSj3DA6"
   },
   "outputs": [],
   "source": [
    "query_patches, support_patches, query_labels, support_labels = tune_episode(tune_set_5,4,1,4,test_class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KznikxjCElAR"
   },
   "outputs": [],
   "source": [
    "# Metrics to gather\n",
    "tune_loss = tf.metrics.Mean(name='tune_loss')\n",
    "tune_acc = tf.metrics.Mean(name='tune_accuracy')\n",
    "\n",
    "def tune_step(support, query, support_labels, query_labels, K, C, N):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, mean_accuracy, mean_predictions = ProtoModel(support, query, support_labels, query_labels, K, C, N,n_times,training=True)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Log loss and accuracy for step\n",
    "    tune_loss(loss)\n",
    "    tune_acc(mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlIrGMXBKN8Y"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir1 = '/content/drive/My Drive/Research/SAMPLE_CODE_TF2_Keras/Inprogress/Hyperspectral_Classification/Training checkpoints/IP/5_shot_ckpts/New_train_5_way_ip_25_128/Tune'\n",
    "checkpoint_prefix1 = os.path.join(checkpoint_dir1, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 ProtoModel = ProtoModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 478694,
     "status": "error",
     "timestamp": 1664643887678,
     "user": {
      "displayName": "Mayank Kantharia",
      "userId": "02386211270688916554"
     },
     "user_tz": -330
    },
    "id": "abc2CqQdBRFB",
    "outputId": "d51f7923-cafb-4f5e-b0d8-f63446e3fd53"
   },
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "temp_list = []\n",
    "std_tune_5 = []\n",
    "for epoch in range(41): \n",
    "    tune_loss.reset_states()  \n",
    "    tune_acc.reset_states()    \n",
    "    for epi in range(n_episodes+1): \n",
    "        tquery_patches, tsupport_patches, query_labels, support_labels = tune_episode(tune_set_5,3,1,4,test_class_labels)    \n",
    "        tune_step(tsupport_patches, tquery_patches,support_labels, query_labels, 1, 3, 4)      \n",
    "        \n",
    "    template = 'Epoch {}, Tune Loss: {:.2f}, Tune Accuracy: {:.2f}'\n",
    "    print(template.format(epoch+1,tune_loss.result(),tune_acc.result()*100))\n",
    "    if (epoch+1)%5 == 0 :\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPjwIpM_tPz0"
   },
   "outputs": [],
   "source": [
    "def test_episode(test_patches_class,test_class_labels,test_C,test_K,i,f) :\n",
    "  selected_classes = test_class_labels[i:f]   # [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "  support_labels = list(selected_classes)\n",
    "  query_labels = []\n",
    "  support_patches = []\n",
    "  query_patches = []\n",
    "  for x in selected_classes :\n",
    "    y = test_class_labels.index(x)\n",
    "    support_imgs = test_patches_class[y][:test_K,:,:,:,:]\n",
    "    query_imgs = test_patches_class[y][test_K:,:,:,:,:]\n",
    "    support_patches.extend(support_imgs)\n",
    "    query_patches.extend(query_imgs)\n",
    "    for i in range(query_imgs.shape[0]) :\n",
    "      query_labels.append(x)\n",
    "  temp1 = list(zip(query_patches, query_labels)) \n",
    "  random.shuffle(temp1) \n",
    "  query_patches, query_labels = zip(*temp1)\n",
    "  x = len(query_labels)\n",
    "  query_patches = tf.convert_to_tensor(np.reshape(np.asarray(query_patches),(x,im_height,im_width,im_depth,1)),dtype=tf.float32)\n",
    "  support_patches = tf.convert_to_tensor(np.reshape(np.asarray(support_patches),(test_C*test_K,im_height,im_width,im_depth,1)),dtype=tf.float32)\n",
    "  return query_patches, support_patches, query_labels, support_labels,x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wO2QtQRf3ZpL"
   },
   "outputs": [],
   "source": [
    "query_patches, support_patches, query_labels, support_labels,x = test_episode(test_patches_class,test_class_labels,3,5,0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SP2cSUDD1H2n"
   },
   "outputs": [],
   "source": [
    "# Metrics to gather\n",
    "test_loss = tf.metrics.Mean(name='test_loss')\n",
    "test_acc = tf.metrics.Mean(name='test_accuracy')\n",
    "def test_step(support, query, support_labels, query_labels, K, C, y):\n",
    "    loss, mc_predictions, mean_accuracy, classwise_mean_acc, y = ProtoModel(support, query, support_labels, query_labels, K, C, y,n_times,training=False)\n",
    "    return loss, mc_predictions, mean_accuracy, classwise_mean_acc, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5625,
     "status": "ok",
     "timestamp": 1664643907115,
     "user": {
      "displayName": "Mayank Kantharia",
      "userId": "02386211270688916554"
     },
     "user_tz": -330
    },
    "id": "XsWfzZo5e242",
    "outputId": "6803e128-3d80-44d5-e75b-e6c25e3657dc"
   },
   "outputs": [],
   "source": [
    "for epoch in range(1): #1000\n",
    "    test_loss.reset_states()  \n",
    "    test_acc.reset_states()        \n",
    "    tquery_patches1, tsupport_patches1, query_labels1, support_labels1, x1 = test_episode(test_patches_class,test_class_labels,3,5,0,3)    \n",
    "    loss1, mc_predictions1, mean_accuracy1, classwise_mean_acc1, y1 = test_step(tsupport_patches1, tquery_patches1,support_labels1, query_labels1, 5, 3, x1/3)      \n",
    "    print('OA1',mean_accuracy1)\n",
    "# Class-wise Accuracy\n",
    "    for i in range(tC) :\n",
    "      print('class',i+1,classwise_mean_acc1[i])\n",
    "    print('loss',loss1)\n",
    "    tquery_patches2, tsupport_patches2, query_labels2, support_labels2, x2 = test_episode(test_patches_class,test_class_labels,3,5,3,6)    \n",
    "    loss2, mc_predictions2, mean_accuracy2, classwise_mean_acc2, y2 = test_step(tsupport_patches2, tquery_patches2,support_labels2, query_labels2, 5, 3, x2/3)\n",
    "    print('OA2',mean_accuracy2) \n",
    "# Class-wise Accuracy\n",
    "    for i in range(tC) :\n",
    "      print('class',i+4,classwise_mean_acc2[i])\n",
    "    print('loss',loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRXITw9htGuw"
   },
   "source": [
    "**Overall Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1664643908863,
     "user": {
      "displayName": "Mayank Kantharia",
      "userId": "02386211270688916554"
     },
     "user_tz": -330
    },
    "id": "oGeG6aLFtFvM",
    "outputId": "d03c0ed0-0266-42c9-b9e6-14eb0f4bdec0"
   },
   "outputs": [],
   "source": [
    "mean_predictions1 =  tf.reduce_mean(mc_predictions1,axis=0)\n",
    "mean_predictions2 =  tf.reduce_mean(mc_predictions2,axis=0)\n",
    "overall_predictions = tf.concat([mean_predictions1,mean_predictions2],axis=0)\n",
    "overall_true_labels = tf.concat([y1,y2],axis=0)\n",
    "correct_pred = tf.cast(tf.equal(                                             # accuracy for the current pass\n",
    "            tf.cast(tf.argmax(overall_predictions, axis=-1), tf.int32), \n",
    "            tf.cast(tf.argmax(overall_true_labels,axis=-1), tf.int32)), tf.float32)\n",
    "o_acc = tf.reduce_mean(correct_pred) \n",
    "print(\"Overall accuracy:\",o_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxDxvPzisig0"
   },
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1664643926157,
     "user": {
      "displayName": "Mayank Kantharia",
      "userId": "02386211270688916554"
     },
     "user_tz": -330
    },
    "id": "CoyQLsRUshg7",
    "outputId": "5c3a5027-cded-4d2b-951b-a91c322c6793"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "mean_predictions1 =  tf.reduce_mean(mc_predictions1,axis=0)\n",
    "cm_pred1 = tf.argmax(mean_predictions1, axis=-1)\n",
    "mean_predictions2 =  tf.reduce_mean(mc_predictions2,axis=0)\n",
    "cm_pred2 = tf.argmax(mean_predictions2, axis=-1) + 3\n",
    "overall_predictions = tf.concat([cm_pred1,cm_pred2],axis=0)\n",
    "cm_true1 = tf.argmax(y1,axis=-1)\n",
    "cm_true2 = tf.argmax(y2,axis=-1) + 3\n",
    "overall_true_labels = tf.concat([cm_true1,cm_true2],axis=0)\n",
    "results = confusion_matrix(overall_true_labels,overall_predictions) \n",
    "print ('Confusion Matrix :')\n",
    "print(results) \n",
    "print ('Report : ')\n",
    "print (classification_report(overall_true_labels, overall_predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOP-rcMX-4jl"
   },
   "source": [
    "**Kappa Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 364,
     "status": "ok",
     "timestamp": 1664643932041,
     "user": {
      "displayName": "Mayank Kantharia",
      "userId": "02386211270688916554"
     },
     "user_tz": -330
    },
    "id": "bkUfTXWg-3xd",
    "outputId": "6a7750ff-dc87-49aa-ce54-de0b649e17c6"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.cohen_kappa_score(overall_true_labels, overall_predictions)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
