{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aditya Sawant's Version of SPN_IP_5Shot.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Used\n",
    "\n",
    "- tensorflow\n",
    "- sklearn\n",
    "- numpy\n",
    "- matplotlib\n",
    "- pandas\n",
    "- scipy\n",
    "- tensorflow_probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn numpy matplotlib scipy tensorflow_probability pandas tqdm plotly\n",
    "# tensorflow[and-cuda]==2.10 [cuDNN 8.1.1 CUDA 11.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.10.0-rc3-6-g359c3cdfc5f 2.10.0\n",
      "True\n",
      "Number of GPUs Available:  1\n",
      "Number of Devices Available:  2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.27.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HSI FSL BE-10 Major Project\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning:\n",
      "\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import statistics\n",
    "import os\n",
    "import importlib\n",
    "import IPython\n",
    "from IPython.display import clear_output\n",
    "  \n",
    "from operator import truediv\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.version.GIT_VERSION, tf.version.VERSION)\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(\"Number of GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Number of Devices Available: \", len(tf.config.experimental.list_physical_devices()))\n",
    "# import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches as matPatch\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.compat.v1.distributions import Bernoulli\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
    "import spectral\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "from datetime import date, datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from encoders.conv_gan import createModel\n",
    "from lib.PrototypicalNetwork import Prototypical\n",
    "from lib.misc import timeIt, plotData\n",
    "from lib.Data import Data\n",
    "from lib.Stats import Stats\n",
    "from lib.Predict import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test each code block individually\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TEST_BLOCKS: bool = False\n",
    "CWD: str = os.getcwd()\n",
    "\n",
    "\n",
    "VERBOSE: bool = False\n",
    "SAVE_REPORT: bool = True\n",
    "\n",
    "\n",
    "SAVE_MODEL: bool = False\n",
    "\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "\n",
    "\n",
    "# Dataset Used : Indian Pines\n",
    "\n",
    "\n",
    "DATASET: str = 'IP' # IP (indian_pines) PU (pavia_university) SA (salinas) HU (houston) \n",
    "\n",
    "\n",
    "PATH_TO_DATASET: str = CWD + '\\\\Datasets\\\\'# PCA\n",
    "\n",
    "\n",
    "PCA_COMPONENTS: int = 30 # Number of components to keep after PCA reduction# Window size for forming image cubes\n",
    "\n",
    "\n",
    "WINDOW_SIZE: int = 11# Image dimensions after forming image cubes\n",
    "\n",
    "\n",
    "IMAGE_WIDTH: int\n",
    "\n",
    "\n",
    "IMAGE_HEIGHT: int\n",
    "\n",
    "\n",
    "IMAGE_DEPTH: int\n",
    "\n",
    "\n",
    "IMAGE_CHANNEL: int \n",
    "\n",
    "\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH, IMAGE_CHANNEL = 11, 11, 30, 1\n",
    "IMAGE_DATA = (IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH, IMAGE_CHANNEL)\n",
    "\n",
    "\n",
    "\n",
    "# Model Parameters\n",
    "\n",
    "\n",
    "\n",
    "N_TIMES = 25 # Number of times to run the model. Internally, the model is runs each episode N_TIMES times# Learning Rate\n",
    "\n",
    "\n",
    "LEARNING_RATE: float = 0.00001\n",
    "\n",
    "# Temprature Scaling\n",
    "\n",
    "\n",
    "TAU: float = 1.8\n",
    "\n",
    "\n",
    "# C (No. of Classes) K (No. of Samples per Class) N (No. of Patches per Class)\n",
    "\n",
    "\n",
    "TRAIN_C: int = 5 # Number of classes to be used for training\n",
    "\n",
    "\n",
    "TRAIN_K: int = 5 # Number of patches per class to be used for support during training\n",
    "\n",
    "\n",
    "TRAIN_N: int = 15 # Number of patches per class to be used for query during training\n",
    "\n",
    "TUNE_C: int = 3 # Number of classes to be used for tunning\n",
    "\n",
    "\n",
    "TUNE_K: int = 1 # Number of patches per class to be used for support during tunning\n",
    "\n",
    "\n",
    "TUNE_N: int = 4 # Number of patches per class to be used for query during tunning\n",
    "\n",
    "TEST_C: int = 3 # Number of classes to be used for testing\n",
    "\n",
    "\n",
    "TEST_K: int = 5 # Number of patches per class to be used for support during testing\n",
    "\n",
    "\n",
    "TEST_N: int = 15 # Number of patches per class to be used for query during testing#\n",
    "# ===================================\n",
    "\n",
    "\n",
    "# DO NOT REMOVE THIS.\n",
    "\n",
    "# ===================================\n",
    "\n",
    "\n",
    "# Don't know this yet, probably used in the model to calculate loss\n",
    "\n",
    "\n",
    "MC_LOSS_WEIGHT: int = 5 \n",
    "\n",
    "\n",
    "# DIRECTLY USED IN PROTOTYPICAL NETWORK CLASS IN TESTING CASE\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# Training Epochs 50\n",
    "\n",
    "\n",
    "TRAINING_EPOCH: int = 40\n",
    "# Training Episode 100\n",
    "\n",
    "\n",
    "TRAINING_EPISODE: int = 100\n",
    "# Tunning Epochs 41\n",
    "\n",
    "\n",
    "TUNNING_EPOCH: int = 15\n",
    "# Tunning Episode 100\n",
    "\n",
    "\n",
    "TUNNING_EPISODE: int = 100\n",
    "# Testing Epochs 1000\n",
    "\n",
    "\n",
    "TESTING_EPOCH: int = 1000\n",
    "# Metrics to be used for evaluation\n",
    "\n",
    "\n",
    "PRE_TUNE_TESTING: bool = True\n",
    "\n",
    "\n",
    "train_loss = tf.metrics.Mean(name='train_loss')\n",
    "\n",
    "\n",
    "train_acc = tf.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "\n",
    "tune_loss = tf.metrics.Mean(name='tune_loss')\n",
    "\n",
    "\n",
    "tune_acc = tf.metrics.Mean(name='tune_accuracy')\n",
    "\n",
    "\n",
    "test_loss = tf.metrics.Mean(name='test_loss')\n",
    "\n",
    "\n",
    "test_acc = tf.metrics.Mean(name='test_accuracy')\n",
    "\n",
    "trainingData = []\n",
    "\n",
    "\n",
    "tunningData = []\n",
    "\n",
    "\n",
    "testingData = []\n",
    "TABLE= ''\n",
    "\n",
    "run_folder =  f'{date.today()}' + '-' + f'{datetime.now().hour}_5_1' + '\\\\' \n",
    "checkpoint_dir = CWD + '\\\\saves\\\\' + run_folder + DATASET + '\\\\' + f'{TRAIN_K}_shot_way' + '\\\\Train'\n",
    "\n",
    "\n",
    "checkpoint_prefix_train = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint_dir1 = CWD + '\\\\saves\\\\' + run_folder + DATASET + '\\\\' + f'{TRAIN_K}_shot_way' + '\\\\Train\\\\Tune'\n",
    "\n",
    "\n",
    "checkpoint_prefix_tune = os.path.join(checkpoint_dir1, \"ckpt\")\n",
    "report_path = CWD + f'\\\\Reports\\\\Report_{date.today()}_{str(datetime.now()).split(\".\")[0].split()[1].replace(\":\", \"-\")}.txt'\n",
    "report_path_pre = CWD + f'\\\\Reports\\\\Report_{date.today()}_{str(datetime.now()).split(\".\")[0].split()[1].replace(\":\", \"-\")}.txt'\n",
    "\n",
    "\n",
    "model_save_path = CWD + '\\\\saves\\\\' + run_folder + DATASET + '\\\\' + f'{TRAIN_K}_shot_way' + '\\\\Train\\\\encoder.h5'\n",
    "\n",
    "\n",
    "\n",
    "finalReportPath: str = ''\n",
    "reportTotal_path: str = ''\n",
    "reportTotal_path_pre: str = ''\n",
    "predict_path: str = ''\n",
    "predictTrue_path: str = ''\n",
    "gt_path: str = ''\n",
    "current_path: str = ''\n",
    "\n",
    "checkpoint = None  # To be used for loading checkpoints. Declared in the Main Block\n",
    "\n",
    "\n",
    "ProtoModel = None  # Prototypical Network Object. Declared in the Main Block\n",
    "\n",
    "\n",
    "model = None  # Model Object. Declared in the Main Block\n",
    "\n",
    "\n",
    "optimizer = None  # Optimizer Object. Declared in the Main Block\n",
    "\n",
    "X = None\n",
    "Y = None\n",
    "patches = None\n",
    "target_names = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainingEpisode(patches:list, labels:list, K:int, C:int, N:int ):\n",
    "    \"\"\"\n",
    "    createTrainingEpisode creates a training episode for the N-way K-shot learning task.\n",
    "    \n",
    "    :param patches: list of all patches classified into different classes.\n",
    "    :param labels: list of classes from which the traning episode is to be created.\n",
    "    :param K: number of patches per class in the support set.\n",
    "    :param C: number of classes in the training episode.\n",
    "    :param N: number of patches per class in the query set.\n",
    "    :return queryPatches, queryLabels, supportPatches, supportLabels: training episode\n",
    "    \n",
    "    Algorithm:\n",
    "    - Select N classes from the list of labels. They should be unique.\n",
    "    - For each class, select K+Q patches. They should be unique.\n",
    "        - First K patches are support patches.\n",
    "        - Last Q patches are query patches.\n",
    "        - Append the support patches to supportPatches.\n",
    "        - Append the query patches to queryPatches.\n",
    "        - Append the class label to queryLabels Q times.\n",
    "    - Shuffle the queryPatches and queryLabels in the same order.\n",
    "    - Convert the queryPatches and supportPatches to tensors.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    selectedLabels = random.sample(labels, C)\n",
    "    supportPatches = []\n",
    "    supportLabels = list(selectedLabels)\n",
    "    queryPatches = []\n",
    "    queryLabels = []\n",
    "    \n",
    "    for n in selectedLabels:\n",
    "        sran_indices = np.random.choice(len(patches[n-1]),K,replace=False)  # for class no X-1: select K samples \n",
    "        supportPatches.extend( patches[n-1][sran_indices,:,:,:,:])\n",
    "        qran_indices = np.random.choice(len(patches[n-1]),N,replace=False)  # N Samples for Query\n",
    "        queryPatches.extend(patches[n-1][qran_indices,:,:,:,:])\n",
    "        queryLabels.extend([n]*N)\n",
    "    \n",
    "    shuffled = list(zip(queryPatches, queryLabels))\n",
    "    random.shuffle(shuffled)\n",
    "    queryPatches, queryLabels = zip(*shuffled)\n",
    "    \n",
    "    queryPatches = tf.convert_to_tensor(np.reshape(np.asarray(queryPatches),(C*N,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    supportPatches = tf.convert_to_tensor(np.reshape(np.asarray(supportPatches),(C*K,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    \n",
    "    return queryPatches, queryLabels, supportPatches, supportLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(support, query, support_labels, query_labels, K, C, N):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, mean_accuracy, mean_predictions = ProtoModel(support, query, support_labels, query_labels, K, C, N,training=True)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # A gradient simply measures the change in all weights with regard to the change in error. You can also think of a gradient as the slope of a function. The higher the gradient, the steeper the slope and the faster a model can learn. But if the slope is zero, the model stops learning\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Log loss and accuracy for step\n",
    "    train_loss(loss)\n",
    "    train_acc(mean_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def trainingEpochs(patches, labels, n_epochs, n_episodes):\n",
    "    \"\"\"\n",
    "    trainingEpochs function trains the model for n_epochs and n_episodes.\n",
    "    \n",
    "    :param patches: image patches to be trained\n",
    "    :param labels: corresponding labels to be used\n",
    "    :param n_epochs: number of epochs\n",
    "    :param n_episodes: number of episodes\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    template = 'Epoch {}/{}, Episode {}/{}, Train Loss: {:.2f}, Train Accuracy: {:.2f}'\n",
    "    # for epoch in tqdm(range(n_epochs), desc='Epochs'):\n",
    "    #     train_loss.reset_states()\n",
    "    #     train_acc.reset_states()\n",
    "    #     for episode in tqdm(range(n_episodes), desc=f'Episodes (Loss: {l:.2f}, Acc: {a:.2f})'):\n",
    "    \n",
    "    break_after_epoch = False\n",
    "    no_of_epoch_before_break = 1\n",
    "    trainObj = tqdm(total=n_episodes * n_epochs, desc=f'Epoch 1/{n_epochs}, Episode 1/{n_episodes}')\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss.reset_states()\n",
    "        train_acc.reset_states()\n",
    "        for episode in range(n_episodes):\n",
    "            queryPatches, queryLabels, supportPatches, supportLabels = createTrainingEpisode(patches, labels, TRAIN_K, TRAIN_C, TRAIN_N)\n",
    "            train_step(supportPatches, queryPatches,supportLabels,  queryLabels, TRAIN_K, TRAIN_C, TRAIN_N)\n",
    "            # clear_output(wait=True)\n",
    "            if(train_acc.result().numpy()*100 >= 100):\n",
    "                break_after_epoch = True\n",
    "            trainObj.set_description(\n",
    "                f'Epoch {epoch+1}/{n_epochs}, Episode {episode+1}/{n_episodes} (Loss: {train_loss.result().numpy():.2f}, Acc: {train_acc.result().numpy()*100:.2f})')\n",
    "            trainObj.update(1)\n",
    "            if(VERBOSE):\n",
    "                print(template.format(epoch+1, n_epochs, episode+1, n_episodes, train_loss.result()*100, train_acc.result()*100))\n",
    "                trainingData.append([train_loss.result(),  train_acc.result()*100])\n",
    "                plotData(trainingData)\n",
    "        # TO BREAK IF ACC REACHES 100\n",
    "        # if(break_after_epoch):\n",
    "        #     if(no_of_epoch_before_break == 0):\n",
    "        #         trainObj.close()\n",
    "        #         print()\n",
    "        #         print('Stoping Training as 100% Acc reached atleast once')\n",
    "        #         checkpoint.save(file_prefix=checkpoint_prefix_train)    \n",
    "        #         break\n",
    "        #     no_of_epoch_before_break -= 1\n",
    "\n",
    "        if(epoch and epoch % 5 == 0):\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix_train)    \n",
    "    trainObj.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTunningEpisodes(patches:list, labels:list, K:int, C:int, N:int):\n",
    "    \"\"\"\n",
    "    createTuningEpisodes creates a tuning episode for the N-way K-shot learning task.\n",
    "    \n",
    "    :param patches: list of all patches classified into different classes.\n",
    "    :param labels: list of classes from which the tuning episode is to be created.\n",
    "    :param K: number of patches per class in the support set.\n",
    "    :param C: number of classes in the tuning episode.\n",
    "    :param N: number of patches per class in the query set.\n",
    "    :return queryPatches, queryLabels, supportPatches, supportLabels: tuning episode\n",
    "    \n",
    "    Algorithm:\n",
    "    - Select C classes from the list of labels. They should be unique.\n",
    "    - For each selected class.\n",
    "        - Shuffle the patches of that class.\n",
    "        - First K patches are support patches.\n",
    "        - Next N patches are query patches. \n",
    "        - Append the support patches to supportPatches.\n",
    "        - Append the query patches to queryPatches.\n",
    "        - Append the class label to queryLabels N times.\n",
    "    - Shuffle the queryPatches and queryLabels in the same order.\n",
    "    - Convert the queryPatches and supportPatches to tensors.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    selected_classes = np.random.choice(labels,C,replace=False)\n",
    "    supportLabels  = list(selected_classes)\n",
    "    queryLabels = []\n",
    "    supportPatches = []\n",
    "    queryPatches = []\n",
    "    \n",
    "    for x in selected_classes :\n",
    "        y = labels.index(x)\n",
    "        np.random.shuffle(patches[y])    \n",
    "        supportPatches.extend(patches[y][:K,:,:,:,:])  # 1st K patches for support set\n",
    "        queryPatches.extend(patches[y][K:K+N,:,:,:,:])   # next N patches for query set\n",
    "        queryLabels.extend([x]*N)            \n",
    "          # next 5 labels for query set\n",
    "    \n",
    "    shuffled = list(zip(queryPatches, queryLabels))\n",
    "    random.shuffle(shuffled)\n",
    "    queryPatches, queryLabels = zip(*shuffled)\n",
    "    \n",
    "    queryPatches = tf.convert_to_tensor(np.reshape(np.asarray(queryPatches),(C*N,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    supportPatches = tf.convert_to_tensor(np.reshape(np.asarray(supportPatches),(C*K,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    \n",
    "    return queryPatches, queryLabels, supportPatches, supportLabels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_step(support, query, support_labels, query_labels, K, C, N):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, mean_accuracy, mean_predictions = ProtoModel(support, query, support_labels, query_labels, K, C, N,training=True)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Log loss and accuracy for step\n",
    "    tune_loss(loss)\n",
    "    tune_acc(mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def tunningEpochs(patches, labels, n_epochs, n_episodes):\n",
    "    \"\"\"\n",
    "    trainingEpochs function trains the model for n_epochs and n_episodes.\n",
    "    \n",
    "    :param patches: image patches to be trained\n",
    "    :param labels: corresponding labels to be used\n",
    "    :param n_epochs: number of epochs\n",
    "    :param n_episodes: number of episodes\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    template = 'Epoch {}/{}, Tune Loss: {:.2f}, Tune Accuracy: {:.2f}'\n",
    "\n",
    "    epochObj = tqdm(range(n_epochs), desc='Epochs')\n",
    "    for epoch in epochObj: \n",
    "        tune_loss.reset_states()  \n",
    "        tune_acc.reset_states()    \n",
    "        for epi in range(n_episodes):  \n",
    "            queryPatches, queryLabels, supportPatches, supportLabels = createTunningEpisodes(patches, labels, TUNE_K, TUNE_C, TUNE_N)    \n",
    "            tune_step(supportPatches, queryPatches,supportLabels, queryLabels, TUNE_K, TUNE_C, TUNE_N)   \n",
    "            # clear_output(wait=True)   \n",
    "        epochObj.set_postfix(\n",
    "            {'Loss': tune_loss.result().numpy(), 'Acc': tune_acc.result().numpy()*100}, refresh=True)\n",
    "        if(VERBOSE):\n",
    "            print(template.format(epoch+1, n_epochs,tune_loss.result(),tune_acc.result()*100))\n",
    "            tunningData.append([tune_loss.result(),  tune_acc.result()*100])\n",
    "            plotData(tunningData)\n",
    "        if (epoch+1)%5 == 0 :\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix_tune) \n",
    "    epochObj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTestingEpisode(patches, labels, K, C, N, i, f):\n",
    "    selected_classes = labels[i:f]   # [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    support_labels = list(selected_classes)\n",
    "    query_labels = []\n",
    "    support_patches = []\n",
    "    query_patches = []\n",
    "    for x in selected_classes:\n",
    "        y = labels.index(x)\n",
    "        # for class no X-1: select K samples\n",
    "        sran_indices = np.random.choice(len(patches[y]), K, replace=False)\n",
    "        support_patches.extend(patches[y][sran_indices, :, :, :, :])\n",
    "        qran_indices = np.random.choice(\n",
    "            len(patches[y]), N, replace=False)  # N Samples for Query\n",
    "        query_patches.extend(patches[y][qran_indices, :, :, :, :])\n",
    "        query_labels.extend([x]*N)\n",
    "\n",
    "    \n",
    "        \n",
    "        # support_imgs = patches[y][:K, :, :, :, :]\n",
    "        # query_imgs = patches[y][K: K + N, :, :, :, :]\n",
    "        # support_patches.extend(support_imgs)\n",
    "        # query_patches.extend(query_imgs)\n",
    "        # for i in range(query_imgs.shape[0]):\n",
    "        #     query_labels.append(x)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    temp1 = list(zip(query_patches, query_labels))\n",
    "    random.shuffle(temp1)\n",
    "    query_patches, query_labels = zip(*temp1)\n",
    "    x = len(query_labels)\n",
    "    query_patches = tf.convert_to_tensor(np.reshape(np.asarray(\n",
    "        query_patches), (x, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH, IMAGE_CHANNEL)), dtype=tf.float32)\n",
    "    support_patches = tf.convert_to_tensor(np.reshape(np.asarray(support_patches), (\n",
    "        C*K, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH, IMAGE_CHANNEL)), dtype=tf.float32)\n",
    "    return query_patches, support_patches, query_labels, support_labels, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(support, query, support_labels, query_labels, K, C, y):\n",
    "    loss, mc_predictions, mean_accuracy, classwise_mean_acc, y = ProtoModel(support, query, support_labels, query_labels, K, C, y,training=False)\n",
    "    return loss, mc_predictions, mean_accuracy, classwise_mean_acc, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def testingEpochs(patches, labels, n_epochs):\n",
    "    \"\"\"\n",
    "    testingEpochs function tests the model for n_epochs.\n",
    "\n",
    "    :param patches: image patches to be trained\n",
    "    :param labels: corresponding labels to be used\n",
    "    :param n_epochs: number of epochs\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    epochObj = tqdm(range(n_epochs), desc=f'Epochs')\n",
    "\n",
    "    all_y_pred1 = []\n",
    "    all_y_label1 = []\n",
    "    all_y_pred2 = []\n",
    "    all_y_label2 = []\n",
    "    for epoch in epochObj:\n",
    "        test_loss.reset_states()\n",
    "        test_acc.reset_states()\n",
    "\n",
    "        tquery_patches1, tsupport_patches1, query_labels1, support_labels1, x1 = createTestingEpisode(\n",
    "            patches, labels, TEST_K, TEST_C, TEST_N, 0, 3)\n",
    "        loss1, mc_predictions1, mean_accuracy1, classwise_mean_acc1, y1 = test_step(\n",
    "            tsupport_patches1, tquery_patches1, support_labels1, query_labels1, TEST_K, TEST_C, TEST_N)\n",
    "        oa1 = mean_accuracy1.numpy()\n",
    "\n",
    "        all_y_pred1.append(tf.convert_to_tensor(mc_predictions1))\n",
    "        all_y_label1.append(tf.convert_to_tensor(y1))\n",
    "\n",
    "        if (DATASET == 'IP' or DATASET == 'SA'):\n",
    "            tquery_patches2, tsupport_patches2, query_labels2, support_labels2, x2 = createTestingEpisode(\n",
    "                patches, labels, TEST_K, TEST_C, TEST_N, 3, 6)\n",
    "            loss2, mc_predictions2, mean_accuracy2, classwise_mean_acc2, y2 = test_step(\n",
    "                tsupport_patches2, tquery_patches2, support_labels2, query_labels2, TEST_K, TEST_C, TEST_N)\n",
    "            oa2 = mean_accuracy2.numpy()\n",
    "\n",
    "            all_y_pred2.append(tf.convert_to_tensor(mc_predictions2))\n",
    "            all_y_label2.append(tf.convert_to_tensor(y2))\n",
    "        oa2 = oa2 if (DATASET == 'IP' or DATASET == 'SA') else 0\n",
    "        epochObj.set_postfix({'OA1': f'{oa1}', 'OA2': f'{oa2}'}, refresh=True)\n",
    "        \n",
    "        if (VERBOSE):\n",
    "            print(\"=========================================\")\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "            print(\"-----------------------------------------\")\n",
    "            print(f\"Overall Accuracy 1 (OA1): {mean_accuracy1}\")\n",
    "            # Class Wise Accuracy\n",
    "            for i in range(TEST_C):\n",
    "                print(f\"Class {i+1} Accuracy: {classwise_mean_acc1[i]}\")\n",
    "            print(f\"Loss: {loss1.numpy():.3f}\")\n",
    "            if (DATASET == 'IP' or DATASET == 'SA'):\n",
    "                print(\"=========================================\")\n",
    "            else:\n",
    "                print(\"-----------------------------------------\")\n",
    "                print(f\"Overall Accuracy 2 (OA2): {mean_accuracy2}\")\n",
    "                # Class Wise Accuracy\n",
    "                for i in range(TEST_C):\n",
    "                    print(\n",
    "                        f\"Class {i+1+TEST_C} Accuracy: {classwise_mean_acc2[i]}\")\n",
    "                print(f\"Loss: {loss2.numpy():.3f}\")\n",
    "                print(\"=========================================\")\n",
    "\n",
    "            testingData.append([mean_accuracy1*100, mean_accuracy2 *\n",
    "                               100, loss1.numpy(), 0 if loss2 == 0 else loss2.numpy()])\n",
    "            plotData(testingData, testing=True)\n",
    "\n",
    "    epochObj.close()\n",
    "    try:\n",
    "        combined_y_pred1 = tf.concat(all_y_pred1, axis=0)\n",
    "        combined_y_label1 = tf.concat(all_y_label1, axis=0)\n",
    "        if(DATASET == 'IP' or DATASET == 'SA'):\n",
    "            combined_y_pred2 = tf.concat(all_y_pred2, axis=0)\n",
    "            combined_y_label2 = tf.concat(all_y_label2, axis=0)\n",
    "        else:\n",
    "            combined_y_pred2 = []\n",
    "            combined_y_label2 = []\n",
    "    except:\n",
    "        combined_y_pred1 = []\n",
    "        combined_y_label1 = []\n",
    "        combined_y_pred2 = []\n",
    "        combined_y_label2 = []\n",
    "    return combined_y_pred1, combined_y_pred2, combined_y_label1, combined_y_label2  # best_mc_predictions1, best_mc_predictions2, best_y1, best_y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Related Initializations\n",
    "\n",
    "def LoadData():\n",
    "    global X, Y, patches,target_names, NUM_CLASSES, TRAINING_CLASSES, TRAINING_LABELS, TUNNING_LABELS, TESTING_CLASSES, TESTING_LABELS, TRAINING_PATCHES, TUNNING_PATCHES, TESTING_PATCHES, data\n",
    "    data = Data(DATASET, PCA_COMPONENTS, WINDOW_SIZE)\n",
    "    X, Y, patches = data.get_data()\n",
    "    target_names = data.get_target_names()\n",
    "    NUM_CLASSES, TRAINING_CLASSES, TRAINING_LABELS, TUNNING_LABELS, TESTING_CLASSES, TESTING_LABELS, TRAINING_PATCHES, TUNNING_PATCHES, TESTING_PATCHES = data.load_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Related Initializations\n",
    "\n",
    "def LoadModel(encoder:str):\n",
    "    global model, ProtoModel, optimizer, checkpoint\n",
    "    encoder = importlib.import_module('encoders.' + encoder)\n",
    "    model = encoder.createModel(\n",
    "        IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH, IMAGE_CHANNEL)\n",
    "    ProtoModel = Prototypical(\n",
    "        model, IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH, IMAGE_CHANNEL, MC_LOSS_WEIGHT, TAU, N_TIMES)\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, ProtoModel=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RUN: 1 | DATASET: IP | ENCODER: conv_sa_adzz | TAU: 1.8 | N_TIMES: 25\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40, Episode 100/100 (Loss: 12.62, Acc: 99.43): 100%|██████████| 4000/4000 [58:56<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 3536.8819s\n",
      "\n",
      "Pre Tune Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [15:36<00:00,  1.07it/s, OA1=0.9333333373069763, OA2=1.0]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 936.9148s\n",
      "\n",
      "OA 97.10 | KA 96.53 | AA 97.10\n",
      "Report Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch: 225it [10:05,  2.69s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OA 94.76 | KA 94.04 | AA 93.47\n",
      "Image Prediction Report Saved!\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 15/15 [13:35<00:00, 54.35s/it, Loss=0.0537, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 815.3009s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [15:28<00:00,  1.08it/s, OA1=1.0, OA2=0.9777777791023254]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 928.7250s\n",
      "\n",
      "OA 99.06 | KA 98.87 | AA 99.06\n",
      "Report Saved!\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch: 225it [09:52,  2.63s/it]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OA 92.16 | KA 91.08 | AA 91.53\n",
      "Image Prediction Report Saved!\n",
      "Function 'doRun' executed in 7416.0019s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def TAU_test_spl():\n",
    "    global testingData, test_acc, test_loss\n",
    "    testingData = []\n",
    "    test_loss = tf.metrics.Mean(name='test_loss')\n",
    "\n",
    "    test_acc = tf.metrics.Mean(name='test_accuracy')\n",
    "\n",
    "\n",
    "def setGlobals(run, tau, n_times, encoder, dataset):\n",
    "    global finalReportPath, predict_path, model_save_path, report_path, run_folder, reportTotal_path_pre, report_path_pre\n",
    "    global checkpoint_dir, checkpoint_prefix_train, checkpoint_dir1, checkpoint_prefix_tune, gt_path, predictTrue_path\n",
    "    global TAU, N_TIMES, ABLATION_FOLDER, trainingData, tunningData, testingData, DATASET, reportTotal_path, current_path\n",
    "    global train_acc, train_loss, tune_acc, tune_loss, test_acc, test_loss, checkpoint, ProtoModel, optimizer, model\n",
    "    TAU = tau\n",
    "    N_TIMES = n_times\n",
    "    DATASET = dataset\n",
    "\n",
    "    ABLATION_FOLDER = CWD + '\\\\ablation\\\\' + TABLE + '\\\\'\n",
    "    run_folder =  f'{date.today()}' + '-' + f'{datetime.now().hour}_5_1' + '\\\\' \n",
    "    checkpoint_dir = CWD + '\\\\saves\\\\' + run_folder + DATASET + '\\\\' + f'{TRAIN_K}_shot_way' + '\\\\Train'\n",
    "    checkpoint_prefix_train = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    checkpoint_dir1 =checkpoint_dir + '\\\\Tune'\n",
    "    checkpoint_prefix_tune = os.path.join(checkpoint_dir1, \"ckpt\")\n",
    "    \n",
    "    current_path = ABLATION_FOLDER + DATASET + '\\\\' + f'{encoder}\\\\{TAU}\\\\{N_TIMES}' + f'\\\\{run+1}'\n",
    "    \n",
    "    model_save_path = current_path + '_encoder.h5'\n",
    "    \n",
    "    report_path = current_path + '_post_tune_report.txt'\n",
    "    reportTotal_path = current_path + '_post_tune_reportTotal.txt'\n",
    "    report_path_pre = current_path + '_pre_tune_report.txt'\n",
    "    reportTotal_path_pre = current_path + '_pre_tune_reportTotal.txt'\n",
    "    predict_path = current_path + '_prediction.png'\n",
    "    predictTrue_path = current_path + '_predictionTrue.png'\n",
    "    gt_path = current_path + '_gt.png'\n",
    "    finalReportPath = ABLATION_FOLDER + 'report.csv'\n",
    "    os.makedirs(os.path.dirname(report_path), exist_ok=True)\n",
    "    trainingData = []\n",
    "    tunningData = []\n",
    "    testingData = []\n",
    "    \n",
    "    train_loss = tf.metrics.Mean(name='train_loss')\n",
    "\n",
    "    train_acc = tf.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "    tune_loss = tf.metrics.Mean(name='tune_loss')\n",
    "\n",
    "    tune_acc = tf.metrics.Mean(name='tune_accuracy')\n",
    "\n",
    "    test_loss = tf.metrics.Mean(name='test_loss')\n",
    "\n",
    "    test_acc = tf.metrics.Mean(name='test_accuracy')\n",
    "        \n",
    "    checkpoint = None  # To be used for loading checkpoints. Declared in the Main Block\n",
    "\n",
    "    ProtoModel = None  # Prototypical Network Object. Declared in the Main Block\n",
    "\n",
    "    model = None  # Model Object. Declared in the Main Block\n",
    "\n",
    "    optimizer = None  # Optimizer Object. Declared in the Main Block\n",
    "\n",
    "@timeIt\n",
    "def doRun(run, dataset, encoder, tau, n_times):\n",
    "\n",
    "    if(DATASET != dataset):\n",
    "        LoadData()\n",
    "\n",
    "    setGlobals(run, tau, n_times, encoder, dataset)\n",
    "\n",
    "    LoadModel(encoder)\n",
    "\n",
    "    print('Training...')\n",
    "    trainingEpochs(patches, TRAINING_LABELS, TRAINING_EPOCH, TRAINING_EPISODE)\n",
    "    print()\n",
    "    if(PRE_TUNE_TESTING):\n",
    "        print('Pre Tune Testing...')\n",
    "        mc_predictions1, mc_predictions2, y1, y2 = testingEpochs(TESTING_PATCHES, TESTING_LABELS, TESTING_EPOCH)\n",
    "        print()\n",
    "\n",
    "        stats_pre = Stats(mc_predictions1, mc_predictions2, y1, y2)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            stats_pre.saveReport(report_path_pre, f'{ABLATION_FOLDER}pre_report.csv',  DATASET, N_TIMES, TAU, run, encoder)\n",
    "            print('Report Saved!')\n",
    "        except Exception as e:\n",
    "            print('Failed to Save Report')\n",
    "            print(e)\n",
    "        predictions, predictionsTrue, Y, y_test, y_pred = predictImage(data, ProtoModel, IMAGE_DATA, N_TIMES, 10,TAU, VotingTimes=5, RandomSupport=True)\n",
    "\n",
    "        try:\n",
    "            stats1_pre = Stats(None, None, None, None, y_test, y_pred, False)\n",
    "            stats1_pre.saveReport(reportTotal_path_pre, finalReportPath,  DATASET, N_TIMES, TAU, run, encoder, True)\n",
    "            print('Image Prediction Report Saved!')\n",
    "        except Exception as e:\n",
    "            print('Failed to Save Image Prediction Report')\n",
    "            print(e)\n",
    "        \n",
    "        spectral.save_rgb(f\"{current_path}_prediction_pre.png\", predictions.astype(int), colors=spectral.spy_colors, format='png')\n",
    "        spectral.save_rgb(f\"{current_path}_predictionTrue_pre.png\", predictionsTrue.astype(int), colors=spectral.spy_colors, format='png')\n",
    "        TAU_test_spl()\n",
    "    # return \n",
    "    print('Tunning...')\n",
    "    tunningEpochs(TUNNING_PATCHES, TESTING_LABELS,TUNNING_EPOCH, TUNNING_EPISODE)\n",
    "    print()\n",
    "    \n",
    "    print('Testing...')\n",
    "    mc_predictions1, mc_predictions2, y1, y2 = testingEpochs(TESTING_PATCHES, TESTING_LABELS, TESTING_EPOCH)\n",
    "    print()\n",
    "\n",
    "    stats = Stats(mc_predictions1, mc_predictions2, y1, y2)\n",
    "    \n",
    "    # stats.printReport()\n",
    "    \n",
    "    try:\n",
    "        stats.saveReport(report_path, finalReportPath,  DATASET, N_TIMES, TAU, run, encoder)\n",
    "        print('Report Saved!')\n",
    "    except Exception as e:\n",
    "        print('Failed to Save Report')\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        # ProtoModel.save(model_save_path)\n",
    "        print('Model Saved!')\n",
    "    except Exception as e:\n",
    "        print(\"Failed to Save Model\")\n",
    "        print(e)\n",
    "    \n",
    "    predictions, predictionsTrue, Y, y_test, y_pred = predictImage(data, ProtoModel, IMAGE_DATA, N_TIMES, 10,TAU, VotingTimes=5, RandomSupport=True)\n",
    "\n",
    "    try:\n",
    "        stats1 = Stats(None, None, None, None, y_test, y_pred, False)\n",
    "        stats1.saveReport(reportTotal_path, finalReportPath,  DATASET, N_TIMES, TAU, run, encoder, True)\n",
    "        print('Image Prediction Report Saved!')\n",
    "    except Exception as e:\n",
    "        print('Failed to Save Image Prediction Report')\n",
    "        print(e)\n",
    "    \n",
    "    spectral.save_rgb(predict_path, predictions.astype(int), colors=spectral.spy_colors, format='png')\n",
    "    spectral.save_rgb(predictTrue_path, predictionsTrue.astype(int), colors=spectral.spy_colors, format='png')\n",
    "    # spectral.save_rgb(gt_path, Y.astype(int), colors=spectral.spy_colors, format='png')\n",
    "    \n",
    "    \n",
    "\n",
    "def main():\n",
    "    # Tweekables:\n",
    "    global TABLE, PRE_TUNE_TESTING\n",
    "    TABLE = 'table tr 40 tu 15 adzz' \n",
    "    PRE_TUNE_TESTING = True\n",
    "    done = [] \n",
    "    n_timess = [1, 5, 15, 20, 25]\n",
    "    taus = [1.8]\n",
    "    datasets = ['IP']\n",
    "    encoders = ['conv_sa_adzz']\n",
    "    runs  = 1\n",
    "    doneRuns = 0 # Increment this if doing more runs for same combinations\n",
    "    for dataset in datasets:\n",
    "        global DATASET\n",
    "        DATASET = dataset\n",
    "        LoadData() \n",
    "        for encoder in encoders:\n",
    "            for tau in taus: \n",
    "                for n_times in n_timess:\n",
    "                    for run in range(runs):\n",
    "                        print(f'\\n\\nRUN: {run+1+doneRuns} | DATASET: {dataset} | ENCODER: {encoder} | TAU: {tau} | N_TIMES: {n_times}\\n')\n",
    "                        try:\n",
    "                            doRun(run, dataset, encoder, tau, n_times)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error: {e}\")\n",
    "                            continue\n",
    "\n",
    "\n",
    "main()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system(\"shutdown /s /t 2\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
