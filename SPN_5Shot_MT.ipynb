{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aditya Sawant's Version of SPN_IP_5Shot.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Used\n",
    "\n",
    "- tensorflow\n",
    "- sklearn\n",
    "- numpy\n",
    "- matplotlib\n",
    "- pandas\n",
    "- scipy\n",
    "- tensorflow_probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn numpy matplotlib scipy tensorflow_probability pandas tqdm plotly\n",
    "# tensorflow[and-cuda]==2.10 [cuDNN 8.1.1 CUDA 11.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.10.0-rc3-6-g359c3cdfc5f 2.10.0\n",
      "True\n",
      "Number of GPUs Available:  1\n",
      "Number of Devices Available:  2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.27.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import statistics\n",
    "import os\n",
    "import importlib\n",
    "import IPython\n",
    "from IPython.display import clear_output\n",
    "  \n",
    "from operator import truediv\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.version.GIT_VERSION, tf.version.VERSION)\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(\"Number of GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Number of Devices Available: \", len(tf.config.experimental.list_physical_devices()))\n",
    "# import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches as matPatch\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.compat.v1.distributions import Bernoulli\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
    "import spectral\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "from datetime import date, datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from encoders.conv_gan import createModel\n",
    "from lib.PrototypicalNetwork import Prototypical\n",
    "from lib.misc import timeIt, plotData\n",
    "from lib.Data import Data\n",
    "from lib.Stats import Stats\n",
    "from lib.Predict import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test each code block individually\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TEST_BLOCKS: bool = False\n",
    "CWD: str = os.getcwd()\n",
    "\n",
    "\n",
    "VERBOSE: bool = False\n",
    "SAVE_REPORT: bool = True\n",
    "\n",
    "\n",
    "SAVE_MODEL: bool = False\n",
    "\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "\n",
    "\n",
    "# Dataset Used : Indian Pines\n",
    "\n",
    "\n",
    "DATASET: str = 'IP' # IP (indian_pines) PU (pavia_university) SA (salinas) HU (houston) \n",
    "\n",
    "\n",
    "PATH_TO_DATASET: str = CWD + '\\\\Datasets\\\\'# PCA\n",
    "\n",
    "\n",
    "PCA_COMPONENTS: int = 30 # Number of components to keep after PCA reduction# Window size for forming image cubes\n",
    "\n",
    "\n",
    "WINDOW_SIZE: int = 11# Image dimensions after forming image cubes\n",
    "\n",
    "\n",
    "IMAGE_WIDTH: int\n",
    "\n",
    "\n",
    "IMAGE_HEIGHT: int\n",
    "\n",
    "\n",
    "IMAGE_DEPTH: int\n",
    "\n",
    "\n",
    "IMAGE_CHANNEL: int \n",
    "\n",
    "\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH, IMAGE_CHANNEL = 11, 11, 30, 1\n",
    "IMAGE_DATA = (IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH, IMAGE_CHANNEL)\n",
    "\n",
    "\n",
    "\n",
    "# Model Parameters\n",
    "\n",
    "\n",
    "\n",
    "N_TIMES = 25 # Number of times to run the model. Internally, the model is runs each episode N_TIMES times# Learning Rate\n",
    "\n",
    "\n",
    "LEARNING_RATE: float = 0.00001\n",
    "\n",
    "# Temprature Scaling\n",
    "\n",
    "\n",
    "TAU: float = 1.8\n",
    "\n",
    "\n",
    "# C (No. of Classes) K (No. of Samples per Class) N (No. of Patches per Class)\n",
    "\n",
    "\n",
    "TRAIN_C: int = 5 # Number of classes to be used for training\n",
    "\n",
    "\n",
    "TRAIN_K: int = 5 # Number of patches per class to be used for support during training\n",
    "\n",
    "\n",
    "TRAIN_N: int = 15 # Number of patches per class to be used for query during training\n",
    "\n",
    "TUNE_C: int = 3 # Number of classes to be used for testing\n",
    "\n",
    "\n",
    "TUNE_K: int = 1 # Number of patches per class to be used for support during testing\n",
    "\n",
    "\n",
    "TUNE_N: int = 4 # Number of patches per class to be used for query during testing\n",
    "\n",
    "TEST_C: int = 3 # Number of classes to be used for testing\n",
    "\n",
    "\n",
    "TEST_K: int = 5 # Number of patches per class to be used for support during testing\n",
    "\n",
    "\n",
    "TEST_N: int = 15 # Number of patches per class to be used for query during testing#\n",
    "# ===================================\n",
    "\n",
    "\n",
    "# DO NOT REMOVE THIS.\n",
    "\n",
    "# ===================================\n",
    "\n",
    "\n",
    "# Don't know this yet, probably used in the model to calculate loss\n",
    "\n",
    "\n",
    "MC_LOSS_WEIGHT: int = 5 \n",
    "\n",
    "\n",
    "# DIRECTLY USED IN PROTOTYPICAL NETWORK CLASS IN TESTING CASE\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# Training Epochs 50\n",
    "\n",
    "\n",
    "TRAINING_EPOCH: int = 40\n",
    "# Training Episode 100\n",
    "\n",
    "\n",
    "TRAINING_EPISODE: int = 100\n",
    "# Tunning Epochs 41\n",
    "\n",
    "\n",
    "TUNNING_EPOCH: int = 15\n",
    "# Tunning Episode 100\n",
    "\n",
    "\n",
    "TUNNING_EPISODE: int = 100\n",
    "# Testing Epochs 1000\n",
    "\n",
    "\n",
    "TESTING_EPOCH: int = 1000\n",
    "# Metrics to be used for evaluation\n",
    "\n",
    "\n",
    "PRE_TUNE_TESTING: bool = True\n",
    "\n",
    "\n",
    "train_loss = tf.metrics.Mean(name='train_loss')\n",
    "\n",
    "\n",
    "train_acc = tf.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "\n",
    "tune_loss = tf.metrics.Mean(name='tune_loss')\n",
    "\n",
    "\n",
    "tune_acc = tf.metrics.Mean(name='tune_accuracy')\n",
    "\n",
    "\n",
    "test_loss = tf.metrics.Mean(name='test_loss')\n",
    "\n",
    "\n",
    "test_acc = tf.metrics.Mean(name='test_accuracy')\n",
    "\n",
    "trainingData = []\n",
    "\n",
    "\n",
    "tunningData = []\n",
    "\n",
    "\n",
    "testingData = []\n",
    "TABLE= ''\n",
    "\n",
    "run_folder =  f'{date.today()}' + '-' + f'{datetime.now().hour}_5_1' + '\\\\' \n",
    "checkpoint_dir = CWD + '\\\\saves\\\\' + run_folder + DATASET + '\\\\' + f'{TRAIN_K}_shot_way' + '\\\\Train'\n",
    "\n",
    "\n",
    "checkpoint_prefix_train = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint_dir1 = CWD + '\\\\saves\\\\' + run_folder + DATASET + '\\\\' + f'{TRAIN_K}_shot_way' + '\\\\Train\\\\Tune'\n",
    "\n",
    "\n",
    "checkpoint_prefix_tune = os.path.join(checkpoint_dir1, \"ckpt\")\n",
    "report_path = CWD + f'\\\\Reports\\\\Report_{date.today()}_{str(datetime.now()).split(\".\")[0].split()[1].replace(\":\", \"-\")}.txt'\n",
    "report_path_pre = CWD + f'\\\\Reports\\\\Report_{date.today()}_{str(datetime.now()).split(\".\")[0].split()[1].replace(\":\", \"-\")}.txt'\n",
    "\n",
    "\n",
    "model_save_path = CWD + '\\\\saves\\\\' + run_folder + DATASET + '\\\\' + f'{TRAIN_K}_shot_way' + '\\\\Train\\\\encoder.h5'\n",
    "\n",
    "\n",
    "\n",
    "finalReportPath: str = ''\n",
    "reportTotal_path: str = ''\n",
    "reportTotal_path_pre: str = ''\n",
    "predict_path: str = ''\n",
    "predictTrue_path: str = ''\n",
    "gt_path: str = ''\n",
    "current_path: str = ''\n",
    "\n",
    "checkpoint = None  # To be used for loading checkpoints. Declared in the Main Block\n",
    "\n",
    "\n",
    "ProtoModel = None  # Prototypical Network Object. Declared in the Main Block\n",
    "\n",
    "\n",
    "model = None  # Model Object. Declared in the Main Block\n",
    "\n",
    "\n",
    "optimizer = None  # Optimizer Object. Declared in the Main Block\n",
    "\n",
    "X = None\n",
    "Y = None\n",
    "patches = None\n",
    "target_names = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = []\n",
    "tunningData = []\n",
    "testingData = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainingData = pd.DataFrame(columns=['Loss', 'Accuracy'])\n",
    "def plotData(data, path,testing=False):\n",
    "    \n",
    "    if testing:\n",
    "        accuracy1_values = [item[0] for item in data]\n",
    "        accuracy2_values = [item[1] for item in data]\n",
    "        loss1_values = [item[2] for item in data]\n",
    "        loss2_values = [item[3] for item in data]\n",
    "\n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(loss1_values, color='red', label='Loss')\n",
    "        plt.plot(accuracy1_values, color='blue', label='Accuracy')\n",
    "        plt.xlabel('Epoch/Episode')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Loss and Overall Accuracy 1')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        # plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(loss2_values, color='red', label='Loss')\n",
    "        plt.plot(accuracy2_values, color='blue', label='Accuracy')\n",
    "        plt.xlabel('Epoch/Episode')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Loss and Overall Accuracy 2')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        # plt.savefig(path)\n",
    "    else:\n",
    "        loss_values = [item[0] for item in data]\n",
    "        accuracy_values = [item[1] for item in data]\n",
    "\n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(loss_values, color='red', label='Loss')\n",
    "        plt.plot(accuracy_values, color='blue', label='Accuracy')\n",
    "        plt.xlabel('Epoch/Episode')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Loss and Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(path)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainingEpisode(patches:list, labels:list, K:int, C:int, N:int ):\n",
    "    \"\"\"\n",
    "    createTrainingEpisode creates a training episode for the N-way K-shot learning task.\n",
    "    \n",
    "    :param patches: list of all patches classified into different classes.\n",
    "    :param labels: list of classes from which the traning episode is to be created.\n",
    "    :param K: number of patches per class in the support set.\n",
    "    :param C: number of classes in the training episode.\n",
    "    :param N: number of patches per class in the query set.\n",
    "    :return queryPatches, queryLabels, supportPatches, supportLabels: training episode\n",
    "    \n",
    "    Algorithm:\n",
    "    - Select N classes from the list of labels. They should be unique.\n",
    "    - For each class, select K+Q patches. They should be unique.\n",
    "        - First K patches are support patches.\n",
    "        - Last Q patches are query patches.\n",
    "        - Append the support patches to supportPatches.\n",
    "        - Append the query patches to queryPatches.\n",
    "        - Append the class label to queryLabels Q times.\n",
    "    - Shuffle the queryPatches and queryLabels in the same order.\n",
    "    - Convert the queryPatches and supportPatches to tensors.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    selectedLabels = random.sample(labels, C)\n",
    "    supportPatches = []\n",
    "    supportLabels = list(selectedLabels)\n",
    "    queryPatches = []\n",
    "    queryLabels = []\n",
    "    \n",
    "    for n in selectedLabels:\n",
    "        sran_indices = np.random.choice(len(patches[n-1]),K,replace=False)  # for class no X-1: select K samples \n",
    "        supportPatches.extend( patches[n-1][sran_indices,:,:,:,:])\n",
    "        qran_indices = np.random.choice(len(patches[n-1]),N,replace=False)  # N Samples for Query\n",
    "        queryPatches.extend(patches[n-1][qran_indices,:,:,:,:])\n",
    "        queryLabels.extend([n]*N)\n",
    "    \n",
    "    shuffled = list(zip(queryPatches, queryLabels))\n",
    "    random.shuffle(shuffled)\n",
    "    queryPatches, queryLabels = zip(*shuffled)\n",
    "    \n",
    "    queryPatches = tf.convert_to_tensor(np.reshape(np.asarray(queryPatches),(C*N,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    supportPatches = tf.convert_to_tensor(np.reshape(np.asarray(supportPatches),(C*K,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    \n",
    "    return queryPatches, queryLabels, supportPatches, supportLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(support, query, support_labels, query_labels, K, C, N):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, mean_accuracy, mean_predictions = ProtoModel(support, query, support_labels, query_labels, K, C, N,training=True)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # A gradient simply measures the change in all weights with regard to the change in error. You can also think of a gradient as the slope of a function. The higher the gradient, the steeper the slope and the faster a model can learn. But if the slope is zero, the model stops learning\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Log loss and accuracy for step\n",
    "    train_loss(loss)\n",
    "    train_acc(mean_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def trainingEpochs(patches, labels, n_epochs, n_episodes):\n",
    "    \"\"\"\n",
    "    trainingEpochs function trains the model for n_epochs and n_episodes.\n",
    "    \n",
    "    :param patches: image patches to be trained\n",
    "    :param labels: corresponding labels to be used\n",
    "    :param n_epochs: number of epochs\n",
    "    :param n_episodes: number of episodes\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    template = 'Epoch {}/{}, Episode {}/{}, Train Loss: {:.2f}, Train Accuracy: {:.2f}'\n",
    "    # for epoch in tqdm(range(n_epochs), desc='Epochs'):\n",
    "    #     train_loss.reset_states()\n",
    "    #     train_acc.reset_states()\n",
    "    #     for episode in tqdm(range(n_episodes), desc=f'Episodes (Loss: {l:.2f}, Acc: {a:.2f})'):\n",
    "    \n",
    "    break_after_epoch = False\n",
    "    no_of_epoch_before_break = 1\n",
    "    trainObj = tqdm(total=n_episodes * n_epochs, desc=f'Epoch 1/{n_epochs}, Episode 1/{n_episodes}')\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss.reset_states()\n",
    "        train_acc.reset_states()\n",
    "        for episode in range(n_episodes):\n",
    "            queryPatches, queryLabels, supportPatches, supportLabels = createTrainingEpisode(patches, labels, TRAIN_K, TRAIN_C, TRAIN_N)\n",
    "            train_step(supportPatches, queryPatches,supportLabels,  queryLabels, TRAIN_K, TRAIN_C, TRAIN_N)\n",
    "            # clear_output(wait=True)\n",
    "            if(train_acc.result().numpy()*100 >= 100):\n",
    "                break_after_epoch = True\n",
    "            trainObj.set_description(\n",
    "                f'Epoch {epoch+1}/{n_epochs}, Episode {episode+1}/{n_episodes} (Loss: {train_loss.result().numpy():.2f}, Acc: {train_acc.result().numpy()*100:.2f})')\n",
    "            trainObj.update(1)\n",
    "            trainingData.append([train_loss.result(),  train_acc.result()*100])\n",
    "            if(VERBOSE):\n",
    "                print(template.format(epoch+1, n_epochs, episode+1, n_episodes, train_loss.result()*100, train_acc.result()*100))\n",
    "                trainingData.append([train_loss.result(),  train_acc.result()*100])\n",
    "                plotData(trainingData)\n",
    "        # TO BREAK IF ACC REACHES 100\n",
    "        # if(break_after_epoch):\n",
    "        #     if(no_of_epoch_before_break == 0):\n",
    "        #         trainObj.close()\n",
    "        #         print()\n",
    "        #         print('Stoping Training as 100% Acc reached atleast once')\n",
    "        #         checkpoint.save(file_prefix=checkpoint_prefix_train)    \n",
    "        #         break\n",
    "        #     no_of_epoch_before_break -= 1\n",
    "\n",
    "        if(epoch and epoch % 5 == 0):\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix_train)    \n",
    "    trainObj.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTunningEpisodes(patches:list, labels:list, K:int, C:int, N:int):\n",
    "    \"\"\"\n",
    "    createTuningEpisodes creates a tuning episode for the N-way K-shot learning task.\n",
    "    \n",
    "    :param patches: list of all patches classified into different classes.\n",
    "    :param labels: list of classes from which the tuning episode is to be created.\n",
    "    :param K: number of patches per class in the support set.\n",
    "    :param C: number of classes in the tuning episode.\n",
    "    :param N: number of patches per class in the query set.\n",
    "    :return queryPatches, queryLabels, supportPatches, supportLabels: tuning episode\n",
    "    \n",
    "    Algorithm:\n",
    "    - Select C classes from the list of labels. They should be unique.\n",
    "    - For each selected class.\n",
    "        - Shuffle the patches of that class.\n",
    "        - First K patches are support patches.\n",
    "        - Next N patches are query patches. \n",
    "        - Append the support patches to supportPatches.\n",
    "        - Append the query patches to queryPatches.\n",
    "        - Append the class label to queryLabels N times.\n",
    "    - Shuffle the queryPatches and queryLabels in the same order.\n",
    "    - Convert the queryPatches and supportPatches to tensors.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    selected_classes = np.random.choice(labels,C,replace=False)\n",
    "    supportLabels  = list(selected_classes)\n",
    "    queryLabels = []\n",
    "    supportPatches = []\n",
    "    queryPatches = []\n",
    "    \n",
    "    for x in selected_classes :\n",
    "        y = labels.index(x)\n",
    "        np.random.shuffle(patches[y])    \n",
    "        supportPatches.extend(patches[y][:K,:,:,:,:])  # 1st K patches for support set\n",
    "        queryPatches.extend(patches[y][K:K+N,:,:,:,:])   # next N patches for query set\n",
    "        queryLabels.extend([x]*N)            \n",
    "          # next 5 labels for query set\n",
    "    \n",
    "    shuffled = list(zip(queryPatches, queryLabels))\n",
    "    random.shuffle(shuffled)\n",
    "    queryPatches, queryLabels = zip(*shuffled)\n",
    "    \n",
    "    queryPatches = tf.convert_to_tensor(np.reshape(np.asarray(queryPatches),(C*N,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    supportPatches = tf.convert_to_tensor(np.reshape(np.asarray(supportPatches),(C*K,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    \n",
    "    return queryPatches, queryLabels, supportPatches, supportLabels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_step(support, query, support_labels, query_labels, K, C, N):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, mean_accuracy, mean_predictions = ProtoModel(support, query, support_labels, query_labels, K, C, N,training=True)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Log loss and accuracy for step\n",
    "    tune_loss(loss)\n",
    "    tune_acc(mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def tunningEpochs(patches, labels, n_epochs, n_episodes):\n",
    "    \"\"\"\n",
    "    trainingEpochs function trains the model for n_epochs and n_episodes.\n",
    "    \n",
    "    :param patches: image patches to be trained\n",
    "    :param labels: corresponding labels to be used\n",
    "    :param n_epochs: number of epochs\n",
    "    :param n_episodes: number of episodes\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    template = 'Epoch {}/{}, Tune Loss: {:.2f}, Tune Accuracy: {:.2f}'\n",
    "\n",
    "    epochObj = tqdm(range(n_epochs), desc='Epochs')\n",
    "    for epoch in epochObj: \n",
    "        tune_loss.reset_states()  \n",
    "        tune_acc.reset_states()    \n",
    "        for epi in range(n_episodes):  \n",
    "            queryPatches, queryLabels, supportPatches, supportLabels = createTunningEpisodes(patches, labels, TUNE_K, TUNE_C, TUNE_N)    \n",
    "            tune_step(supportPatches, queryPatches,supportLabels, queryLabels, TUNE_K, TUNE_C, TUNE_N)   \n",
    "            # clear_output(wait=True)   \n",
    "        epochObj.set_postfix(\n",
    "            {'Loss': tune_loss.result().numpy(), 'Acc': tune_acc.result().numpy()*100}, refresh=True)\n",
    "        tunningData.append([tune_loss.result(),  tune_acc.result()*100])\n",
    "        if(VERBOSE):\n",
    "            print(template.format(epoch+1, n_epochs,tune_loss.result(),tune_acc.result()*100))\n",
    "            tunningData.append([tune_loss.result(),  tune_acc.result()*100])\n",
    "            plotData(tunningData)\n",
    "        if (epoch+1)%5 == 0 :\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix_tune) \n",
    "    epochObj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTestingEpisode(patches, labels, K, C, N, i, f):\n",
    "    selected_classes = labels[i:f]   # [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    support_labels = list(selected_classes)\n",
    "    query_labels = []\n",
    "    support_patches = []\n",
    "    query_patches = []\n",
    "    for x in selected_classes:\n",
    "        y = labels.index(x)\n",
    "        # for class no X-1: select K samples\n",
    "        sran_indices = np.random.choice(len(patches[y]), K, replace=False)\n",
    "        support_patches.extend(patches[y][sran_indices, :, :, :, :])\n",
    "        qran_indices = np.random.choice(\n",
    "            len(patches[y]), N, replace=False)  # N Samples for Query\n",
    "        query_patches.extend(patches[y][qran_indices, :, :, :, :])\n",
    "        query_labels.extend([x]*N)\n",
    "\n",
    "    \n",
    "        \n",
    "        # support_imgs = patches[y][:K, :, :, :, :]\n",
    "        # query_imgs = patches[y][K: K + N, :, :, :, :]\n",
    "        # support_patches.extend(support_imgs)\n",
    "        # query_patches.extend(query_imgs)\n",
    "        # for i in range(query_imgs.shape[0]):\n",
    "        #     query_labels.append(x)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    temp1 = list(zip(query_patches, query_labels))\n",
    "    random.shuffle(temp1)\n",
    "    query_patches, query_labels = zip(*temp1)\n",
    "    x = len(query_labels)\n",
    "    query_patches = tf.convert_to_tensor(np.reshape(np.asarray(\n",
    "        query_patches), (x, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH, IMAGE_CHANNEL)), dtype=tf.float32)\n",
    "    support_patches = tf.convert_to_tensor(np.reshape(np.asarray(support_patches), (\n",
    "        C*K, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH, IMAGE_CHANNEL)), dtype=tf.float32)\n",
    "    return query_patches, support_patches, query_labels, support_labels, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(support, query, support_labels, query_labels, K, C, y):\n",
    "    loss, mc_predictions, mean_accuracy, classwise_mean_acc, y = ProtoModel(support, query, support_labels, query_labels, K, C, y,training=False)\n",
    "    return loss, mc_predictions, mean_accuracy, classwise_mean_acc, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def testingEpochs(patches, labels, n_epochs):\n",
    "    \"\"\"\n",
    "    testingEpochs function tests the model for n_epochs.\n",
    "\n",
    "    :param patches: image patches to be trained\n",
    "    :param labels: corresponding labels to be used\n",
    "    :param n_epochs: number of epochs\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    epochObj = tqdm(range(n_epochs), desc=f'Epochs')\n",
    "\n",
    "    all_y_pred1 = []\n",
    "    all_y_label1 = []\n",
    "    all_y_pred2 = []\n",
    "    all_y_label2 = []\n",
    "    for epoch in epochObj:\n",
    "        test_loss.reset_states()\n",
    "        test_acc.reset_states()\n",
    "\n",
    "        tquery_patches1, tsupport_patches1, query_labels1, support_labels1, x1 = createTestingEpisode(\n",
    "            patches, labels, TEST_K, TEST_C, TEST_N, 0, 3)\n",
    "        loss1, mc_predictions1, mean_accuracy1, classwise_mean_acc1, y1 = test_step(\n",
    "            tsupport_patches1, tquery_patches1, support_labels1, query_labels1, TEST_K, TEST_C, TEST_N)\n",
    "        oa1 = mean_accuracy1.numpy()\n",
    "\n",
    "        all_y_pred1.append(tf.convert_to_tensor(mc_predictions1))\n",
    "        all_y_label1.append(tf.convert_to_tensor(y1))\n",
    "\n",
    "        if (DATASET == 'IP' or DATASET == 'SA'):\n",
    "            tquery_patches2, tsupport_patches2, query_labels2, support_labels2, x2 = createTestingEpisode(\n",
    "                patches, labels, TEST_K, TEST_C, TEST_N, 3, 6)\n",
    "            loss2, mc_predictions2, mean_accuracy2, classwise_mean_acc2, y2 = test_step(\n",
    "                tsupport_patches2, tquery_patches2, support_labels2, query_labels2, TEST_K, TEST_C, TEST_N)\n",
    "            oa2 = mean_accuracy2.numpy()\n",
    "\n",
    "            all_y_pred2.append(tf.convert_to_tensor(mc_predictions2))\n",
    "            all_y_label2.append(tf.convert_to_tensor(y2))\n",
    "        oa2 = oa2 if (DATASET == 'IP' or DATASET == 'SA') else 0\n",
    "        epochObj.set_postfix({'OA1': f'{oa1}', 'OA2': f'{oa2}'}, refresh=True)\n",
    "        \n",
    "        testingData.append([mean_accuracy1*100, mean_accuracy2 *\n",
    "                            100, loss1.numpy(), 0 if loss2 == 0 else loss2.numpy()])\n",
    "        if (VERBOSE):\n",
    "            print(\"=========================================\")\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "            print(\"-----------------------------------------\")\n",
    "            print(f\"Overall Accuracy 1 (OA1): {mean_accuracy1}\")\n",
    "            # Class Wise Accuracy\n",
    "            for i in range(TEST_C):\n",
    "                print(f\"Class {i+1} Accuracy: {classwise_mean_acc1[i]}\")\n",
    "            print(f\"Loss: {loss1.numpy():.3f}\")\n",
    "            if (DATASET == 'IP' or DATASET == 'SA'):\n",
    "                print(\"=========================================\")\n",
    "            else:\n",
    "                print(\"-----------------------------------------\")\n",
    "                print(f\"Overall Accuracy 2 (OA2): {mean_accuracy2}\")\n",
    "                # Class Wise Accuracy\n",
    "                for i in range(TEST_C):\n",
    "                    print(\n",
    "                        f\"Class {i+1+TEST_C} Accuracy: {classwise_mean_acc2[i]}\")\n",
    "                print(f\"Loss: {loss2.numpy():.3f}\")\n",
    "                print(\"=========================================\")\n",
    "\n",
    "            testingData.append([mean_accuracy1*100, mean_accuracy2 *\n",
    "                               100, loss1.numpy(), 0 if loss2 == 0 else loss2.numpy()])\n",
    "            plotData(testingData, testing=True)\n",
    "\n",
    "    epochObj.close()\n",
    "    try:\n",
    "        combined_y_pred1 = tf.concat(all_y_pred1, axis=0)\n",
    "        combined_y_label1 = tf.concat(all_y_label1, axis=0)\n",
    "        if(DATASET == 'IP' or DATASET == 'SA'):\n",
    "            combined_y_pred2 = tf.concat(all_y_pred2, axis=0)\n",
    "            combined_y_label2 = tf.concat(all_y_label2, axis=0)\n",
    "        else:\n",
    "            combined_y_pred2 = []\n",
    "            combined_y_label2 = []\n",
    "    except:\n",
    "        combined_y_pred1 = []\n",
    "        combined_y_label1 = []\n",
    "        combined_y_pred2 = []\n",
    "        combined_y_label2 = []\n",
    "    return combined_y_pred1, combined_y_pred2, combined_y_label1, combined_y_label2  # best_mc_predictions1, best_mc_predictions2, best_y1, best_y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Related Initializations\n",
    "\n",
    "def LoadData():\n",
    "    global X, Y, patches,target_names, NUM_CLASSES, TRAINING_CLASSES, TRAINING_LABELS, TUNNING_LABELS, TESTING_CLASSES, TESTING_LABELS, TRAINING_PATCHES, TUNNING_PATCHES, TESTING_PATCHES, data\n",
    "    data = Data(DATASET, PCA_COMPONENTS, WINDOW_SIZE)\n",
    "    X, Y, patches = data.get_data()\n",
    "    target_names = data.get_target_names()\n",
    "    NUM_CLASSES, TRAINING_CLASSES, TRAINING_LABELS, TUNNING_LABELS, TESTING_CLASSES, TESTING_LABELS, TRAINING_PATCHES, TUNNING_PATCHES, TESTING_PATCHES = data.load_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Related Initializations\n",
    "\n",
    "def LoadModel(encoder:str):\n",
    "    global model, ProtoModel, optimizer, checkpoint\n",
    "    encoder = importlib.import_module('encoders.' + encoder)\n",
    "    model = encoder.createModel(\n",
    "        IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH, IMAGE_CHANNEL)\n",
    "    ProtoModel = Prototypical(\n",
    "        model, IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH, IMAGE_CHANNEL, MC_LOSS_WEIGHT, TAU, N_TIMES)\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, ProtoModel=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RUN: 1 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 15\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Episode 1/100:   0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40, Episode 100/100 (Loss: 13.59, Acc: 99.67): 100%|██████████| 4000/4000 [37:03<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 2223.7242s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 15/15 [08:29<00:00, 33.98s/it, Loss=0.185, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 509.6892s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [10:19<00:00,  1.62it/s, OA1=0.9555555582046509, OA2=1.0]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 619.0854s\n",
      "\n",
      "OA 98.44 | KA 98.12 | AA 98.44\n",
      "Report Saved!\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch: 225it [06:27,  1.72s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OA 88.53 | KA 87.04 | AA 91.28\n",
      "Image Prediction Report Saved!\n",
      "Function 'doRun' executed in 3741.0884s\n",
      "\n",
      "\n",
      "RUN: 2 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 15\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40, Episode 100/100 (Loss: 12.35, Acc: 99.77): 100%|██████████| 4000/4000 [36:14<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 2174.6668s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 15/15 [07:50<00:00, 31.36s/it, Loss=0.116, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 470.4240s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [09:46<00:00,  1.70it/s, OA1=1.0, OA2=1.0]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 586.8146s\n",
      "\n",
      "OA 99.72 | KA 99.67 | AA 99.72\n",
      "Report Saved!\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch: 225it [06:17,  1.68s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OA 87.95 | KA 86.35 | AA 90.43\n",
      "Image Prediction Report Saved!\n",
      "Function 'doRun' executed in 3610.3131s\n",
      "\n",
      "\n",
      "RUN: 3 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 15\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40, Episode 100/100 (Loss: 12.42, Acc: 99.67): 100%|██████████| 4000/4000 [35:49<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 2149.9461s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 15/15 [07:51<00:00, 31.44s/it, Loss=0.113, Acc=100] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 471.6145s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [09:46<00:00,  1.70it/s, OA1=1.0, OA2=1.0]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 586.8171s\n",
      "\n",
      "OA 98.33 | KA 97.99 | AA 98.33\n",
      "Report Saved!\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch: 225it [06:10,  1.65s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OA 91.00 | KA 89.79 | AA 90.60\n",
      "Image Prediction Report Saved!\n",
      "Function 'doRun' executed in 3579.6054s\n",
      "\n",
      "\n",
      "RUN: 4 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 15\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40, Episode 100/100 (Loss: 10.79, Acc: 99.60): 100%|██████████| 4000/4000 [35:10<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 2110.0689s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 15/15 [07:50<00:00, 31.38s/it, Loss=0.0981, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 470.7727s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [09:49<00:00,  1.70it/s, OA1=1.0, OA2=0.8888888955116272]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 589.0905s\n",
      "\n",
      "OA 97.93 | KA 97.52 | AA 97.93\n",
      "Report Saved!\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch: 225it [06:07,  1.63s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OA 88.98 | KA 87.52 | AA 90.16\n",
      "Image Prediction Report Saved!\n",
      "Function 'doRun' executed in 3537.9736s\n",
      "\n",
      "\n",
      "RUN: 5 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 15\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40, Episode 100/100 (Loss: 12.44, Acc: 99.79): 100%|██████████| 4000/4000 [35:17<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 2117.3118s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 15/15 [07:51<00:00, 31.42s/it, Loss=0.122, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 471.2904s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [09:45<00:00,  1.71it/s, OA1=1.0, OA2=1.0]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 585.8902s\n",
      "\n",
      "OA 99.19 | KA 99.03 | AA 99.19\n",
      "Report Saved!\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch: 225it [06:09,  1.64s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OA 93.46 | KA 92.55 | AA 92.33\n",
      "Image Prediction Report Saved!\n",
      "Function 'doRun' executed in 3544.5277s\n",
      "\n",
      "\n",
      "RUN: 1 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 20\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40, Episode 100/100 (Loss: 15.14, Acc: 99.40): 100%|██████████| 4000/4000 [46:28<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 2788.9196s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 15/15 [10:22<00:00, 41.52s/it, Loss=0.177, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 622.8228s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [12:28<00:00,  1.34it/s, OA1=1.0, OA2=0.9777777791023254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 748.3804s\n",
      "\n",
      "OA 98.99 | KA 98.79 | AA 98.99\n",
      "Report Saved!\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch: 225it [08:05,  2.16s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OA 90.41 | KA 89.10 | AA 90.87\n",
      "Image Prediction Report Saved!\n",
      "Function 'doRun' executed in 4645.9775s\n",
      "\n",
      "\n",
      "RUN: 2 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 20\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40, Episode 100/100 (Loss: 9.02, Acc: 99.91): 100%|██████████| 4000/4000 [46:33<00:00,  1.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 2793.7888s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 15/15 [10:24<00:00, 41.63s/it, Loss=0.0626, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 624.4942s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [12:31<00:00,  1.33it/s, OA1=1.0, OA2=0.9777777791023254]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 751.1581s\n",
      "\n",
      "OA 99.63 | KA 99.55 | AA 99.63\n",
      "Report Saved!\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch: 225it [08:04,  2.15s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OA 91.01 | KA 89.82 | AA 91.24\n",
      "Image Prediction Report Saved!\n",
      "Function 'doRun' executed in 4654.4346s\n",
      "\n",
      "\n",
      "RUN: 3 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 20\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40, Episode 100/100 (Loss: 12.29, Acc: 99.88): 100%|██████████| 4000/4000 [46:30<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 2790.0762s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 15/15 [10:24<00:00, 41.61s/it, Loss=0.0843, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 624.2209s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [12:30<00:00,  1.33it/s, OA1=1.0, OA2=0.9777777791023254]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 750.1867s\n",
      "\n",
      "OA 99.46 | KA 99.36 | AA 99.46\n",
      "Report Saved!\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch: 225it [08:03,  2.15s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OA 92.38 | KA 91.34 | AA 91.85\n",
      "Image Prediction Report Saved!\n",
      "Function 'doRun' executed in 4648.1747s\n",
      "\n",
      "\n",
      "RUN: 4 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 20\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40, Episode 100/100 (Loss: 12.20, Acc: 99.68): 100%|██████████| 4000/4000 [46:32<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 2792.6183s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 15/15 [10:23<00:00, 41.54s/it, Loss=0.114, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 623.1434s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [12:31<00:00,  1.33it/s, OA1=1.0, OA2=0.9777777791023254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 751.0286s\n",
      "\n",
      "OA 99.32 | KA 99.18 | AA 99.32\n",
      "Report Saved!\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch: 225it [08:03,  2.15s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OA 91.07 | KA 89.87 | AA 91.98\n",
      "Image Prediction Report Saved!\n",
      "Function 'doRun' executed in 4651.0813s\n",
      "\n",
      "\n",
      "RUN: 5 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 20\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40, Episode 100/100 (Loss: 10.56, Acc: 99.87): 100%|██████████| 4000/4000 [46:31<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 2791.0370s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 15/15 [10:23<00:00, 41.57s/it, Loss=0.0909, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 623.5010s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [12:30<00:00,  1.33it/s, OA1=0.9777777791023254, OA2=1.0]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 750.9504s\n",
      "\n",
      "OA 98.69 | KA 98.43 | AA 98.69\n",
      "Report Saved!\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch: 225it [08:03,  2.15s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OA 93.81 | KA 92.96 | AA 89.57\n",
      "Image Prediction Report Saved!\n",
      "Function 'doRun' executed in 4649.0208s\n",
      "\n",
      "\n",
      "RUN: 1 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 25\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40, Episode 100/100 (Loss: 12.91, Acc: 99.83): 100%|██████████| 4000/4000 [57:20<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 3440.8963s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 15/15 [12:50<00:00, 51.40s/it, Loss=0.0994, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 770.9759s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [15:14<00:00,  1.09it/s, OA1=1.0, OA2=1.0]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 914.9797s\n",
      "\n",
      "OA 99.44 | KA 99.33 | AA 99.44\n",
      "Report Saved!\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch: 225it [09:53,  2.64s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OA 81.28 | KA 78.84 | AA 85.08\n",
      "Image Prediction Report Saved!\n",
      "Function 'doRun' executed in 5720.6887s\n",
      "\n",
      "\n",
      "RUN: 2 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 25\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40, Episode 100/100 (Loss: 14.59, Acc: 99.57): 100%|██████████| 4000/4000 [57:39<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 3459.2988s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 15/15 [12:52<00:00, 51.52s/it, Loss=0.17, Acc=100] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 772.8008s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [15:19<00:00,  1.09it/s, OA1=0.9777777791023254, OA2=0.9777777791023254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 919.3468s\n",
      "\n",
      "OA 98.92 | KA 98.71 | AA 98.92\n",
      "Report Saved!\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch: 225it [10:04,  2.68s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OA 88.57 | KA 87.06 | AA 90.24\n",
      "Image Prediction Report Saved!\n",
      "Function 'doRun' executed in 5756.0044s\n",
      "\n",
      "\n",
      "RUN: 3 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 25\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40, Episode 100/100 (Loss: 12.10, Acc: 99.84): 100%|██████████| 4000/4000 [58:20<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 3500.1260s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 15/15 [13:09<00:00, 52.63s/it, Loss=0.108, Acc=100] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 789.4760s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [15:57<00:00,  1.04it/s, OA1=1.0, OA2=1.0]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 957.9393s\n",
      "\n",
      "OA 99.64 | KA 99.57 | AA 99.64\n",
      "Report Saved!\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch: 225it [10:36,  2.83s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OA 90.58 | KA 89.27 | AA 87.70\n",
      "Image Prediction Report Saved!\n",
      "Function 'doRun' executed in 5884.8731s\n",
      "\n",
      "\n",
      "RUN: 4 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 25\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40, Episode 100/100 (Loss: 11.95, Acc: 99.79): 100%|██████████| 4000/4000 [1:00:44<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 3644.3939s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 15/15 [13:37<00:00, 54.51s/it, Loss=0.0826, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 817.5874s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [16:15<00:00,  1.03it/s, OA1=1.0, OA2=0.9777777791023254]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 975.2145s\n",
      "\n",
      "OA 98.99 | KA 98.79 | AA 98.99\n",
      "Report Saved!\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch: 225it [11:00,  2.94s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OA 90.18 | KA 88.87 | AA 91.06\n",
      "Image Prediction Report Saved!\n",
      "Function 'doRun' executed in 6098.2169s\n",
      "\n",
      "\n",
      "RUN: 5 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 25\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40, Episode 100/100 (Loss: 12.59, Acc: 99.80): 100%|██████████| 4000/4000 [59:40<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 3580.2690s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 15/15 [12:56<00:00, 51.77s/it, Loss=0.114, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 776.4886s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [15:40<00:00,  1.06it/s, OA1=1.0, OA2=1.0]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 940.7663s\n",
      "\n",
      "OA 98.25 | KA 97.90 | AA 98.25\n",
      "Report Saved!\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch: 225it [11:21,  3.03s/it]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OA 90.88 | KA 89.65 | AA 88.16\n",
      "Image Prediction Report Saved!\n",
      "Function 'doRun' executed in 5979.5881s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def TAU_test_spl():\n",
    "    global testingData, test_acc, test_loss\n",
    "    testingData = []\n",
    "    test_loss = tf.metrics.Mean(name='test_loss')\n",
    "\n",
    "    test_acc = tf.metrics.Mean(name='test_accuracy')\n",
    "\n",
    "\n",
    "def setGlobals(run, tau, n_times, encoder, dataset):\n",
    "    global finalReportPath, predict_path, model_save_path, report_path, run_folder, reportTotal_path_pre, report_path_pre\n",
    "    global checkpoint_dir, checkpoint_prefix_train, checkpoint_dir1, checkpoint_prefix_tune, gt_path, predictTrue_path\n",
    "    global TAU, N_TIMES, ABLATION_FOLDER, trainingData, tunningData, testingData, DATASET, reportTotal_path, current_path\n",
    "    global train_acc, train_loss, tune_acc, tune_loss, test_acc, test_loss, checkpoint, ProtoModel, optimizer, model\n",
    "    global plot_train, plot_tune, plot_test\n",
    "    TAU = tau\n",
    "    N_TIMES = n_times\n",
    "    DATASET = dataset\n",
    "\n",
    "    ABLATION_FOLDER = CWD + '\\\\ablation_review\\\\' + TABLE + '\\\\'\n",
    "    run_folder =  f'{date.today()}' + '-' + f'{datetime.now().hour}_5_1' + '\\\\' \n",
    "    checkpoint_dir = CWD + '\\\\saves\\\\' + run_folder + DATASET + '\\\\' + f'{TRAIN_K}_shot_way' + '\\\\Train'\n",
    "    checkpoint_prefix_train = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    checkpoint_dir1 =checkpoint_dir + '\\\\Tune'\n",
    "    checkpoint_prefix_tune = os.path.join(checkpoint_dir1, \"ckpt\")\n",
    "    \n",
    "    current_path = ABLATION_FOLDER + DATASET + '\\\\' + f'{encoder}\\\\{TAU}\\\\{N_TIMES}' + f'\\\\{run+1}'\n",
    "    \n",
    "    model_save_path = current_path + '_encoder.h5'\n",
    "    \n",
    "    report_path = current_path + '_post_tune_report.txt'\n",
    "    reportTotal_path = current_path + '_post_tune_reportTotal.txt'\n",
    "    report_path_pre = current_path + '_pre_tune_report.txt'\n",
    "    reportTotal_path_pre = current_path + '_pre_tune_reportTotal.txt'\n",
    "    predict_path = current_path + '_prediction.png'\n",
    "    predictTrue_path = current_path + '_predictionTrue.png'\n",
    "    gt_path = current_path + '_gt.png'\n",
    "\n",
    "    plot_train = current_path + '_train_plot.png'\n",
    "    plot_tune = current_path + '_tune_plot.png'\n",
    "    plot_test = current_path + '_test_plot.png'\n",
    "\n",
    "    finalReportPath = ABLATION_FOLDER + 'report.csv'\n",
    "    os.makedirs(os.path.dirname(report_path), exist_ok=True)\n",
    "    trainingData = []\n",
    "    tunningData = []\n",
    "    testingData = []\n",
    "    \n",
    "    train_loss = tf.metrics.Mean(name='train_loss')\n",
    "\n",
    "    train_acc = tf.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "    tune_loss = tf.metrics.Mean(name='tune_loss')\n",
    "\n",
    "    tune_acc = tf.metrics.Mean(name='tune_accuracy')\n",
    "\n",
    "    test_loss = tf.metrics.Mean(name='test_loss')\n",
    "\n",
    "    test_acc = tf.metrics.Mean(name='test_accuracy')\n",
    "        \n",
    "    checkpoint = None  # To be used for loading checkpoints. Declared in the Main Block\n",
    "\n",
    "    ProtoModel = None  # Prototypical Network Object. Declared in the Main Block\n",
    "\n",
    "    model = None  # Model Object. Declared in the Main Block\n",
    "\n",
    "    optimizer = None  # Optimizer Object. Declared in the Main Block\n",
    "\n",
    "@timeIt\n",
    "def doRun(run, dataset, encoder, tau, n_times):\n",
    "\n",
    "    if(DATASET != dataset):\n",
    "        LoadData()\n",
    "\n",
    "    setGlobals(run, tau, n_times, encoder, dataset)\n",
    "\n",
    "    LoadModel(encoder)\n",
    "\n",
    "    print('Training...')\n",
    "    trainingEpochs(patches, TRAINING_LABELS, TRAINING_EPOCH, TRAINING_EPISODE)\n",
    "    print()\n",
    "    if(PRE_TUNE_TESTING):\n",
    "        print('Pre Tune Testing...')\n",
    "        mc_predictions1, mc_predictions2, y1, y2 = testingEpochs(TESTING_PATCHES, TESTING_LABELS, TESTING_EPOCH)\n",
    "        print()\n",
    "\n",
    "        stats_pre = Stats(mc_predictions1, mc_predictions2, y1, y2)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            stats_pre.saveReport(report_path_pre, f'{ABLATION_FOLDER}pre_report.csv',  DATASET, N_TIMES, TAU, run, encoder)\n",
    "            print('Report Saved!')\n",
    "        except Exception as e:\n",
    "            print('Failed to Save Report')\n",
    "            print(e)\n",
    "        predictions, predictionsTrue, Y, y_test, y_pred = predictImage(data, ProtoModel, IMAGE_DATA, N_TIMES, 10,TAU, VotingTimes=5, RandomSupport=True)\n",
    "\n",
    "        try:\n",
    "            stats1_pre = Stats(None, None, None, None, y_test, y_pred, False)\n",
    "            stats1_pre.saveReport(reportTotal_path_pre, finalReportPath,  DATASET, N_TIMES, TAU, run, encoder, True)\n",
    "            print('Image Prediction Report Saved!')\n",
    "        except Exception as e:\n",
    "            print('Failed to Save Image Prediction Report')\n",
    "            print(e)\n",
    "        \n",
    "        spectral.save_rgb(f\"{current_path}_prediction_pre.png\", predictions.astype(int), colors=spectral.spy_colors, format='png')\n",
    "        spectral.save_rgb(f\"{current_path}_predictionTrue_pre.png\", predictionsTrue.astype(int), colors=spectral.spy_colors, format='png')\n",
    "        TAU_test_spl()\n",
    "    # return \n",
    "    print('Tunning...')\n",
    "    tunningEpochs(TUNNING_PATCHES, TESTING_LABELS,TUNNING_EPOCH, TUNNING_EPISODE)\n",
    "    print()\n",
    "    \n",
    "    print('Testing...')\n",
    "    mc_predictions1, mc_predictions2, y1, y2 = testingEpochs(TESTING_PATCHES, TESTING_LABELS, TESTING_EPOCH)\n",
    "    print()\n",
    "\n",
    "    stats = Stats(mc_predictions1, mc_predictions2, y1, y2)\n",
    "    \n",
    "    # stats.printReport()\n",
    "    \n",
    "    try:\n",
    "        stats.saveReport(report_path, finalReportPath,  DATASET, N_TIMES, TAU, run, encoder)\n",
    "        print('Report Saved!')\n",
    "    except Exception as e:\n",
    "        print('Failed to Save Report')\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        ProtoModel.save(model_save_path)\n",
    "        print('Model Saved!')\n",
    "    except Exception as e:\n",
    "        print(\"Failed to Save Model\")\n",
    "        print(e)\n",
    "    \n",
    "    predictions, predictionsTrue, Y, y_test, y_pred = predictImage(data, ProtoModel, IMAGE_DATA, N_TIMES, 10,TAU, VotingTimes=5, RandomSupport=True)\n",
    "\n",
    "    try:\n",
    "        stats1 = Stats(None, None, None, None, y_test, y_pred, False)\n",
    "        stats1.saveReport(reportTotal_path, finalReportPath,  DATASET, N_TIMES, TAU, run, encoder, True)\n",
    "        print('Image Prediction Report Saved!')\n",
    "    except Exception as e:\n",
    "        print('Failed to Save Image Prediction Report')\n",
    "        print(e)\n",
    "    \n",
    "    spectral.save_rgb(predict_path, predictions.astype(int), colors=spectral.spy_colors, format='png')\n",
    "    spectral.save_rgb(predictTrue_path, predictionsTrue.astype(int), colors=spectral.spy_colors, format='png')\n",
    "    # spectral.save_rgb(gt_path, Y.astype(int), colors=spectral.spy_colors, format='png')\n",
    "\n",
    "    # plotData(trainingData, plot_train, testing=False)\n",
    "    # plotData(tunningData, plot_tune, testing=False)\n",
    "    # plotData(testingData, plot_test, testing=True)\n",
    "\n",
    "def main():\n",
    "    # Tweekables:\n",
    "    global TABLE, PRE_TUNE_TESTING\n",
    "    TABLE = 'tr 40 tu 15' \n",
    "    PRE_TUNE_TESTING = False\n",
    "    done = [] \n",
    "    n_timess = [15, 20, 25]\n",
    "    taus = [1.8]\n",
    "    datasets = ['IP']\n",
    "    encoders = ['conv_sa']\n",
    "    runs  = 5\n",
    "    doneRuns = 0 # Increment this if doing more runs for same combinations\n",
    "    for dataset in datasets:\n",
    "        global DATASET\n",
    "        DATASET = dataset\n",
    "        LoadData() \n",
    "        for encoder in encoders:\n",
    "            for tau in taus: \n",
    "                for n_times in n_timess:\n",
    "                    for run in range(runs):\n",
    "                        print(f'\\n\\nRUN: {run+1+doneRuns} | DATASET: {dataset} | ENCODER: {encoder} | TAU: {tau} | N_TIMES: {n_times}\\n')\n",
    "                        try:\n",
    "                            doRun(run, dataset, encoder, tau, n_times)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error: {e}\")\n",
    "                            continue\n",
    "\n",
    "\n",
    "main()\n",
    "# os.system(\"shutdown /s /t 2\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system(\"shutdown /s /t 2\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
