{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aditya Sawant's Version of SPN_IP_5Shot.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Used\n",
    "\n",
    "- tensorflow\n",
    "- sklearn\n",
    "- numpy\n",
    "- matplotlib\n",
    "- pandas\n",
    "- scipy\n",
    "- tensorflow_probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn numpy matplotlib scipy tensorflow_probability pandas tqdm plotly\n",
    "# tensorflow[and-cuda]==2.10 [cuDNN 8.1.1 CUDA 11.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import statistics\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "  \n",
    "from operator import truediv\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.version.GIT_VERSION, tf.version.VERSION)\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(\"Number of GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Number of Devices Available: \", len(tf.config.experimental.list_physical_devices()))\n",
    "# import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.compat.v1.distributions import Bernoulli\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
    "\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "from datetime import date, datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from encoders.conv_sa import createModel\n",
    "from lib.PrototypicalNetwork import Prototypical\n",
    "from lib.misc import timeIt, plotData\n",
    "from lib.Data import Data\n",
    "from lib.Stats import Stats\n",
    "from lib.Predcit import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test each code block individually\n",
    "TEST_BLOCKS: bool = False\n",
    "CWD: str = os.getcwd()\n",
    "VERBOSE: bool = False\n",
    "\n",
    "SAVE_REPORT: bool = True\n",
    "SAVE_MODEL: bool = False\n",
    "# Data Loading and Preprocessing\n",
    "\n",
    "\n",
    "# Dataset Used : Indian Pines\n",
    "DATASET: str = 'IP' # IP (indian_pines) PU (pavia_university) SA (salinas) HU (houston) \n",
    "PATH_TO_DATASET: str = CWD + '\\\\Datasets\\\\'\n",
    "\n",
    "# PCA\n",
    "PCA_COMPONENTS: int = 30 # Number of components to keep after PCA reduction\n",
    "\n",
    "# Window size for forming image cubes\n",
    "WINDOW_SIZE: int = 11\n",
    "\n",
    "# Image dimensions after forming image cubes\n",
    "IMAGE_WIDTH: int\n",
    "IMAGE_HEIGHT: int\n",
    "IMAGE_DEPTH: int\n",
    "IMAGE_CHANNEL: int \n",
    "IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH, IMAGE_CHANNEL = 11, 11, 30, 1\n",
    "\n",
    "# Model Parameters\n",
    "\n",
    "N_TIMES = 5 # Number of times to run the model. Internally, the model is runs each episode N_TIMES times\n",
    "\n",
    "# Learning Rate\n",
    "LEARNING_RATE: float = 0.00001\n",
    "\n",
    "# Temprature Scaling\n",
    "TAU: float = 1.8\n",
    "\n",
    "# C (No. of Classes) K (No. of Samples per Class) N (No. of Patches per Class)\n",
    "TRAIN_C: int = 5 # Number of classes to be used for training\n",
    "TRAIN_K: int = 5 # Number of patches per class to be used for support during training\n",
    "TRAIN_N: int = 15 # Number of patches per class to be used for query during training\n",
    "\n",
    "TUNE_C: int = 3 # Number of classes to be used for testing\n",
    "TUNE_K: int = 1 # Number of patches per class to be used for support during testing\n",
    "TUNE_N: int = 4 # Number of patches per class to be used for query during testing\n",
    "\n",
    "TEST_C: int = 3 # Number of classes to be used for testing\n",
    "TEST_K: int = 5 # Number of patches per class to be used for support during testing\n",
    "TEST_N: int = 5 # Number of patches per class to be used for query during testing\n",
    "\n",
    "# ===================================\n",
    "# DO NOT REMOVE THIS.\n",
    "tC = 3   # classes in a test episode \n",
    "# Don't know this yet, probably used in the model to calculate loss\n",
    "MC_LOSS_WEIGHT: int = 5 \n",
    "# DIRECTLY USED IN PROTOTYPICAL NETWORK CLASS IN TESTING CASE\n",
    "# ===================================\n",
    "\n",
    "# Training Epochs 50\n",
    "TRAINING_EPOCH: int = 50\n",
    "\n",
    "# Training Episode 100\n",
    "TRAINING_EPISODE: int = 100\n",
    "\n",
    "# Tunning Epochs 41\n",
    "TUNNING_EPOCH: int = 41\n",
    "\n",
    "# Tunning Episode 100\n",
    "TUNNING_EPISODE: int = 100\n",
    "\n",
    "# Testing Epochs 1000\n",
    "TESTING_EPOCH: int = 1000\n",
    "\n",
    "# Metrics to be used for evaluation\n",
    "train_loss = tf.metrics.Mean(name='train_loss')\n",
    "train_acc = tf.metrics.Mean(name='train_accuracy')\n",
    "tune_loss = tf.metrics.Mean(name='tune_loss')\n",
    "tune_acc = tf.metrics.Mean(name='tune_accuracy')\n",
    "test_loss = tf.metrics.Mean(name='test_loss')\n",
    "test_acc = tf.metrics.Mean(name='test_accuracy')\n",
    "\n",
    "trainingData = []\n",
    "tunningData = []\n",
    "testingData = []\n",
    "\n",
    "run_folder =  f'{date.today()}' + '-' + f'{datetime.now().hour}_5_1' + '\\\\' \n",
    "\n",
    "checkpoint_dir = CWD + '\\\\saves\\\\' + run_folder + DATASET + '\\\\' + f'{TRAIN_K}_shot_way' + '\\\\Train'\n",
    "checkpoint_prefix_train = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "checkpoint_dir1 = CWD + '\\\\saves\\\\' + run_folder + DATASET + '\\\\' + f'{TRAIN_K}_shot_way' + '\\\\Train\\\\Tune'\n",
    "checkpoint_prefix_tune = os.path.join(checkpoint_dir1, \"ckpt\")\n",
    "\n",
    "report_path = CWD + f'\\\\Reports\\\\Report_{date.today()}_{str(datetime.now()).split(\".\")[0].split()[1].replace(\":\", \"-\")}.txt'\n",
    "model_save_path = CWD + '\\\\saves\\\\' + run_folder + DATASET + '\\\\' + f'{TRAIN_K}_shot_way' + '\\\\Train\\\\encoder.h5'\n",
    "\n",
    "\n",
    "checkpoint = None  # To be used for loading checkpoints. Declared in the Main Block\n",
    "ProtoModel = None  # Prototypical Network Object. Declared in the Main Block\n",
    "model = None  # Model Object. Declared in the Main Block\n",
    "optimizer = None  # Optimizer Object. Declared in the Main Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainingEpisode(patches:list, labels:list, K:int, C:int, N:int ):\n",
    "    \"\"\"\n",
    "    createTrainingEpisode creates a training episode for the N-way K-shot learning task.\n",
    "    \n",
    "    :param patches: list of all patches classified into different classes.\n",
    "    :param labels: list of classes from which the traning episode is to be created.\n",
    "    :param K: number of patches per class in the support set.\n",
    "    :param C: number of classes in the training episode.\n",
    "    :param N: number of patches per class in the query set.\n",
    "    :return queryPatches, queryLabels, supportPatches, supportLabels: training episode\n",
    "    \n",
    "    Algorithm:\n",
    "    - Select N classes from the list of labels. They should be unique.\n",
    "    - For each class, select K+Q patches. They should be unique.\n",
    "        - First K patches are support patches.\n",
    "        - Last Q patches are query patches.\n",
    "        - Append the support patches to supportPatches.\n",
    "        - Append the query patches to queryPatches.\n",
    "        - Append the class label to queryLabels Q times.\n",
    "    - Shuffle the queryPatches and queryLabels in the same order.\n",
    "    - Convert the queryPatches and supportPatches to tensors.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    selectedLabels = random.sample(labels, C)\n",
    "    supportPatches = []\n",
    "    supportLabels = list(selectedLabels)\n",
    "    queryPatches = []\n",
    "    queryLabels = []\n",
    "    \n",
    "    for n in selectedLabels:\n",
    "        sran_indices = np.random.choice(len(patches[n-1]),K,replace=False)  # for class no X-1: select K samples \n",
    "        supportPatches.extend( patches[n-1][sran_indices,:,:,:,:])\n",
    "        qran_indices = np.random.choice(len(patches[n-1]),N,replace=False)  # N Samples for Query\n",
    "        queryPatches.extend(patches[n-1][qran_indices,:,:,:,:])\n",
    "        queryLabels.extend([n]*N)\n",
    "    \n",
    "    shuffled = list(zip(queryPatches, queryLabels))\n",
    "    random.shuffle(shuffled)\n",
    "    queryPatches, queryLabels = zip(*shuffled)\n",
    "    \n",
    "    queryPatches = tf.convert_to_tensor(np.reshape(np.asarray(queryPatches),(C*N,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    supportPatches = tf.convert_to_tensor(np.reshape(np.asarray(supportPatches),(C*K,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    \n",
    "    return queryPatches, queryLabels, supportPatches, supportLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(support, query, support_labels, query_labels, K, C, N):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, mean_accuracy, mean_predictions = ProtoModel(support, query, support_labels, query_labels, K, C, N,N_TIMES,training=True)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # A gradient simply measures the change in all weights with regard to the change in error. You can also think of a gradient as the slope of a function. The higher the gradient, the steeper the slope and the faster a model can learn. But if the slope is zero, the model stops learning\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Log loss and accuracy for step\n",
    "    train_loss(loss)\n",
    "    train_acc(mean_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def trainingEpochs(patches, labels, n_epochs, n_episodes):\n",
    "    \"\"\"\n",
    "    trainingEpochs function trains the model for n_epochs and n_episodes.\n",
    "    \n",
    "    :param patches: image patches to be trained\n",
    "    :param labels: corresponding labels to be used\n",
    "    :param n_epochs: number of epochs\n",
    "    :param n_episodes: number of episodes\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    template = 'Epoch {}/{}, Episode {}/{}, Train Loss: {:.2f}, Train Accuracy: {:.2f}'\n",
    "    # for epoch in tqdm(range(n_epochs), desc='Epochs'):\n",
    "    #     train_loss.reset_states()\n",
    "    #     train_acc.reset_states()\n",
    "    #     for episode in tqdm(range(n_episodes), desc=f'Episodes (Loss: {l:.2f}, Acc: {a:.2f})'):\n",
    "    \n",
    "    trainObj = tqdm(total=n_episodes * n_epochs, desc=f'Epoch 1/{n_epochs}, Episode 1/{n_episodes}')\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss.reset_states()\n",
    "        train_acc.reset_states()\n",
    "        for episode in range(n_episodes):\n",
    "            queryPatches, queryLabels, supportPatches, supportLabels = createTrainingEpisode(patches, labels, TRAIN_K, TRAIN_C, TRAIN_N)\n",
    "            train_step(supportPatches, queryPatches,supportLabels,  queryLabels, TRAIN_K, TRAIN_C, TRAIN_N)\n",
    "            # clear_output(wait=True)\n",
    "            trainObj.set_description(\n",
    "                f'Epoch {epoch+1}/{n_epochs}, Episode {episode+1}/{n_episodes} (Loss: {train_loss.result().numpy()*100:.2f}, Acc: {train_acc.result().numpy()*100:.2f})')\n",
    "            trainObj.update(1)\n",
    "            if(VERBOSE):\n",
    "                print(template.format(epoch+1, n_epochs, episode+1, n_episodes, train_loss.result()*100, train_acc.result()*100))\n",
    "                trainingData.append([train_loss.result(),  train_acc.result()*100])\n",
    "                plotData(trainingData)\n",
    "        \n",
    "        if(epoch and epoch % 5 == 0):\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix_train)    \n",
    "    trainObj.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTunningEpisodes(patches:list, labels:list, K:int, C:int, N:int):\n",
    "    \"\"\"\n",
    "    createTuningEpisodes creates a tuning episode for the N-way K-shot learning task.\n",
    "    \n",
    "    :param patches: list of all patches classified into different classes.\n",
    "    :param labels: list of classes from which the tuning episode is to be created.\n",
    "    :param K: number of patches per class in the support set.\n",
    "    :param C: number of classes in the tuning episode.\n",
    "    :param N: number of patches per class in the query set.\n",
    "    :return queryPatches, queryLabels, supportPatches, supportLabels: tuning episode\n",
    "    \n",
    "    Algorithm:\n",
    "    - Select C classes from the list of labels. They should be unique.\n",
    "    - For each selected class.\n",
    "        - Shuffle the patches of that class.\n",
    "        - First K patches are support patches.\n",
    "        - Next N patches are query patches. \n",
    "        - Append the support patches to supportPatches.\n",
    "        - Append the query patches to queryPatches.\n",
    "        - Append the class label to queryLabels N times.\n",
    "    - Shuffle the queryPatches and queryLabels in the same order.\n",
    "    - Convert the queryPatches and supportPatches to tensors.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    selected_classes = np.random.choice(labels,C,replace=False)\n",
    "    supportLabels  = list(selected_classes)\n",
    "    queryLabels = []\n",
    "    supportPatches = []\n",
    "    queryPatches = []\n",
    "    \n",
    "    for x in selected_classes :\n",
    "        y = labels.index(x)\n",
    "        np.random.shuffle(patches[y])    \n",
    "        supportPatches.extend(patches[y][:K,:,:,:,:])  # 1st K patches for support set\n",
    "        queryPatches.extend(patches[y][K:K+N,:,:,:,:])   # next N patches for query set\n",
    "        queryLabels.extend([x]*N)            \n",
    "          # next 5 labels for query set\n",
    "    \n",
    "    shuffled = list(zip(queryPatches, queryLabels))\n",
    "    random.shuffle(shuffled)\n",
    "    queryPatches, queryLabels = zip(*shuffled)\n",
    "    \n",
    "    queryPatches = tf.convert_to_tensor(np.reshape(np.asarray(queryPatches),(C*N,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    supportPatches = tf.convert_to_tensor(np.reshape(np.asarray(supportPatches),(C*K,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    \n",
    "    return queryPatches, queryLabels, supportPatches, supportLabels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_step(support, query, support_labels, query_labels, K, C, N):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, mean_accuracy, mean_predictions = ProtoModel(support, query, support_labels, query_labels, K, C, N,N_TIMES,training=True)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Log loss and accuracy for step\n",
    "    tune_loss(loss)\n",
    "    tune_acc(mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def tunningEpochs(patches, labels, n_epochs, n_episodes):\n",
    "    \"\"\"\n",
    "    trainingEpochs function trains the model for n_epochs and n_episodes.\n",
    "    \n",
    "    :param patches: image patches to be trained\n",
    "    :param labels: corresponding labels to be used\n",
    "    :param n_epochs: number of epochs\n",
    "    :param n_episodes: number of episodes\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    template = 'Epoch {}/{}, Tune Loss: {:.2f}, Tune Accuracy: {:.2f}'\n",
    "\n",
    "    epochObj = tqdm(range(n_epochs), desc='Epochs')\n",
    "    for epoch in epochObj: \n",
    "        tune_loss.reset_states()  \n",
    "        tune_acc.reset_states()    \n",
    "        for epi in range(n_episodes):  \n",
    "            queryPatches, queryLabels, supportPatches, supportLabels = createTunningEpisodes(patches, labels, TUNE_K, TUNE_C, TUNE_N)    \n",
    "            tune_step(supportPatches, queryPatches,supportLabels, queryLabels, TUNE_K, TUNE_C, TUNE_N)   \n",
    "            # clear_output(wait=True)   \n",
    "        epochObj.set_postfix(\n",
    "            {'Loss': tune_loss.result().numpy(), 'Acc': tune_acc.result().numpy()*100}, refresh=True)\n",
    "        if(VERBOSE):\n",
    "            print(template.format(epoch+1, n_epochs,tune_loss.result(),tune_acc.result()*100))\n",
    "            tunningData.append([tune_loss.result(),  tune_acc.result()*100])\n",
    "            plotData(tunningData)\n",
    "        if (epoch+1)%5 == 0 :\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix_tune) \n",
    "    epochObj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTestingEpisode(patches, labels, K, C, i, f):\n",
    "    selected_classes = labels[i:f]   # [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    support_labels = list(selected_classes)\n",
    "    query_labels = []\n",
    "    support_patches = []\n",
    "    query_patches = []\n",
    "    for x in selected_classes :\n",
    "        y = labels.index(x)\n",
    "        \n",
    "        support_imgs = patches[y][:K,:,:,:,:]\n",
    "        query_imgs = patches[y][K:,:,:,:,:]\n",
    "        support_patches.extend(support_imgs)\n",
    "        query_patches.extend(query_imgs)\n",
    "        for i in range(query_imgs.shape[0]) :\n",
    "            query_labels.append(x)\n",
    "    temp1 = list(zip(query_patches, query_labels)) \n",
    "    random.shuffle(temp1) \n",
    "    query_patches, query_labels = zip(*temp1)\n",
    "    x = len(query_labels)\n",
    "    query_patches = tf.convert_to_tensor(np.reshape(np.asarray(query_patches),(x,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    support_patches = tf.convert_to_tensor(np.reshape(np.asarray(support_patches),(C*K,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    return query_patches, support_patches, query_labels, support_labels,x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(support, query, support_labels, query_labels, K, C, y):\n",
    "    loss, mc_predictions, mean_accuracy, classwise_mean_acc, y = ProtoModel(support, query, support_labels, query_labels, K, C, y,N_TIMES,training=False)\n",
    "    return loss, mc_predictions, mean_accuracy, classwise_mean_acc, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def testingEpochs(patches, labels, n_epochs):\n",
    "    \"\"\"\n",
    "    testingEpochs function tests the model for n_epochs.\n",
    "    \n",
    "    :param patches: image patches to be trained\n",
    "    :param labels: corresponding labels to be used\n",
    "    :param n_epochs: number of epochs\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    epochObj = tqdm(range(n_epochs), desc=f'Epochs')\n",
    "    \n",
    "    for epoch in epochObj:\n",
    "        test_loss.reset_states()  \n",
    "        test_acc.reset_states()     \n",
    "        \n",
    "        tquery_patches1, tsupport_patches1, query_labels1, support_labels1, x1 = createTestingEpisode(patches,labels,TEST_K,TEST_C,0,3)    \n",
    "        loss1, mc_predictions1, mean_accuracy1, classwise_mean_acc1, y1 = test_step(tsupport_patches1, tquery_patches1,support_labels1, query_labels1, TEST_K, TEST_C, y=x1/3) \n",
    "        if(DATASET == 'IP' or DATASET == 'SA'):\n",
    "            tquery_patches2, tsupport_patches2, query_labels2, support_labels2, x2 = createTestingEpisode(patches,labels,TEST_K,TEST_C,3,6)    \n",
    "            loss2, mc_predictions2, mean_accuracy2, classwise_mean_acc2, y2 = test_step(tsupport_patches2, tquery_patches2,support_labels2, query_labels2, 5, 3, x2/3)\n",
    "            oa2 = mean_accuracy2\n",
    "        else:\n",
    "            oa2 = 0\n",
    "            mc_predictions2 = 0\n",
    "            y2 = 0\n",
    "            loss2 = 0\n",
    "            mean_accuracy2 = 0\n",
    "            classwise_mean_acc2 = 0\n",
    "        \n",
    "        oa1 = mean_accuracy1\n",
    "        epochObj.set_postfix(\n",
    "            {'OA1': oa1.numpy(), 'OA2': oa2.numpy()}, refresh=True)\n",
    "        if(VERBOSE):\n",
    "            print(\"=========================================\")\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "            print(\"-----------------------------------------\")\n",
    "            print(f\"Overall Accuracy 1 (OA1): {mean_accuracy1}\")\n",
    "            # Class Wise Accuracy\n",
    "            for i in range(TEST_C):\n",
    "                print(f\"Class {i+1} Accuracy: {classwise_mean_acc1[i]}\")\n",
    "            print(f\"Loss: {loss1.numpy():.3f}\")\n",
    "            if(DATASET == 'IP' or DATASET == 'SA'):\n",
    "                print(\"=========================================\")\n",
    "            else:\n",
    "                print(\"-----------------------------------------\")\n",
    "                print(f\"Overall Accuracy 2 (OA2): {mean_accuracy2}\")\n",
    "                # Class Wise Accuracy\n",
    "                for i in range(TEST_C):\n",
    "                    print(f\"Class {i+1+TEST_C} Accuracy: {classwise_mean_acc2[i]}\")\n",
    "                print(f\"Loss: {loss2.numpy():.3f}\")\n",
    "                print(\"=========================================\")\n",
    "            \n",
    "            testingData.append([mean_accuracy1*100, mean_accuracy2*100, loss1.numpy(), 0 if loss2 == 0 else loss2.numpy()])\n",
    "            plotData(testingData, testing=True)\n",
    "        \n",
    "    epochObj.close()\n",
    "    return mc_predictions1, mc_predictions2, y1, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Each Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWindow(X, Y, windowSize):\n",
    "    # TODO: \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "data = Data(DATASET, PCA_COMPONENTS, WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, patches = data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES, TRAINING_CLASSES, TRAINING_LABELS, TUNNING_LABELS, TESTING_CLASSES, TESTING_LABELS, TRAINING_PATCHES,TUNNING_PATCHES, TESTING_PATCHES = data.load_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of the model\n",
    "model = createModel(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH, IMAGE_CHANNEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of the Prototypical Network\n",
    "ProtoModel = Prototypical(model, IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH, IMAGE_CHANNEL, MC_LOSS_WEIGHT, TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of the Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of the Checkpoint\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, ProtoModel = ProtoModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainingEpochs(patches, TRAINING_LABELS, TRAINING_EPOCH, TRAINING_EPISODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunningEpochs(TUNNING_PATCHES, TESTING_LABELS, TUNNING_EPOCH, TUNNING_EPISODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_predictions1, mc_predictions2, y1, y2 =  testingEpochs(TESTING_PATCHES, TESTING_LABELS, TESTING_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = Stats(mc_predictions1, mc_predictions2, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.printReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(SAVE_REPORT):\n",
    "    stats.saveReport(report_path, DATASET, N_TIMES, TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(SAVE_MODEL):\n",
    "    ProtoModel.save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
