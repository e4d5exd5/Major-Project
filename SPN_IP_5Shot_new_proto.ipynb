{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aditya Sawant's Version of SPN_IP_5Shot.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Used\n",
    "\n",
    "- tensorflow\n",
    "- sklearn\n",
    "- numpy\n",
    "- matplotlib\n",
    "- pandas\n",
    "- scipy\n",
    "- tensorflow_probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn numpy matplotlib scipy tensorflow_probability pandas tqdm plotly\n",
    "# tensorflow[and-cuda]==2.10 [cuDNN 8.1.1 CUDA 11.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.10.0-rc3-6-g359c3cdfc5f 2.10.0\n",
      "True\n",
      "Number of GPUs Available:  1\n",
      "Number of Devices Available:  2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.27.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HSI FSL BE-10 Major Project\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning:\n",
      "\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import statistics\n",
    "import os\n",
    "import importlib\n",
    "import IPython\n",
    "from IPython.display import clear_output\n",
    "  \n",
    "from operator import truediv\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.version.GIT_VERSION, tf.version.VERSION)\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(\"Number of GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Number of Devices Available: \", len(tf.config.experimental.list_physical_devices()))\n",
    "# import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.compat.v1.distributions import Bernoulli\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
    "import spectral\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "from datetime import date, datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from encoders.conv_gan import createModel\n",
    "from lib.PrototypicalNetwork import Prototypical\n",
    "from lib.misc import timeIt, plotData\n",
    "from lib.Data import Data\n",
    "from lib.Stats import Stats\n",
    "from lib.Predict import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test each code block individually\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TEST_BLOCKS: bool = False\n",
    "CWD: str = os.getcwd()\n",
    "\n",
    "\n",
    "VERBOSE: bool = False\n",
    "SAVE_REPORT: bool = True\n",
    "\n",
    "\n",
    "SAVE_MODEL: bool = False\n",
    "\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "\n",
    "\n",
    "# Dataset Used : Indian Pines\n",
    "\n",
    "\n",
    "DATASET: str = 'IP' # IP (indian_pines) PU (pavia_university) SA (salinas) HU (houston) \n",
    "\n",
    "\n",
    "PATH_TO_DATASET: str = CWD + '\\\\Datasets\\\\'# PCA\n",
    "\n",
    "\n",
    "PCA_COMPONENTS: int = 30 # Number of components to keep after PCA reduction# Window size for forming image cubes\n",
    "\n",
    "\n",
    "WINDOW_SIZE: int = 11# Image dimensions after forming image cubes\n",
    "\n",
    "\n",
    "IMAGE_WIDTH: int\n",
    "\n",
    "\n",
    "IMAGE_HEIGHT: int\n",
    "\n",
    "\n",
    "IMAGE_DEPTH: int\n",
    "\n",
    "\n",
    "IMAGE_CHANNEL: int \n",
    "\n",
    "\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH, IMAGE_CHANNEL = 11, 11, 30, 1\n",
    "IMAGE_DATA = (IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH, IMAGE_CHANNEL)\n",
    "\n",
    "\n",
    "\n",
    "# Model Parameters\n",
    "\n",
    "\n",
    "\n",
    "N_TIMES = 5 # Number of times to run the model. Internally, the model is runs each episode N_TIMES times# Learning Rate\n",
    "\n",
    "\n",
    "LEARNING_RATE: float = 0.00001# Temprature Scaling\n",
    "\n",
    "\n",
    "TAU: float = 1.8\n",
    "\n",
    "\n",
    "# C (No. of Classes) K (No. of Samples per Class) N (No. of Patches per Class)\n",
    "\n",
    "\n",
    "TRAIN_C: int = 5 # Number of classes to be used for training\n",
    "\n",
    "\n",
    "TRAIN_K: int = 5 # Number of patches per class to be used for support during training\n",
    "\n",
    "\n",
    "TRAIN_N: int = 15 # Number of patches per class to be used for query during training\n",
    "\n",
    "TUNE_C: int = 3 # Number of classes to be used for testing\n",
    "\n",
    "\n",
    "TUNE_K: int = 1 # Number of patches per class to be used for support during testing\n",
    "\n",
    "\n",
    "TUNE_N: int = 4 # Number of patches per class to be used for query during testing\n",
    "\n",
    "TEST_C: int = 3 # Number of classes to be used for testing\n",
    "\n",
    "\n",
    "TEST_K: int = 5 # Number of patches per class to be used for support during testing\n",
    "\n",
    "\n",
    "TEST_N: int = 5 # Number of patches per class to be used for query during testing#\n",
    "# ===================================\n",
    "\n",
    "\n",
    "# DO NOT REMOVE THIS.\n",
    "\n",
    "# ===================================\n",
    "\n",
    "\n",
    "# Don't know this yet, probably used in the model to calculate loss\n",
    "\n",
    "\n",
    "MC_LOSS_WEIGHT: int = 5 \n",
    "\n",
    "\n",
    "# DIRECTLY USED IN PROTOTYPICAL NETWORK CLASS IN TESTING CASE\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# Training Epochs 50\n",
    "\n",
    "\n",
    "TRAINING_EPOCH: int = 50\n",
    "# Training Episode 100\n",
    "\n",
    "\n",
    "TRAINING_EPISODE: int = 100\n",
    "# Tunning Epochs 41\n",
    "\n",
    "\n",
    "TUNNING_EPOCH: int = 41\n",
    "# Tunning Episode 100\n",
    "\n",
    "\n",
    "TUNNING_EPISODE: int = 100\n",
    "# Testing Epochs 1000\n",
    "\n",
    "\n",
    "TESTING_EPOCH: int = 1000\n",
    "# Metrics to be used for evaluation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loss = tf.metrics.Mean(name='train_loss')\n",
    "\n",
    "\n",
    "train_acc = tf.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "\n",
    "tune_loss = tf.metrics.Mean(name='tune_loss')\n",
    "\n",
    "\n",
    "tune_acc = tf.metrics.Mean(name='tune_accuracy')\n",
    "\n",
    "\n",
    "test_loss = tf.metrics.Mean(name='test_loss')\n",
    "\n",
    "\n",
    "test_acc = tf.metrics.Mean(name='test_accuracy')\n",
    "\n",
    "trainingData = []\n",
    "\n",
    "\n",
    "tunningData = []\n",
    "\n",
    "\n",
    "testingData = []\n",
    "\n",
    "run_folder =  f'{date.today()}' + '-' + f'{datetime.now().hour}_5_1' + '\\\\' \n",
    "checkpoint_dir = CWD + '\\\\saves\\\\' + run_folder + DATASET + '\\\\' + f'{TRAIN_K}_shot_way' + '\\\\Train'\n",
    "\n",
    "\n",
    "checkpoint_prefix_train = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint_dir1 = CWD + '\\\\saves\\\\' + run_folder + DATASET + '\\\\' + f'{TRAIN_K}_shot_way' + '\\\\Train\\\\Tune'\n",
    "\n",
    "\n",
    "checkpoint_prefix_tune = os.path.join(checkpoint_dir1, \"ckpt\")\n",
    "report_path = CWD + f'\\\\Reports\\\\Report_{date.today()}_{str(datetime.now()).split(\".\")[0].split()[1].replace(\":\", \"-\")}.txt'\n",
    "\n",
    "\n",
    "model_save_path = CWD + '\\\\saves\\\\' + run_folder + DATASET + '\\\\' + f'{TRAIN_K}_shot_way' + '\\\\Train\\\\encoder.h5'\n",
    "\n",
    "\n",
    "\n",
    "finalReportPath: str = ''\n",
    "predict_path: str = ''\n",
    "\n",
    "checkpoint = None  # To be used for loading checkpoints. Declared in the Main Block\n",
    "\n",
    "\n",
    "ProtoModel = None  # Prototypical Network Object. Declared in the Main Block\n",
    "\n",
    "\n",
    "model = None  # Model Object. Declared in the Main Block\n",
    "\n",
    "\n",
    "optimizer = None  # Optimizer Object. Declared in the Main Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainingEpisode(patches:list, labels:list, K:int, C:int, N:int ):\n",
    "    \"\"\"\n",
    "    createTrainingEpisode creates a training episode for the N-way K-shot learning task.\n",
    "    \n",
    "    :param patches: list of all patches classified into different classes.\n",
    "    :param labels: list of classes from which the traning episode is to be created.\n",
    "    :param K: number of patches per class in the support set.\n",
    "    :param C: number of classes in the training episode.\n",
    "    :param N: number of patches per class in the query set.\n",
    "    :return queryPatches, queryLabels, supportPatches, supportLabels: training episode\n",
    "    \n",
    "    Algorithm:\n",
    "    - Select N classes from the list of labels. They should be unique.\n",
    "    - For each class, select K+Q patches. They should be unique.\n",
    "        - First K patches are support patches.\n",
    "        - Last Q patches are query patches.\n",
    "        - Append the support patches to supportPatches.\n",
    "        - Append the query patches to queryPatches.\n",
    "        - Append the class label to queryLabels Q times.\n",
    "    - Shuffle the queryPatches and queryLabels in the same order.\n",
    "    - Convert the queryPatches and supportPatches to tensors.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    selectedLabels = random.sample(labels, C)\n",
    "    supportPatches = []\n",
    "    supportLabels = list(selectedLabels)\n",
    "    queryPatches = []\n",
    "    queryLabels = []\n",
    "    \n",
    "    for n in selectedLabels:\n",
    "        sran_indices = np.random.choice(len(patches[n-1]),K,replace=False)  # for class no X-1: select K samples \n",
    "        supportPatches.extend( patches[n-1][sran_indices,:,:,:,:])\n",
    "        qran_indices = np.random.choice(len(patches[n-1]),N,replace=False)  # N Samples for Query\n",
    "        queryPatches.extend(patches[n-1][qran_indices,:,:,:,:])\n",
    "        queryLabels.extend([n]*N)\n",
    "    \n",
    "    shuffled = list(zip(queryPatches, queryLabels))\n",
    "    random.shuffle(shuffled)\n",
    "    queryPatches, queryLabels = zip(*shuffled)\n",
    "    \n",
    "    queryPatches = tf.convert_to_tensor(np.reshape(np.asarray(queryPatches),(C*N,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    supportPatches = tf.convert_to_tensor(np.reshape(np.asarray(supportPatches),(C*K,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    \n",
    "    return queryPatches, queryLabels, supportPatches, supportLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(support, query, support_labels, query_labels, K, C, N):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, mean_accuracy, mean_predictions = ProtoModel(support, query, support_labels, query_labels, K, C, N,training=True)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # A gradient simply measures the change in all weights with regard to the change in error. You can also think of a gradient as the slope of a function. The higher the gradient, the steeper the slope and the faster a model can learn. But if the slope is zero, the model stops learning\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Log loss and accuracy for step\n",
    "    train_loss(loss)\n",
    "    train_acc(mean_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def trainingEpochs(patches, labels, n_epochs, n_episodes):\n",
    "    \"\"\"\n",
    "    trainingEpochs function trains the model for n_epochs and n_episodes.\n",
    "    \n",
    "    :param patches: image patches to be trained\n",
    "    :param labels: corresponding labels to be used\n",
    "    :param n_epochs: number of epochs\n",
    "    :param n_episodes: number of episodes\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    template = 'Epoch {}/{}, Episode {}/{}, Train Loss: {:.2f}, Train Accuracy: {:.2f}'\n",
    "    # for epoch in tqdm(range(n_epochs), desc='Epochs'):\n",
    "    #     train_loss.reset_states()\n",
    "    #     train_acc.reset_states()\n",
    "    #     for episode in tqdm(range(n_episodes), desc=f'Episodes (Loss: {l:.2f}, Acc: {a:.2f})'):\n",
    "    \n",
    "    trainObj = tqdm(total=n_episodes * n_epochs, desc=f'Epoch 1/{n_epochs}, Episode 1/{n_episodes}')\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss.reset_states()\n",
    "        train_acc.reset_states()\n",
    "        for episode in range(n_episodes):\n",
    "            queryPatches, queryLabels, supportPatches, supportLabels = createTrainingEpisode(patches, labels, TRAIN_K, TRAIN_C, TRAIN_N)\n",
    "            train_step(supportPatches, queryPatches,supportLabels,  queryLabels, TRAIN_K, TRAIN_C, TRAIN_N)\n",
    "            # clear_output(wait=True)\n",
    "            trainObj.set_description(\n",
    "                f'Epoch {epoch+1}/{n_epochs}, Episode {episode+1}/{n_episodes} (Loss: {train_loss.result().numpy():.2f}, Acc: {train_acc.result().numpy()*100:.2f})')\n",
    "            trainObj.update(1)\n",
    "            if(VERBOSE):\n",
    "                print(template.format(epoch+1, n_epochs, episode+1, n_episodes, train_loss.result()*100, train_acc.result()*100))\n",
    "                trainingData.append([train_loss.result(),  train_acc.result()*100])\n",
    "                plotData(trainingData)\n",
    "        \n",
    "        if(epoch and epoch % 5 == 0):\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix_train)    \n",
    "    trainObj.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTunningEpisodes(patches:list, labels:list, K:int, C:int, N:int):\n",
    "    \"\"\"\n",
    "    createTuningEpisodes creates a tuning episode for the N-way K-shot learning task.\n",
    "    \n",
    "    :param patches: list of all patches classified into different classes.\n",
    "    :param labels: list of classes from which the tuning episode is to be created.\n",
    "    :param K: number of patches per class in the support set.\n",
    "    :param C: number of classes in the tuning episode.\n",
    "    :param N: number of patches per class in the query set.\n",
    "    :return queryPatches, queryLabels, supportPatches, supportLabels: tuning episode\n",
    "    \n",
    "    Algorithm:\n",
    "    - Select C classes from the list of labels. They should be unique.\n",
    "    - For each selected class.\n",
    "        - Shuffle the patches of that class.\n",
    "        - First K patches are support patches.\n",
    "        - Next N patches are query patches. \n",
    "        - Append the support patches to supportPatches.\n",
    "        - Append the query patches to queryPatches.\n",
    "        - Append the class label to queryLabels N times.\n",
    "    - Shuffle the queryPatches and queryLabels in the same order.\n",
    "    - Convert the queryPatches and supportPatches to tensors.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    selected_classes = np.random.choice(labels,C,replace=False)\n",
    "    supportLabels  = list(selected_classes)\n",
    "    queryLabels = []\n",
    "    supportPatches = []\n",
    "    queryPatches = []\n",
    "    \n",
    "    for x in selected_classes :\n",
    "        y = labels.index(x)\n",
    "        np.random.shuffle(patches[y])    \n",
    "        supportPatches.extend(patches[y][:K,:,:,:,:])  # 1st K patches for support set\n",
    "        queryPatches.extend(patches[y][K:K+N,:,:,:,:])   # next N patches for query set\n",
    "        queryLabels.extend([x]*N)            \n",
    "          # next 5 labels for query set\n",
    "    \n",
    "    shuffled = list(zip(queryPatches, queryLabels))\n",
    "    random.shuffle(shuffled)\n",
    "    queryPatches, queryLabels = zip(*shuffled)\n",
    "    \n",
    "    queryPatches = tf.convert_to_tensor(np.reshape(np.asarray(queryPatches),(C*N,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    supportPatches = tf.convert_to_tensor(np.reshape(np.asarray(supportPatches),(C*K,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    \n",
    "    return queryPatches, queryLabels, supportPatches, supportLabels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_step(support, query, support_labels, query_labels, K, C, N):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, mean_accuracy, mean_predictions = ProtoModel(support, query, support_labels, query_labels, K, C, N,training=True)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Log loss and accuracy for step\n",
    "    tune_loss(loss)\n",
    "    tune_acc(mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def tunningEpochs(patches, labels, n_epochs, n_episodes):\n",
    "    \"\"\"\n",
    "    trainingEpochs function trains the model for n_epochs and n_episodes.\n",
    "    \n",
    "    :param patches: image patches to be trained\n",
    "    :param labels: corresponding labels to be used\n",
    "    :param n_epochs: number of epochs\n",
    "    :param n_episodes: number of episodes\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    template = 'Epoch {}/{}, Tune Loss: {:.2f}, Tune Accuracy: {:.2f}'\n",
    "\n",
    "    epochObj = tqdm(range(n_epochs), desc='Epochs')\n",
    "    for epoch in epochObj: \n",
    "        tune_loss.reset_states()  \n",
    "        tune_acc.reset_states()    \n",
    "        for epi in range(n_episodes):  \n",
    "            queryPatches, queryLabels, supportPatches, supportLabels = createTunningEpisodes(patches, labels, TUNE_K, TUNE_C, TUNE_N)    \n",
    "            tune_step(supportPatches, queryPatches,supportLabels, queryLabels, TUNE_K, TUNE_C, TUNE_N)   \n",
    "            # clear_output(wait=True)   \n",
    "        epochObj.set_postfix(\n",
    "            {'Loss': tune_loss.result().numpy(), 'Acc': tune_acc.result().numpy()*100}, refresh=True)\n",
    "        if(VERBOSE):\n",
    "            print(template.format(epoch+1, n_epochs,tune_loss.result(),tune_acc.result()*100))\n",
    "            tunningData.append([tune_loss.result(),  tune_acc.result()*100])\n",
    "            plotData(tunningData)\n",
    "        if (epoch+1)%5 == 0 :\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix_tune) \n",
    "    epochObj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTestingEpisode(patches, labels, K, C, i, f):\n",
    "    selected_classes = labels[i:f]   # [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    support_labels = list(selected_classes)\n",
    "    query_labels = []\n",
    "    support_patches = []\n",
    "    query_patches = []\n",
    "    for x in selected_classes :\n",
    "        y = labels.index(x)\n",
    "        \n",
    "        support_imgs = patches[y][:K,:,:,:,:]\n",
    "        query_imgs = patches[y][K:,:,:,:,:]\n",
    "        support_patches.extend(support_imgs)\n",
    "        query_patches.extend(query_imgs)\n",
    "        for i in range(query_imgs.shape[0]) :\n",
    "            query_labels.append(x)\n",
    "    temp1 = list(zip(query_patches, query_labels)) \n",
    "    random.shuffle(temp1) \n",
    "    query_patches, query_labels = zip(*temp1)\n",
    "    x = len(query_labels)\n",
    "    query_patches = tf.convert_to_tensor(np.reshape(np.asarray(query_patches),(x,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    support_patches = tf.convert_to_tensor(np.reshape(np.asarray(support_patches),(C*K,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH,IMAGE_CHANNEL)),dtype=tf.float32)\n",
    "    return query_patches, support_patches, query_labels, support_labels,x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(support, query, support_labels, query_labels, K, C, y):\n",
    "    loss, mc_predictions, mean_accuracy, classwise_mean_acc, y = ProtoModel(support, query, support_labels, query_labels, K, C, y,training=False)\n",
    "    return loss, mc_predictions, mean_accuracy, classwise_mean_acc, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeIt\n",
    "def testingEpochs(patches, labels, n_epochs):\n",
    "    \"\"\"\n",
    "    testingEpochs function tests the model for n_epochs.\n",
    "    \n",
    "    :param patches: image patches to be trained\n",
    "    :param labels: corresponding labels to be used\n",
    "    :param n_epochs: number of epochs\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    epochObj = tqdm(range(n_epochs), desc=f'Epochs')\n",
    "    \n",
    "    for epoch in epochObj:\n",
    "        test_loss.reset_states()  \n",
    "        test_acc.reset_states()     \n",
    "        \n",
    "        tquery_patches1, tsupport_patches1, query_labels1, support_labels1, x1 = createTestingEpisode(patches,labels,TEST_K,TEST_C,0,3)    \n",
    "        loss1, mc_predictions1, mean_accuracy1, classwise_mean_acc1, y1 = test_step(tsupport_patches1, tquery_patches1,support_labels1, query_labels1, TEST_K, TEST_C, y=x1/3) \n",
    "        if(DATASET == 'IP' or DATASET == 'SA'):\n",
    "            tquery_patches2, tsupport_patches2, query_labels2, support_labels2, x2 = createTestingEpisode(patches,labels,TEST_K,TEST_C,3,6)    \n",
    "            loss2, mc_predictions2, mean_accuracy2, classwise_mean_acc2, y2 = test_step(tsupport_patches2, tquery_patches2,support_labels2, query_labels2, 5, 3, x2/3)\n",
    "            oa2 = mean_accuracy2\n",
    "        else:\n",
    "            oa2 = 0\n",
    "            mc_predictions2 = 0\n",
    "            y2 = 0\n",
    "            loss2 = 0\n",
    "            mean_accuracy2 = 0\n",
    "            classwise_mean_acc2 = 0\n",
    "        \n",
    "        oa1 = mean_accuracy1\n",
    "        epochObj.set_postfix(\n",
    "            {'OA1': oa1.numpy(), 'OA2': oa2.numpy()}, refresh=True)\n",
    "        if(VERBOSE):\n",
    "            print(\"=========================================\")\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "            print(\"-----------------------------------------\")\n",
    "            print(f\"Overall Accuracy 1 (OA1): {mean_accuracy1}\")\n",
    "            # Class Wise Accuracy\n",
    "            for i in range(TEST_C):\n",
    "                print(f\"Class {i+1} Accuracy: {classwise_mean_acc1[i]}\")\n",
    "            print(f\"Loss: {loss1.numpy():.3f}\")\n",
    "            if(DATASET == 'IP' or DATASET == 'SA'):\n",
    "                print(\"=========================================\")\n",
    "            else:\n",
    "                print(\"-----------------------------------------\")\n",
    "                print(f\"Overall Accuracy 2 (OA2): {mean_accuracy2}\")\n",
    "                # Class Wise Accuracy\n",
    "                for i in range(TEST_C):\n",
    "                    print(f\"Class {i+1+TEST_C} Accuracy: {classwise_mean_acc2[i]}\")\n",
    "                print(f\"Loss: {loss2.numpy():.3f}\")\n",
    "                print(\"=========================================\")\n",
    "            \n",
    "            testingData.append([mean_accuracy1*100, mean_accuracy2*100, loss1.numpy(), 0 if loss2 == 0 else loss2.numpy()])\n",
    "            plotData(testingData, testing=True)\n",
    "        \n",
    "    epochObj.close()\n",
    "    return mc_predictions1, mc_predictions2, y1, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Related Initializations\n",
    "\n",
    "def LoadData():\n",
    "    global X, Y, patches, NUM_CLASSES, TRAINING_CLASSES, TRAINING_LABELS, TUNNING_LABELS, TESTING_CLASSES, TESTING_LABELS, TRAINING_PATCHES, TUNNING_PATCHES, TESTING_PATCHES, data\n",
    "    data = Data(DATASET, PCA_COMPONENTS, WINDOW_SIZE)\n",
    "    X, Y, patches = data.get_data()\n",
    "    NUM_CLASSES, TRAINING_CLASSES, TRAINING_LABELS, TUNNING_LABELS, TESTING_CLASSES, TESTING_LABELS, TRAINING_PATCHES, TUNNING_PATCHES, TESTING_PATCHES = data.load_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Related Initializations\n",
    "\n",
    "def LoadModel(encoder:str):\n",
    "    global model, ProtoModel, optimizer, checkpoint\n",
    "    encoder = importlib.import_module('encoders.' + encoder)\n",
    "    model = encoder.createModel(\n",
    "        IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH, IMAGE_CHANNEL)\n",
    "    ProtoModel = Prototypical(\n",
    "        model, IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH, IMAGE_CHANNEL, MC_LOSS_WEIGHT, TAU, N_TIMES)\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, ProtoModel=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RUN: 1 | DATASET: IP | ENCODER: conv_gan | TAU: 1.8 | N_TIMES: 10\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 6.90, Acc: 99.95): 100%|██████████| 5000/5000 [27:01<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 1621.2076s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [11:11<00:00, 16.37s/it, Loss=0.0439, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 671.3486s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [28:29<00:00,  1.71s/it, OA1=1, OA2=0.927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 1709.4922s\n",
      "\n",
      "OA 96.33 | KA 94.91 | AA 98.17\n",
      "Model not saved\n",
      "Function 'doRun' executed in 4002.1382s\n",
      "\n",
      "\n",
      "RUN: 2 | DATASET: IP | ENCODER: conv_gan | TAU: 1.8 | N_TIMES: 10\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 5.49, Acc: 99.95): 100%|██████████| 5000/5000 [26:57<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 1617.1615s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [11:20<00:00, 16.59s/it, Loss=0.0312, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 680.2046s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [27:59<00:00,  1.68s/it, OA1=1, OA2=0.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 1679.3167s\n",
      "\n",
      "OA 98.50 | KA 97.90 | AA 99.25\n",
      "Model not saved\n",
      "Function 'doRun' executed in 3976.7721s\n",
      "\n",
      "\n",
      "RUN: 3 | DATASET: IP | ENCODER: conv_gan | TAU: 1.8 | N_TIMES: 10\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 8.27, Acc: 99.84): 100%|██████████| 5000/5000 [26:46<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 1606.3359s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [11:20<00:00, 16.59s/it, Loss=0.0344, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 680.2239s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [28:07<00:00,  1.69s/it, OA1=1, OA2=0.967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 1687.5546s\n",
      "\n",
      "OA 98.33 | KA 97.67 | AA 99.17\n",
      "Model not saved\n",
      "Function 'doRun' executed in 3974.2001s\n",
      "\n",
      "\n",
      "RUN: 4 | DATASET: IP | ENCODER: conv_gan | TAU: 1.8 | N_TIMES: 10\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 5.07, Acc: 99.96): 100%|██████████| 5000/5000 [26:42<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 1602.3297s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [11:09<00:00, 16.33s/it, Loss=0.0124, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 669.6969s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [27:56<00:00,  1.68s/it, OA1=1, OA2=0.941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 1676.4046s\n",
      "\n",
      "OA 96.99 | KA 95.82 | AA 98.50\n",
      "Model not saved\n",
      "Function 'doRun' executed in 3948.5072s\n",
      "\n",
      "\n",
      "RUN: 5 | DATASET: IP | ENCODER: conv_gan | TAU: 1.8 | N_TIMES: 10\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 6.98, Acc: 99.91): 100%|██████████| 5000/5000 [26:53<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 1613.3488s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [11:16<00:00, 16.51s/it, Loss=0.0312, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 676.8926s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [28:01<00:00,  1.68s/it, OA1=0.943, OA2=0.911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 1681.2553s\n",
      "\n",
      "OA 92.65 | KA 89.98 | AA 96.53\n",
      "Model not saved\n",
      "Function 'doRun' executed in 3971.5578s\n",
      "\n",
      "\n",
      "RUN: 1 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 1\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 0.17, Acc: 93.93): 100%|██████████| 5000/5000 [03:37<00:00, 22.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 217.7279s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [01:48<00:00,  2.66s/it, Loss=0.0022, Acc=99.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 108.8833s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [10:01<00:00,  1.66it/s, OA1=0.997, OA2=0.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 601.6636s\n",
      "\n",
      "OA 97.83 | KA 96.97 | AA 98.93\n",
      "Model not saved\n",
      "Function 'doRun' executed in 928.3961s\n",
      "\n",
      "\n",
      "RUN: 2 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 1\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 0.11, Acc: 96.45): 100%|██████████| 5000/5000 [03:38<00:00, 22.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 218.1797s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [01:49<00:00,  2.66s/it, Loss=0.000808, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 109.0027s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [10:39<00:00,  1.56it/s, OA1=0.983, OA2=0.964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 639.8285s\n",
      "\n",
      "OA 97.33 | KA 96.28 | AA 98.05\n",
      "Model not saved\n",
      "Function 'doRun' executed in 967.1030s\n",
      "\n",
      "\n",
      "RUN: 3 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 1\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 0.12, Acc: 95.64): 100%|██████████| 5000/5000 [03:41<00:00, 22.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 221.7360s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [01:50<00:00,  2.69s/it, Loss=0.000264, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 110.2649s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [10:23<00:00,  1.60it/s, OA1=0.946, OA2=0.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 623.3536s\n",
      "\n",
      "OA 95.33 | KA 93.54 | AA 96.85\n",
      "Model not saved\n",
      "Function 'doRun' executed in 955.4749s\n",
      "\n",
      "\n",
      "RUN: 4 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 1\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 0.14, Acc: 94.65): 100%|██████████| 5000/5000 [03:38<00:00, 22.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 218.0435s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [01:48<00:00,  2.65s/it, Loss=0.000507, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 108.7474s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [10:00<00:00,  1.67it/s, OA1=0.983, OA2=0.983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 600.0313s\n",
      "\n",
      "OA 98.33 | KA 97.66 | AA 98.89\n",
      "Model not saved\n",
      "Function 'doRun' executed in 926.9335s\n",
      "\n",
      "\n",
      "RUN: 5 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 1\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 0.13, Acc: 95.11): 100%|██████████| 5000/5000 [03:40<00:00, 22.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 220.9450s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [01:49<00:00,  2.68s/it, Loss=0.000358, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 109.7766s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [10:47<00:00,  1.55it/s, OA1=0.932, OA2=0.987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 647.1320s\n",
      "\n",
      "OA 95.99 | KA 94.46 | AA 98.23\n",
      "Model not saved\n",
      "Function 'doRun' executed in 977.9457s\n",
      "\n",
      "\n",
      "RUN: 1 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 5\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 8.49, Acc: 99.77): 100%|██████████| 5000/5000 [14:48<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 888.8647s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [07:12<00:00, 10.54s/it, Loss=0.0681, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 432.3333s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [17:38<00:00,  1.06s/it, OA1=0.973, OA2=0.917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 1058.5985s\n",
      "\n",
      "OA 94.49 | KA 92.36 | AA 95.33\n",
      "Model not saved\n",
      "Function 'doRun' executed in 2379.8972s\n",
      "\n",
      "\n",
      "RUN: 2 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 5\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 9.42, Acc: 99.65): 100%|██████████| 5000/5000 [14:57<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 897.2823s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [07:16<00:00, 10.64s/it, Loss=0.0107, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 436.1209s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [17:36<00:00,  1.06s/it, OA1=0.993, OA2=0.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 1056.1407s\n",
      "\n",
      "OA 98.66 | KA 98.13 | AA 99.02\n",
      "Model not saved\n",
      "Function 'doRun' executed in 2389.6323s\n",
      "\n",
      "\n",
      "RUN: 3 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 5\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 8.82, Acc: 99.73): 100%|██████████| 5000/5000 [14:53<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 893.9195s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [07:16<00:00, 10.65s/it, Loss=0.0307, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 436.7858s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [17:19<00:00,  1.04s/it, OA1=0.986, OA2=0.983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 1039.6794s\n",
      "\n",
      "OA 98.50 | KA 97.90 | AA 99.30\n",
      "Model not saved\n",
      "Function 'doRun' executed in 2370.4915s\n",
      "\n",
      "\n",
      "RUN: 4 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 5\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 11.81, Acc: 99.43): 100%|██████████| 5000/5000 [14:45<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 885.4512s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [07:19<00:00, 10.73s/it, Loss=0.0358, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 439.8628s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [17:14<00:00,  1.03s/it, OA1=0.959, OA2=0.964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 1034.6144s\n",
      "\n",
      "OA 96.16 | KA 94.66 | AA 96.21\n",
      "Model not saved\n",
      "Function 'doRun' executed in 2360.0181s\n",
      "\n",
      "\n",
      "RUN: 5 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 5\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 7.15, Acc: 99.72): 100%|██████████| 5000/5000 [14:58<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 898.7816s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [07:30<00:00, 11.00s/it, Loss=0.0603, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 450.9887s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [17:45<00:00,  1.07s/it, OA1=0.919, OA2=0.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 1065.7386s\n",
      "\n",
      "OA 95.49 | KA 93.77 | AA 97.36\n",
      "Model not saved\n",
      "Function 'doRun' executed in 2415.6094s\n",
      "\n",
      "\n",
      "RUN: 1 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 10\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 7.22, Acc: 99.87): 100%|██████████| 5000/5000 [29:35<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 1775.4727s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [14:39<00:00, 21.45s/it, Loss=0.0111, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 879.4363s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [28:40<00:00,  1.72s/it, OA1=0.983, OA2=0.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 1720.4090s\n",
      "\n",
      "OA 98.16 | KA 97.43 | AA 98.81\n",
      "Model not saved\n",
      "Function 'doRun' executed in 4375.4245s\n",
      "\n",
      "\n",
      "RUN: 2 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 10\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 8.16, Acc: 99.91): 100%|██████████| 5000/5000 [29:20<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 1760.9896s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [14:45<00:00, 21.61s/it, Loss=0.0447, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 885.8488s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [28:32<00:00,  1.71s/it, OA1=0.963, OA2=0.987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 1712.4616s\n",
      "\n",
      "OA 97.50 | KA 96.51 | AA 98.88\n",
      "Model not saved\n",
      "Function 'doRun' executed in 4359.4161s\n",
      "\n",
      "\n",
      "RUN: 3 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 10\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 8.32, Acc: 99.84): 100%|██████████| 5000/5000 [29:30<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 1770.1431s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [14:54<00:00, 21.82s/it, Loss=0.032, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 894.4475s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [28:51<00:00,  1.73s/it, OA1=1, OA2=0.983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 1731.2406s\n",
      "\n",
      "OA 99.17 | KA 98.83 | AA 99.58\n",
      "Model not saved\n",
      "Function 'doRun' executed in 4395.9453s\n",
      "\n",
      "\n",
      "RUN: 4 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 10\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 7.74, Acc: 99.88): 100%|██████████| 5000/5000 [29:31<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 1771.0653s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [14:19<00:00, 20.97s/it, Loss=0.0234, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 859.6803s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [28:33<00:00,  1.71s/it, OA1=1, OA2=0.987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 1713.7468s\n",
      "\n",
      "OA 99.33 | KA 99.06 | AA 99.67\n",
      "Model not saved\n",
      "Function 'doRun' executed in 4344.6040s\n",
      "\n",
      "\n",
      "RUN: 5 | DATASET: IP | ENCODER: conv_sa | TAU: 1.8 | N_TIMES: 10\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Episode 100/100 (Loss: 10.34, Acc: 99.68): 100%|██████████| 5000/5000 [29:17<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'trainingEpochs' executed in 1757.5750s\n",
      "\n",
      "Tunning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 41/41 [14:45<00:00, 21.60s/it, Loss=0.0104, Acc=100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tunningEpochs' executed in 885.5672s\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [28:49<00:00,  1.73s/it, OA1=0.922, OA2=0.947]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'testingEpochs' executed in 1729.1158s\n",
      "\n",
      "OA 93.49 | KA 91.04 | AA 94.34\n",
      "Model not saved\n",
      "Function 'doRun' executed in 4372.3673s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def setGlobals(run, tau, n_times, encoder):\n",
    "    global finalReportPath, predict_path, model_save_path, report_path, run_folder, checkpoint_dir, checkpoint_prefix_train, checkpoint_dir1, checkpoint_prefix_tune, TAU, N_TIMES, ABLATION_FOLDER, trainingData, tunningData, testingData\n",
    "\n",
    "    TAU = tau\n",
    "    N_TIMES = n_times\n",
    "    \n",
    "    TABLE = 'table test'\n",
    "    ABLATION_FOLDER = CWD + '\\\\ablation\\\\' + TABLE + '\\\\'\n",
    "    run_folder =  f'{date.today()}' + '-' + f'{datetime.now().hour}_5_1' + '\\\\' \n",
    "    checkpoint_dir = CWD + '\\\\saves\\\\' + run_folder + DATASET + '\\\\' + f'{TRAIN_K}_shot_way' + '\\\\Train'\n",
    "    checkpoint_prefix_train = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    checkpoint_dir1 = CWD + '\\\\saves\\\\' + run_folder + DATASET + '\\\\' + f'{TRAIN_K}_shot_way' + '\\\\Train\\\\Tune'\n",
    "    checkpoint_prefix_tune = os.path.join(checkpoint_dir1, \"ckpt\")\n",
    "    \n",
    "    model_save_path = ABLATION_FOLDER + DATASET + '\\\\' + f'{TAU}\\\\{N_TIMES}' + f'\\\\{run+1}_encoder.h5'\n",
    "    \n",
<<<<<<< HEAD
    "    report_path = ABLATION_FOLDER + DATASET + '\\\\' + f'{encoder}\\\\{TAU}\\\\{N_TIMES}' + f'\\\\{run+1}_report.csv'\n",
    "    predict_path = ABLATION_FOLDER + DATASET + '\\\\' + f'{encoder}\\\\{TAU}\\\\{N_TIMES}' + f'\\\\{run+1}_prediction.png'\n",
=======
    "    report_path = ABLATION_FOLDER + DATASET + '\\\\' + f'{TAU}\\\\{N_TIMES}' + f'\\\\{run+1}_report.csv'\n",
    "    predict_path = ABLATION_FOLDER + DATASET + '\\\\' + f'{TAU}\\\\{N_TIMES}' + f'\\\\{run+1}_prediction.png'\n",
>>>>>>> 5a74a5b2b7fcd94b4a7beaf51a86d8232ed6a84e
    "    finalReportPath = ABLATION_FOLDER + 'report.csv'\n",
    "    trainingData = []\n",
    "    tunningData = []\n",
    "    testingData = []\n",
    "    \n",
    "    train_loss = tf.metrics.Mean(name='train_loss')\n",
    "\n",
    "    train_acc = tf.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "    tune_loss = tf.metrics.Mean(name='tune_loss')\n",
    "\n",
    "    tune_acc = tf.metrics.Mean(name='tune_accuracy')\n",
    "\n",
    "    test_loss = tf.metrics.Mean(name='test_loss')\n",
    "\n",
    "    test_acc = tf.metrics.Mean(name='test_accuracy')\n",
    "        \n",
    "    checkpoint = None  # To be used for loading checkpoints. Declared in the Main Block\n",
    "\n",
    "    ProtoModel = None  # Prototypical Network Object. Declared in the Main Block\n",
    "\n",
    "    model = None  # Model Object. Declared in the Main Block\n",
    "\n",
    "    optimizer = None  # Optimizer Object. Declared in the Main Block\n",
    "\n",
    "@timeIt\n",
    "def doRun(run, dataset, encoder, tau, n_times):\n",
    "\n",
    "    setGlobals(run, tau, n_times, encoder)\n",
    "\n",
    "    LoadModel(encoder)\n",
    "\n",
    "    print('Training...')\n",
    "    trainingEpochs(patches, TRAINING_LABELS, TRAINING_EPOCH, TRAINING_EPISODE)\n",
    "    print()\n",
    "    \n",
    "    print('Tunning...')\n",
    "    tunningEpochs(TUNNING_PATCHES, TESTING_LABELS,TUNNING_EPOCH, TUNNING_EPISODE)\n",
    "    print()\n",
    "    \n",
    "    print('Testing...')\n",
    "    mc_predictions1, mc_predictions2, y1, y2 = testingEpochs(TESTING_PATCHES, TESTING_LABELS, TESTING_EPOCH)\n",
    "    print()\n",
    "\n",
    "    stats = Stats(mc_predictions1, mc_predictions2, y1, y2)\n",
    "    \n",
    "    # stats.printReport()\n",
    "    \n",
    "    stats.saveReport(report_path, finalReportPath,  DATASET, N_TIMES, TAU, run, encoder)\n",
    "    \n",
    "    try:\n",
    "        ProtoModel.save(model_save_path)\n",
    "    except:\n",
    "        print(\"Model not saved\")\n",
    "    \n",
    "    # predictions, Y, all_preds, all_y_preds = predictImage(data, ProtoModel, IMAGE_DATA, N_TIMES, 10)\n",
    "    \n",
<<<<<<< HEAD
    "    # spectral.save_rgb(predict_path, predictions.astype(int), colors=spectral.spy_colors)\n",
=======
    "    spectral.save_rgb(predict_path, predictions.astype(\n",
    "        int), colors=spectral.spy_colors, format='png')\n",
>>>>>>> 5a74a5b2b7fcd94b4a7beaf51a86d8232ed6a84e
    "    \n",
    "    \n",
    "\n",
    "def main():\n",
    "    # Tweekables:\n",
    "    done = [True] * 2\n",
    "    n_timess = [1, 5, 10]\n",
    "    taus = [1.8]\n",
    "    datasets = ['IP']\n",
    "    encoders = ['conv_gan', 'conv_sa']\n",
    "    runs  = 5\n",
    "    for dataset in datasets:\n",
    "        DATASET = dataset\n",
    "        LoadData()\n",
    "        for encoder in encoders:\n",
    "            for tau in taus:\n",
    "                for n_times in n_timess:\n",
    "                    if(done and done.pop()): continue\n",
    "                    for run in range(runs):\n",
    "                        print(f'\\n\\nRUN: {run+1} | DATASET: {dataset} | ENCODER: {encoder} | TAU: {tau} | N_TIMES: {n_times}\\n')\n",
    "                        try:\n",
    "                            doRun(run, dataset, encoder, tau, n_times)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error: {e}\")\n",
    "                            continue\n",
    "\n",
    "\n",
    "main()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
